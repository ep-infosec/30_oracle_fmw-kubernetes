[
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/oracle-cloud/prepare-oke-environment/",
	"title": "Preparing an OKE environment",
	"tags": [],
	"description": "Running WebLogic Kubernetes Operator managed WebCenter Content domain on Oracle Kubernetes Engine (OKE).",
	"content": "Contents  Create Public SSH Key to access all the Bastion and Worker nodes Create a compartment for OKE Create Container Clusters (OKE) Create Bastion Node to access Cluster Setup OCI CLI to download kubeconfig and access OKE Cluster  Create Public SSH Key to access all the Bastion and Worker nodes Create SSH key using ssh-keygen on linux terminal to access (ssh) the Compute instances (worker/bastion) in OCI.\nssh-keygen -t rsa -N \u0026#34;\u0026#34; -b 2048 -C demokey -f id_rsa Create a compartment for OKE Within your tenancy, there must be a compartment to contain the necessary network resources (VCN, subnets, internet gateway, route table, security lists).\n Go to OCI console, and use the top-left Menu to select the Identity \u0026gt; Compartments option. Click the Create Compartment button. Enter the compartment name(For example, WCCStorage) and description(OKE compartment), the click the Create Compartment button.  Create Container Clusters (OKE)  In the Console, open the navigation menu. Go to Developer Services and click Kubernetes Clusters (OKE).  Choose a Compartment you have permission to work in. Here we will use WCCStorage compartment. On the Cluster List page, select your Compartment and click Create Cluster. In the Create Cluster dialog, select Quick Create and click Launch Workflow.  On the Create Cluster page specify the values as per your environment (like the sample values shown below)  NAME: WCCOKEPHASE1 COMPARTMENT: WCCStorage KUBERNETES VERSION: v1.18.10 CHOOSE VISIBILITY TYPE: Private SHAPE: VM.Standard.E3.Flex (Choose the available shape for worker node pool. The list shows only those shapes available in your tenancy that are supported by Container Engine for Kubernetes. See Supported Images and Shapes for Worker Nodes.) NUMBER OF NODES: 3 (The number of worker nodes to create in the node pool, placed in the regional subnet created for the \u0026lsquo;quick cluster\u0026rsquo;). Click Show Advanced Options and enter PUBLIC SSK KEY: ssh-rsa AA\u0026hellip;\u0026hellip;bmVnWgX/ demokey (The public key id_rsa.pub created at Step1)    Click Next to review the details you entered for the new cluster.\n Click Create Cluster to create the new network resources and the new cluster.  Container Engine for Kubernetes starts creating resources (as shown in the Creating cluster and associated network resources dialog). Click Close to return to the Console.  Initially, the new cluster appears in the Console with a status of Creating. When the cluster has been created, it has a status of Active.  Click on the Node Pools on Resources and then View to view the Node Pool and worker node status  You can view the status of Worker node and make sure all Node State in Active and Kubernetes Node Condition is Ready.The worker node gets listed in the kubectl command once the Kubernetes Node Condition is Ready.  To access the Cluster, Click on Access Cluster on the Cluster WCCOKEPHASE1 page.  We will be creating the bastion node and then access the Cluster.  Create Bastion Node to access Cluster Setup a bastion node for accessing internal resources. We will create the bastion node in same VCN following below steps, so that we can ssh into worker nodes. Here we will choose CIDR Block: 10.0.22.0/24 . You can choose a different block, if you want.\n  Click on the VCN Name from the Cluster Page as shown below   Next Click on Security List and then Create Security List   Create a bastion-private-sec-list security with below Ingress and Egress Rules.\nIngress Rules:\nEgress Rules:   Create a bastion-public-sec-list security with below Ingress and Egress Rules.\nIngress Rules:\nEgress Rules:   Create the bastion-route-table with Internet Gateway, so that we can add to bastion instance for internet access   Next create a Regional Public Subnet for bastion instance with name bastion-subnet with below details:\n CIDR BLOCK: 10.0.22.0/24 ROUTE TABLE: oke-bastion-routetables SUBNET ACCESS: PUBLIC SUBNET Security List: bastion-public-sec-list DHCP OPTIONS: Select the Default DHCP Options     Next Click on the Private Subnet which has Worker Nodes   And then add the bastion-private-sec-list to Worker Private Subnet, so that bastion instance can access the Worker nodes   Next Create Compute Instance oke-bastion with below details\n Name: BastionHost Image: Oracle Linux 7.X Availability Domain: Choose any AD which has limit for creating Instance VIRTUAL CLOUD NETWORK COMPARTMENT: WCCStorage( i.e., OKE Compartment) SELECT A VIRTUAL CLOUD NETWORK: Select VCN created by Quick Cluster SUBNET COMPARTMENT: WCCStorage ( i.e., OKE Compartment) SUBNET: bastion-subnet (create above) SELECT ASSIGN A PUBLIC IP ADDRESS SSH KEYS: Copy content of id_rsa.pub created in Step1     Once bastion Instance BastionHost is created, get the Public IP to ssh into the bastion instance   Login to bastion host as below\nssh -i \u0026lt;your_ssh_bastion.key\u0026gt; opc@123.456.xxx.xxx   Setup OCI CLI  Install OCI CLI bash -c \u0026#34;$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34;  Respond to the Installation Script Prompts. To download the kubeconfig later after setup, we need to setup the oci config file. Follow the below command and enter the details when prompted $ oci setup config    Click here to see the Sample Output   $ oci setup config This command provides a walkthrough of creating a valid CLI config file. The following links explain where to find the information required by this script: User API Signing Key, OCID and Tenancy OCID: https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#Other Region: https://docs.cloud.oracle.com/Content/General/Concepts/regions.htm General config documentation: https://docs.cloud.oracle.com/Content/API/Concepts/sdkconfig.htm Enter a location for your config [/home/opc/.oci/config]: Enter a user OCID: ocid1.user.oc1..aaaaaaaao3qji52eu4ulgqvg3k4yf7xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Enter a tenancy OCID: ocid1.tenancy.oc1..aaaaaaaaf33wodv3uhljnn5etiuafoxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Enter a region (e.g. ap-hyderabad-1, ap-melbourne-1, ap-mumbai-1, ap-osaka-1, ap-seoul-1, ap-sydney-1, ap-tokyo-1, ca-montreal-1, ca-toronto-1, eu-amsterdam-1, eu-frankfurt-1, eu-zurich-1, me-jeddah-1, sa-saopaulo-1, uk-gov-london-1, uk-london-1, us-ashburn-1, us-gov-ashburn-1, us-gov-chicago-1, us-gov-phoenix-1, us-langley-1, us-luke-1, us-phoenix-1): us-phoenix-1 Do you want to generate a new API Signing RSA key pair? (If you decline you will be asked to supply the path to an existing key.) [Y/n]: Y Enter a directory for your keys to be created [/home/opc/.oci]: Enter a name for your key [oci_api_key]: Public key written to: /home/opc/.oci/oci_api_key_public.pem Enter a passphrase for your private key (empty for no passphrase): Private key written to: /home/opc/.oci/oci_api_key.pem Fingerprint: 74:d2:f2:db:62:a9:c4:bd:9b:4f:6c:d8:31:1d:a1:d8 Config written to /home/opc/.oci/config If you haven't already uploaded your API Signing public key through the console, follow the instructions on the page linked below in the section 'How to upload the public key': https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#How2     Now you need to upload the created public key in $HOME/.oci (oci_api_key_public.pem) to OCI console Login to OCI Console and navigate to User Settings, which is in the drop down under your OCI userprofile, located at the top-right corner of the page.  On User Details page, Click Api Keys link, located near bottom-left corner of the page and then Click the Add API Key button. Copy the content of oci_api_key_public.pem and Click Add.  Now you can use the oci cli to access the OCI resources. To access the Cluster, Click on Access Cluster on the Cluster WCCOKEPHASE1 page  To access the Cluster from Bastion node perform steps as per the Local Access. $ oci -v $ mkdir -p $HOME/.kube $ oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.phx.aaaaaaaaae4xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxrqgjtd --file $HOME/.kube/config --region us-phoenix-1 --token-version 2.0.0 $ export KUBECONFIG=$HOME/.kube/config  Install kubectl Client to access the Cluster $ curl -LO https://dl.k8s.io/release/v1.15.7/bin/linux/amd64/kubectl $ sudo mv kubectl /bin/ $ sudo chmod +x /bin/kubectl  Access the Cluster from bastion node $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.197 Ready node 14d v1.18.10 10.0.10.206 Ready node 14d v1.18.10 10.0.10.50 Ready node 14d v1.18.10  Install required add-ons for Oracle WebCenter Content Cluster setup  Install helm v3 $ wget https://get.helm.sh/helm-v3.1.1-linux-amd64.tar.gz $ tar -zxvf helm-v3.1.1-linux-amd64.tar.gz $ sudo mv linux-amd64/helm /bin/helm $ helm version version.BuildInfo{Version:\u0026#34;v3.1.1\u0026#34;, GitCommit:\u0026#34;afe70585407b420d0097d07b21c47dc511525ac8\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.13.8\u0026#34;}  Install git sudo yum install git -y     "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/appendix/wcc-cluster-sizing-info/",
	"title": "Domain resource sizing",
	"tags": [],
	"description": "Describes the resourse sizing information for Oracle WebCenter Content domain setup on Kubernetes cluster.",
	"content": "Oracle WebCenter Content cluster sizing recommendations    Oracle WCC Normal Usage Moderate Usage High Usage     Administration Server No of CPU core(s) : 1, Memory : 4GB No of CPU core(s) : 1, Memory : 4GB No of CPU core(s) : 1, Memory : 4GB   Number of Managed Servers 2 3 5   Configurations per Managed Server No of CPU core(s) : 2, Memory : 16GB No of CPU core(s) : 4, Memory : 16GB No of CPU core(s) : 6, Memory : 16-32GB   PV Storage Minimum 250GB Minimum 250GB Minimum 500GB    "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/adminguide/configure-load-balancer/",
	"title": "Set up a load balancer",
	"tags": [],
	"description": "Configure different load balancers for Oracle WebCenter Content domains.",
	"content": "WebLogic Kubernetes Operator supports ingress-based load balancers such as Traefik and NGINX (kubernetes/ingress-nginx). It also supports Apache webtier load balancer.\n Traefik  Configure the ingress-based Traefik load balancer for Oracle WebCenter Content domains.\n NGINX  Configure the ingress-based NGINX load balancer for Oracle WebCenter Content domain.\n Apache webtier  Configure the Apache webtier load balancer for Oracle WebCenter Content domain.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/oracle-cloud/filesystem/",
	"title": "Preparing a file system",
	"tags": [],
	"description": "Running WebLogic Kubernetes Operator managed Oracle WebCenter Content domains on OKE",
	"content": "Create Filesystem and security list for FSS  Note: Make sure you create the filesystem and security list in the OKE created VCN\n   Login to OCI Console and go to Storage and Click File System   Click Create File System   You can create File System and Mount Targets with the default values. But in case you want to rename the file System and mount targets, follow below steps.\n Note: Make sure the Virtual Cloud Network in Mount Target refers to the one where your OKE Cluster is created and you will be accessing this file system.\n   Edit and change the File System name. You can choose any name of your choice. Following instructions will assume that the File System name chosen is WCCFS.   Edit and change the Mount Target name to WCCFS and make sure the Virtual Cloud Network selected is the one where all the instances are created. Select Public Subnet and Click Create   Once the File System is created, it lands at below page. Click on WCCFS link.   Click on Mount Commands which gives details on how to mount this file system on your instances.   Mount Command pop up gives details on what must be configured on security list to access the mount targets from instances. Note down the mount command which need to be executed on the instance   Note down the mount path and NFS server from the COMMAND TO MOUNT THE FILE SYSTEM. We will use this as NFS for Domain Home with below details. Sample from the above mount command.\n NFSServer: 10.0.20.xxx Mount Path: /WCCFS    Create the security list fss_seclist with below Ingress Rules as given in the Mount commands pop up   Create the Egress rules as below as given in the Mount commands pop up.   Make sure to add the created security list fss_security list to each subnets as shown below: Otherwise the created security list rules will not apply to the instances.   Once the security list fss_security list is added into the subnet, login to the instances and mount the file systems on to Bastion Node.\n Note: Please make sure to replace the sample NFS server address (10.0.20.235, as shown in the example below) according to your environment.\n # Run below command in same order(sequence) as a root user. # login as root sudo su # Install NFS Utils yum install nfs-utils # Create directory where you want the mount the file system sudo mkdir -p /mnt/WCCFS # Mount Command sudo mount 10.0.20.235:/WCCFS /mnt/WCCFS # Alternatively you can use: \u0026quot;mount 10.0.20.235:/WCCFS /mnt/WCCFS\u0026quot;. To persist on reboot add into /etc/fstab echo \u0026quot;10.0.20.235:/WCCFS /mnt/WCCFS nfs nfsvers=3 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab mount -a # Change proper permissions so that all users can access the share volume sudo chown -R 1000:0 /mnt/WCCFS   Confirm that /WCCFS is now pointing to created File System\n[root@bastionhost WCCFS]# cd /mnt/WCCFS/ [root@bastionhost WCCFS]# df -h . Filesystem Size Used Avail Use% Mounted on 10.0.20.235:/WCCFS 8.0E 0 8.0E 0% /mnt/WCCFS   "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/oracle-cloud/ocir/",
	"title": "Preparing OCIR",
	"tags": [],
	"description": "Running WebLogic Kubernetes Operator managed Oracle WebCenter Content domains on OKE",
	"content": "Publish images to OCIR Push all the required images to OCIR and subsequently use from there. Follow the below steps for pushing the images to OCIR\nCreate an \u0026ldquo;Auth token\u0026rdquo; Create an \u0026ldquo;Auth token\u0026rdquo; which will be used as docker password to push and pull images from OCIR. Login to OCI Console and navigate to User Settings, which is in the drop down under your OCI user-profile, located at the top-right corner of the OCI console page.  On User Details page, Click Auth Tokens link located near bottom-left corner of the page and then Click the Generate Token button: Enter a Name and Click \u0026ldquo;Generate Token\u0026rdquo;  Token will get generated  Copy the generated token.  NOTE: It will only be displayed this one time, and you will need to copy it to a secure place for further use.\n   Using the OCIR Using the Docker CLI to login to OCIR ( for phoenix : phx.ocir.io , ashburn: iad.ocir.io etc)\n docker login phx.ocir.io When promoted for username enter docker username as OCIR RepoName/oci username ( eg., axcmmdmzqtqb/oracleidentitycloudservice/myemailid@oracle.com) When prompted for your password, enter the generated Auth Token Now you can tag the WCC Docker image and push to OCIR. Sample steps as below  $ docker login phx.ocir.io $ username - axcmmdmzqtqb/oracleidentitycloudservice/myemailid@oracle.com $ password - abCXYz942,vcde (Token Generated for OCIR using user setting) $ docker tag oracle/wccontent:12.2.1.4.0-20210311104247 phx.ocir.io/axcmmdmzqtqb/oracle/wccontent:12.2.1.4.0-20210311104247 $ docker push phx.ocir.io/axcmmdmzqtqb/oracle/wccontent:12.2.1.4.0-20210311104247 This has to be done on Bastion Node for all the images.\nVerify the OCIR Images Get the OCIR repository name by logging in to Oracle Cloud Infrastructure Console. In the OCI Console, open the Navigation menu. Under Solutions and Platform, go to Developer Services and click Container Registry (OCIR) and select the your Compartment.\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/",
	"title": "Oracle Fusion Middleware on Kubernetes",
	"tags": [],
	"description": "This document lists all the Oracle Fusion Middleware products deployment supported on Kubernetes.",
	"content": "Oracle Fusion Middleware on Kubernetes Oracle supports the deployment of the following Oracle Fusion Middleware products on Kubernetes. Click on the appropriate document link below to get started on setting up the product.\n Oracle WebCenter Content  WebLogic Kubernetes Operator (the “operator”) supports deployment of Oracle WebCenter Content servers such as Oracle WebCenter Content(Content Server) and Oracle WebCenter Content(Inbound Refinery Server). Follow the instructions in this guide to set up Oracle WebCenter Content domain on Kubernetes.\n Oracle WebCenter Portal  The WebLogic Kubernetes operator (the “operator”) supports deployment of Oracle WebCenter Portal. Follow the instructions in this guide to set up Oracle WebCenter Portal domain on Kubernetes.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/appendix/wcp-cluster-sizing-info/",
	"title": "Domain resource sizing",
	"tags": [],
	"description": "Describes the resourse sizing information for the Oracle WebCenter Portal domain setup on Kubernetes cluster.",
	"content": "Oracle WebCenter Portal cluster sizing recommendations    WebCenter Portal Normal Usage Moderate Usage High Usage     Admin Server No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB   Number of Managed Server No of Servers : 2 No of Servers : 2 No of Servers : 3   Configurations per Managed Server No of CPU(s) : 2, Memory : 16GB No of CPU(s) : 4, Memory : 16GB No of CPU(s) : 6, Memory : 16-32GB   PV Storage Minimum 250GB Minimum 250GB Minimum 500GB    "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/manage-wcportal-domains/configure-load-balancer/",
	"title": "Set up a load balancer",
	"tags": [],
	"description": "Configure different load balancers for the Oracle WebCenter Portal domain.",
	"content": "The WebLogic Kubernetes Operator supports ingress-based load balancers such as Traefik and NGINX (kubernetes/ingress-nginx) . It also supports the Apache webtier load balancer.\n Traefik  Configure the ingress-based Traefik load balancer for an Oracle WebCenter Portal domain.\n NGINX  Configure the ingress-based NGINX load balancer for an Oracle WebCenter Portal domain.\n Apache webtier  Configure the Apache webtier load balancer for an Oracle WebCenter Portal domain.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/manage-wcportal-domains/configure-load-balancer/traefik/",
	"title": "Traefik",
	"tags": [],
	"description": "Configure the ingress-based Traefik load balancer for an Oracle WebCenter Portal domain.",
	"content": "To load balance Oracle WebCenter Portal domain clusters, you can install the ingress-based Traefik load balancer (version 2.2.1 or later for production deployments) and configure it for non-SSL, SSL termination, and end-to-end SSL access of the application URL. Follow these steps to set up Traefik as a load balancer for an Oracle WebCenter Portal domain in a Kubernetes cluster:\n  Non-SSL and SSL termination\n Install the Traefik (ingress-based) load balancer Configure Traefik to manage ingresses Create an Ingress for the domain Verify domain application URL access Uninstall the Traefik ingress    End-to-end SSL configuration\n Install the Traefik load balancer for End-to-end SSL Configure Traefik to manage domain Create IngressRouteTCP Verify end-to-end SSL access Uninstall Traefik    Non-SSL and SSL termination Install the Traefik (ingress-based) load balancer   Use Helm to install the Traefik (ingress-based) load balancer. You can use the following values.yaml sample file and set kubernetes.namespaces as required.\n$ cd ${WORKDIR} $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ helm install traefik traefik/traefik \\  --namespace traefik \\  --values charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=NodePort\u0026#34; --wait    Click here to see the sample output.   LAST DEPLOYED: Sun Sep 13 21:32:00 2020 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None    A sample values.yaml for deployment of Traefik 2.2.x looks like this:\nimage: name: traefik tag: 2.2.8 pullPolicy: IfNotPresent ingressRoute: dashboard: enabled: true annotations: {} labels: {} providers: kubernetesCRD: enabled: true kubernetesIngress: enabled: true ports: traefik: port: 9000 expose: true exposedPort: 9000 protocol: TCP web: port: 8000 expose: true exposedPort: 30305 nodePort: 30305 protocol: TCP websecure: port: 8443 expose: true exposedPort: 30443 protocol: TCP nodePort: 30443   Verify the Traefik status and find the port number of the SSL and non-SSL services:\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-f9cf58697-29dlx 1/1 Running 0 35s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik NodePort 10.100.113.37 \u0026lt;none\u0026gt; 9000:30070/TCP,30305:30305/TCP,30443:30443/TCP 35s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 36s NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-f9cf58697 1 1 1 36s      Access the Traefik dashboard through the URL http://$(hostname -f):30070, with the HTTP host traefik.example.com:\n$ curl -H \u0026#34;host: $(hostname -f)\u0026#34; http://$(hostname -f):30070/dashboard/  Note: Make sure that you specify a fully qualified node name for $(hostname -f)\n   Configure Traefik to manage ingresses Configure Traefik to manage ingresses created in this namespace. In the following sample, traefik is the Traefik namespace and wcpns is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik \\ --reuse-values \\ --namespace traefik \\ --set \u0026#34;kubernetes.namespaces={traefik,wcpns}\u0026#34; \\ --wait    Click here to see the sample output.   Release \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Tue Jan 12 04:33:15 2021 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create an ingress for the domain Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK , tls is Non-SSL. You can override these values by passing values through the command line or edit them in the sample values.yaml file based on the type of configuration (non-SSL or SSL).\n NOTE: This is not an exhaustive list of rules. You can enhance it based on the application URLs that need to be accessed externally.\n If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. The template YAML file for the Traefik (ingress-based) load balancer is located at ${WORKDIR}/charts/ingress-per-domain/templates/traefik-ingress.yaml You can add new path rules like shown below .\n- path: /NewPathRule backend: serviceName: \u0026#39;Backend Service Name\u0026#39; servicePort: \u0026#39;Backend Service Port\u0026#39;   Install ingress-per-domain using Helm for non-SSL configuration:\n$ cd ${WORKDIR} $ helm install wcp-traefik-ingress \\  charts/ingress-per-domain \\  --namespace wcpns \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; Sample output:\nNAME: wcp-traefik-ingress LAST DEPLOYED: Mon Jul 20 11:44:13 2020 NAMESPACE: wcpns STATUS: deployed REVISION: 1 TEST SUITE: None   For secured access (SSL) to the Oracle WebCenter Portal application, create a certificate and generate a Kubernetes secret:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n wcpns create secret tls wcp-domain-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt  Note: The value of CN is the host on which this ingress is to be deployed.\n   Create the Traefik TLSStore custom resource.\nIn case of SSL termination, Traefik should be configured to use the user-defined SSL certificate. If the user-defined SSL certificate is not configured, Traefik creates a default SSL certificate. To configure a user-defined SSL certificate for Traefik, use the TLSStore custom resource. The Kubernetes secret created with the SSL certificate should be referenced in the TLSStore object. Run the following command to create the TLSStore:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: traefik.containo.us/v1alpha1 kind: TLSStore metadata: name: default namespace: wcpns spec: defaultCertificate: secretName: wcp-domain-tls-cert EOF   Install ingress-per-domain using Helm for SSL configuration.\nThe Kubernetes secret name should be updated in the template file.\nThe template file also contains the following annotations:\ntraefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.tls: \u0026#34;true\u0026#34; traefik.ingress.kubernetes.io/router.middlewares: wcpns-wls-proxy-ssl@kubernetescrd The entry point for SSL access and the Middleware name should be updated in the annotation. The Middleware name should be in the form \u0026lt;namespace\u0026gt;-\u0026lt;middleware name\u0026gt;@kubernetescrd.\n$ cd ${WORKDIR} $ helm install wcp-traefik-ingress \\  charts/ingress-per-domain \\  --namespace wcpns \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; \\  --set sslType=SSL Sample output:\nNAME: wcp-traefik-ingress LAST DEPLOYED: Mon Jul 20 11:44:13 2020 NAMESPACE: wcpns STATUS: deployed REVISION: 1 TEST SUITE: None   For non-SSL access to the Oracle WebCenter Portal application, get the details of the services by the ingress:\n$ kubectl describe ingress wcp-domain-traefik -n wcpns    Click here to see all services supported by the above deployed ingress.   Name: wcp-domain-traefik Namespace: wcpns Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- www.example.com /webcenter wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /console wcp-domain-adminserver:7001 (10.244.0.51:7001) /rsscrawl wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /rest wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /webcenterhelp wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /em wcp-domain-adminserver:7001 (10.244.0.51:7001) /wsrp-tools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.52:8889,10.244.0.53:8889) /portalTools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.52:8889,10.244.0.53:8889) Annotations: kubernetes.io/ingress.class: traefik meta.helm.sh/release-name: wcp-traefik-ingress meta.helm.sh/release-namespace: wcpns Events: \u0026lt;none\u0026gt;      For SSL access to the Oracle WebCenter Portal application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress wcp-domain-traefik -n wcpns    Click here to see all services supported by the above deployed ingress.   Name: wcp-domain-traefik Namespace: wcpns Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) TLS: wcp-domain-tls-cert terminates www.example.com Rules: Host Path Backends ---- ---- -------- www.example.com /webcenter wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /console wcp-domain-adminserver:7001 (10.244.0.51:7001) /rsscrawl wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /rest wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /webcenterhelp wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /em wcp-domain-adminserver:7001 (10.244.0.51:7001) /wsrp-tools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.52:8889,10.244.0.53:8889) /portalTools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.52:8889,10.244.0.53:8889) Annotations: kubernetes.io/ingress.class: traefik meta.helm.sh/release-name: wcp-traefik-ingress meta.helm.sh/release-namespace: wcpns traefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.middlewares: wcpns-wls-proxy-ssl@kubernetescrd traefik.ingress.kubernetes.io/router.tls: true Events: \u0026lt;none\u0026gt;      To confirm that the load balancer noticed the new ingress and is successfully routing to the domain server pods, you can send a request to the URL for the WebLogic ReadyApp framework, which should return an HTTP 200 status code, as follows:\n$ curl -v http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_PORT}/weblogic/ready * Trying 149.87.129.203... \u0026gt; GET http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_PORT}/weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; Proxy-Connection: Keep-Alive \u0026gt; host: $(hostname -f) \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Sat, 14 Mar 2020 08:35:03 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; Content-Length: 0 \u0026lt; Proxy-Connection: Keep-Alive \u0026lt; * Connection #0 to host localhost left intact   Verify domain application URL access For non-SSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain application URLs are accessible through the non-SSL load balancer port 30305 for HTTP access. The sample URLs for Oracle WebCenter Portal domain are:\nhttp://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenter http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rsscrawl http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rest http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenterhelp http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/wsrp-tools http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/portalTools For SSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain applications are accessible through the SSL load balancer port 30443 for HTTPS access. The sample URLs for Oracle WebCenter Portal domain are:\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenter https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/rsscrawl https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/rest https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenterhelp https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/wsrp-tools https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/portalTools Uninstall the Traefik ingress Uninstall and delete the ingress deployment:\n$ helm delete wcp-traefik-ingress -n wcpns End-to-end SSL configuration Install the Traefik load balancer for end-to-end SSL   Use Helm to install the Traefik (ingress-based) load balancer. You can use the values.yaml sample file and set kubernetes.namespaces as required.\n$ cd ${WORKDIR} $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ helm install traefik traefik/traefik \\  --namespace traefik \\  --values charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=NodePort\u0026#34; --wait    Click here to see the sample output.   LAST DEPLOYED: Sun Sep 13 21:32:00 2020 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None      Verify the Traefik operator status and find the port number of the SSL and non-SSL services:\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-845f5d6dbb-swb96 1/1 Running 0 32s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik NodePort 10.99.52.249 \u0026lt;none\u0026gt; 9000:31288/TCP,30305:30305/TCP,30443:30443/TCP 32s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 33s NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-845f5d6dbb 1 1 1 33s      Access the Traefik dashboard through the URL http://$(hostname -f):31288, with the HTTP host traefik.example.com:\n$ curl -H \u0026#34;host: $(hostname -f)\u0026#34; http://$(hostname -f):31288/dashboard/  Note: Make sure that you specify a fully qualified node name for $(hostname -f).\n   Configure Traefik to manage the domain Configure Traefik to manage the domain application service created in this namespace. In the following sample, traefik is the Traefik namespace and wcpns is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\ --set \u0026#34;kubernetes.namespaces={traefik,wcpns}\u0026#34;    Click here to see the sample output.   Release \u0026quot;traefik\u0026quot; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Sun Sep 13 21:32:12 2020 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create IngressRouteTCP   For each backend service, create different ingresses, as Traefik does not support multiple paths or rules with annotation ssl-passthrough. For example, for wcp-domain-adminserver and wcp-domain-cluster-wcp-cluster, different ingresses must be created.\n  To enable SSL passthrough in Traefik, you can configure a TCP router. A sample YAML for IngressRouteTCP is available at ${WORKDIR}/charts/ingress-per-domain/tls/traefik-tls.yaml. The following should be updated in traefik-tls.yaml:\n The service name and the SSL port should be updated in the services. The load balancer host name should be updated in the HostSNI rule.  Sample traefik-tls.yaml:\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: wcp-domain-cluster-routetcp namespace: wcpns spec: entryPoints: - websecure routes: - match: HostSNI(`${LOADBALANCER_HOSTNAME}`) services: - name: wcp-domain-cluster-wcp-cluster port: 8888 weight: 3 TerminationDelay: 400 tls: passthrough: true   Create the IngressRouteTCP:\n$ kubectl apply -f traefik-tls.yaml   Verify end-to-end SSL access Verify the access to application URLs exposed through the configured service. The configured WCP cluster service enables you to access the following WCP domain URLs:\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenter https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/rsscrawl https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/rest https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenterhelp Uninstall Traefik $ helm delete traefik -n traefik $ cd ${WORKDIR}/charts/ingress-per-domain/tls $ kubectl delete -f traefik-tls.yaml "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/publishing-logs/fluentd/",
	"title": " Fluentd",
	"tags": [],
	"description": "Describes how to configure a WebCenter Portal domain to use Fluentd to send log information to Elasticsearch.",
	"content": "Overview You can configure your WebLogic domain to use Fluentd so it can send the log information to Elasticsearch.\nHere\u0026rsquo;s how this works:\n fluentd runs as a separate container in the Administration Server and Managed Server pods. The log files reside on a volume that is shared between the weblogic-server and fluentd containers. fluentd tails the domain logs files and exports them to Elasticsearch. A ConfigMap contains the filter and format rules for exporting log records.  Prerequisites It is assumed that you are editing an existing WebCenter Portal domain. However, you can make all the changes to the domain YAML before creating the domain. A complete example of a domain definition with fluentd configuration is at the end of this document.\nThese identifiers are used in the sample commands.\n wcpns: WebCenter Portal domain namespace wcp-domain: domainUID wcp-domain-domain-credentials: Kubernetes secret   The sample Elasticsearch configuration is:\nelasticsearchhost: elasticsearch.wcp-domain.sample.com elasticsearchport: 443 elasticsearchuser: username elasticsearchpassword: password Install Elasticsearch and Kibana To install Elasticsearch and Kibana, run the following command:\n$ kubectl apply -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml Configure log files to use a volume The domain log files must be written to a volume that can be shared between the weblogic-server and fluentd containers. The following elements are required to accomplish this:\n logHome must be a path that can be shared between containers. logHomeEnabled must be set to true so that the logs are written outside the pod and persist across pod restarts. A volume must be defined on which the log files will reside. In the example, emptyDir is a volume that gets created when a Pod is created. It will persist across pod restarts but deleting the pod would delete the emptyDir content. The volumeMounts mounts the named volume created with emptyDir and establishes the base path for accessing the volume.  NOTE: For brevity, only the paths to the relevant configuration are here.\nFor Example, run : kubectl edit domain wcp-domain -n wcpns and make the following edits:\nspec: logHome: /u01/oracle/user_projects/domains/logs/wcp-domain logHomeEnabled: true serverPod: volumes: - emptyDir: {} name: weblogic-domain-storage-volume volumeMounts: - mountPath: /scratch name: weblogic-domain-storage-volume Add Elasticsearch secrets to WebLogic domain credentials Configure the fluentd container to look for Elasticsearch parameters in the domain credentials. Edit the domain credentials and add the parameters shown in the example below.\nFor example, run: kubectl edit secret wcp-domain-domain-credentials -n wcpns and add the base64 encoded values of each Elasticsearch parameter:\nelasticsearchhost: ZWxhc3RpY3NlYXJjaC5ib2JzLWJvb2tzLnNhbXBsZS5jb20= elasticsearchport: NDQz elasticsearchuser: Ym9i elasticsearchpassword: d2VsY29tZTE= Create Fluentd configuration Create a ConfigMap named fluentd-config in the namespace of the domain. The ConfigMap contains the parsing rules and Elasticsearch configuration.\nHere\u0026rsquo;s an explanation of some elements defined in the ConfigMap:\n The @type tail indicates that tail is used to obtain updates to the log file. The path of the log file obtained from the LOG_PATH environment variable that is defined in the fluentd container. The tag value of log records obtained from the DOMAIN_UID environment variable that is defined in the fluentd container. The \u0026lt;parse\u0026gt; section defines how to interpret and tag each element of a log record. The \u0026lt;match **\u0026gt; section contains the configuration information for connecting to Elasticsearch and defines the index name of each record to be the domainUID. The scheme indicates type of connection between fluentd and Elasticsearch.  The following is an example of how to create the ConfigMap:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: labels: weblogic.domainUID: wcp-domain weblogic.resourceVersion: domain-v2 name: fluentd-config namespace: wcpns data: fluentd.conf: | \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; \u0026lt;source\u0026gt; @type tail path \u0026#34;#{ENV[\u0026#39;LOG_PATH\u0026#39;]}\u0026#34; pos_file /tmp/server.log.pos read_from_head true tag \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; # multiline_flush_interval 20s \u0026lt;parse\u0026gt; @type multiline format_firstline /^####/ format1 /^####\u0026lt;(?\u0026lt;timestamp\u0026gt;(.*?))\u0026gt;/ format2 / \u0026lt;(?\u0026lt;level\u0026gt;(.*?))\u0026gt;/ format3 / \u0026lt;(?\u0026lt;subSystem\u0026gt;(.*?))\u0026gt;/ format4 / \u0026lt;(?\u0026lt;serverName\u0026gt;(.*?))\u0026gt;/ format5 / \u0026lt;(?\u0026lt;serverName2\u0026gt;(.*?))\u0026gt;/ format6 / \u0026lt;(?\u0026lt;threadName\u0026gt;(.*?))\u0026gt;/ format7 / \u0026lt;(?\u0026lt;info1\u0026gt;(.*?))\u0026gt;/ format8 / \u0026lt;(?\u0026lt;info2\u0026gt;(.*?))\u0026gt;/ format9 / \u0026lt;(?\u0026lt;info3\u0026gt;(.*?))\u0026gt;/ format10 / \u0026lt;(?\u0026lt;sequenceNumber\u0026gt;(.*?))\u0026gt;/ format11 / \u0026lt;(?\u0026lt;severity\u0026gt;(.*?))\u0026gt;/ format12 / \u0026lt;(?\u0026lt;messageID\u0026gt;(.*?))\u0026gt;/ format13 / \u0026lt;(?\u0026lt;message\u0026gt;(.*?))\u0026gt;/ \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;match **\u0026gt; @type elasticsearch host \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_HOST\u0026#39;]}\u0026#34; port \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PORT\u0026#39;]}\u0026#34; user \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_USER\u0026#39;]}\u0026#34; password \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PASSWORD\u0026#39;]}\u0026#34; index_name \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; scheme http \u0026lt;/match\u0026gt; EOF Mount the ConfigMap as a volume in the weblogic-server container Edit the domain definition and configure a volume for the ConfigMap containing the fluentd configuration.\nNOTE: For brevity, only the paths to the relevant configuration are shown.\nFor example, run: kubectl edit domain wcp-domain -n wcpns and add the following portions to the domain definition.\nspec: serverPod: volumes: - configMap: defaultMode: 420 name: fluentd-config name: fluentd-config-volume Add fluentd container Add a container to the domain to run fluentd in the Administration Server and Managed Server pods.\nThe container definition:\n Defines a LOG_PATH environment variable that points to the log location of bobbys-front-end. Defines ELASTICSEARCH_HOST, ELASTICSEARCH_PORT, ELASTICSEARCH_USER, and ELASTICSEARCH_PASSWORD environment variables that are all retrieving their values from the secret wcp-domain-domain-credentials. Includes volume mounts for the fluentd-config ConfigMap and the volume containing the domain logs.  NOTE: For brevity, only the paths to the relevant configuration are shown.\nFor example, run: kubectl edit domain wcp-domain -n wcpcns and add the following container definition.\nspec: serverPod: containers: - args: - -c - /etc/fluent.conf env: - name: DOMAIN_UID valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.domainUID\u0026#39;] - name: SERVER_NAME valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.serverName\u0026#39;] - name: LOG_PATH value: /u01/oracle/user_projects/domains/logs/wcp-domain/$(SERVER_NAME).log - name: FLUENTD_CONF value: fluentd.conf - name: FLUENT_ELASTICSEARCH_SED_DISABLE value: \u0026#34;true\u0026#34; - name: ELASTICSEARCH_HOST valueFrom: secretKeyRef: key: elasticsearchhost name: wcp-domain-domain-credentials - name: ELASTICSEARCH_PORT valueFrom: secretKeyRef: key: elasticsearchport name: wcp-domain-domain-credentials - name: ELASTICSEARCH_USER valueFrom: secretKeyRef: key: elasticsearchuser name: wcp-domain-domain-credentials optional: true - name: ELASTICSEARCH_PASSWORD valueFrom: secretKeyRef: key: elasticsearchpassword name: wcp-domain-domain-credentials optional: true image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 imagePullPolicy: IfNotPresent name: fluentd resources: {} volumeMounts: - mountPath: /fluentd/etc/fluentd.conf name: fluentd-config-volume subPath: fluentd.conf - mountPath: /scratch name: weblogic-domain-storage-volume Verify logs exported to Elasticsearch The logs are sent to Elasticsearch after you start the Administration Server and Managed Server pods after making the changes described previously.\nYou can check if the fluentd container is successfully tailing the log by executing a command like kubectl logs -f wcp-domain-adminserver -n wcpns fluentd. The log output should look similar to this:\n2019-10-01 16:23:44 +0000 [info]: #0 starting fluentd worker pid=13 ppid=9 worker=0 2019-10-01 16:23:44 +0000 [warn]: #0 /scratch/logs/bobs-bookstore/managed-server1.log not found. Continuing without tailing it. 2019-10-01 16:23:44 +0000 [info]: #0 fluentd worker is now running worker=0 2019-10-01 16:24:01 +0000 [info]: #0 following tail of /scratch/logs/bobs-bookstore/managed-server1.log When you connect to Kibana, you will see an index created for the domainUID.\nDomain example The following is a complete example of a domain custom resource with a fluentd container configured.\napiVersion: weblogic.oracle/v8 kind: Domain metadata: labels: weblogic.domainUID: wcp-domain name: wcp-domain namespace: wcpns spec: domainHome: /u01/oracle/user_projects/domains/wcp-domain domainHomeSourceType: PersistentVolume image: \u0026#34;oracle/wcportal:12.2.1.4\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; webLogicCredentialsSecret: name: wcp-domain-domain-credentials includeServerOutInPodLog: true logHomeEnabled: true httpAccessLogInLogHome: true logHome: /u01/oracle/user_projects/domains/logs/wcp-domain dataHome: \u0026#34;\u0026#34; serverStartPolicy: \u0026#34;IF_NEEDED\u0026#34; adminServer: serverStartState: \u0026#34;RUNNING\u0026#34; clusters: - clusterName: wcp_cluster serverStartState: \u0026#34;RUNNING\u0026#34; serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026#34;weblogic.clusterName\u0026#34; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; replicas: 2 serverPod: containers: - args: - -c - /etc/fluent.conf env: - name: DOMAIN_UID valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.domainUID\u0026#39;] - name: SERVER_NAME valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.serverName\u0026#39;] - name: LOG_PATH value: /u01/oracle/user_projects/domains/logs/wcp-domain/$(SERVER_NAME).log - name: FLUENTD_CONF value: fluentd.conf - name: FLUENT_ELASTICSEARCH_SED_DISABLE value: \u0026#34;true\u0026#34; - name: ELASTICSEARCH_HOST valueFrom: secretKeyRef: key: elasticsearchport name: wcp-domain-domain-credentials - name: ELASTICSEARCH_PORT valueFrom: secretKeyRef: key: elasticsearchhost name: wcp-domain-domain-credentials - name: ELASTICSEARCH_USER valueFrom: secretKeyRef: key: elasticsearchuser name: wcp-domain-domain-credentials - name: ELASTICSEARCH_PASSWORD valueFrom: secretKeyRef: key: elasticsearchpassword name: wcp-domain-domain-credentials image: fluent/fluentd-kubernetes-daemonset:v1.11.5-debian-elasticsearch6-1.0 imagePullPolicy: IfNotPresent name: fluentd resources: {} volumeMounts: - mountPath: /fluentd/etc/fluentd.conf name: fluentd-config-volume subPath: fluentd.conf - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume env: - name: JAVA_OPTIONS value: -Dweblogic.StdoutDebugEnabled=false - name: USER_MEM_ARGS value: \u0026#39;-Djava.security.egd=file:/dev/./urandom -Xms1g -Xmx2g\u0026#39; volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wcp-domain-domain-pvc - emptyDir: {} name: weblogic-domain-storage-volume - configMap: defaultMode: 420 name: fluentd-config name: fluentd-config-volume serverStartPolicy: IF_NEEDED webLogicCredentialsSecret: name: wcp-domain-domain-credentials Get the Kibana dashboard port information as shown below: -bash-4.2$ kubectl get pods -w NAME READY STATUS RESTARTS AGE elasticsearch-8bdb7cf54-mjs6s 1/1 Running 0 4m3s kibana-dbf8964b6-n8rcj 1/1 Running 0 4m3s -bash-4.2$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.100.11.154 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 4m32s kibana NodePort 10.97.205.0 \u0026lt;none\u0026gt; 5601:31884/TCP 4m32s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 71d You can access the Kibana dashboard at http://mycompany.com:kibana-nodeport/. In our example, the node port is 31884.\nCreate an Index Pattern in Kibana Create an index pattern wcp-domain* in Kibana by navigating to the dashboard through the Management option. When the servers are started, the log data is shown on the Kibana dashboard.\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/monitoring-domain/",
	"title": " Monitor a WebCenter Portal domain",
	"tags": [],
	"description": "Monitor an WebCenter Portal instance using Prometheus and Grafana.",
	"content": "You can monitor a WebCenter Portal domain using Prometheus and Grafana by exporting the metrics from the domain instance using the WebLogic Monitoring Exporter. This sample shows you how to set up the WebLogic Monitoring Exporter to push the data to Prometheus.\nPrerequisites This document assumes that the Prometheus Operator is deployed on the Kubernetes cluster. If it is not already deployed, follow the steps below for deploying the Prometheus Operator.\nPrepare to use the setup monitoring script The sample scripts for setup monitoring for OracleWebCenterPortal domain are available at ${WORKDIR}/monitoring-service.\nYou must edit monitoring-inputs.yaml(or a copy of it) to provide the details of your domain. Refer to the configuration parameters below to understand the information that you must provide in this file.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Description Default     domainUID domainUID of the OracleWebCenterPortal domain. wcp-domain   domainNamespace Kubernetes namespace of the OracleWebCenterPortal domain. wcpns   setupKubePrometheusStack Boolean value indicating whether kube-prometheus-stack (Prometheus, Grafana and Alertmanager) to be installed true   additionalParamForKubePrometheusStack The script install\u0026rsquo;s kube-prometheus-stack with service.type as NodePort and values for service.nodePort as per the parameters defined in monitoring-inputs.yaml. Use additionalParamForKubePrometheusStack parameter to further configure with additional parameters as per values.yaml. Sample value to disable NodeExporter, Prometheus-Operator TLS support and Admission webhook support for PrometheusRules resources is --set nodeExporter.enabled=false --set prometheusOperator.tls.enabled=false --set prometheusOperator.admissionWebhooks.enabled=false    monitoringNamespace Kubernetes namespace for monitoring setup. monitoring   adminServerName Name of the Administration Server. AdminServer   adminServerPort Port number for the Administration Server inside the Kubernetes cluster. 7001   wcpClusterName Name of the wcpCluster. wcp_cluster   wcpManagedServerPort Port number of the managed servers in the wcpCluster. 8888   wlsMonitoringExporterTowcpCluster Boolean value indicating whether to deploy WebLogic Monitoring Exporter to wcpCluster. false   wcpPortletClusterName Name of the wcpPortletCluster. wcportlet-cluster   wcpManagedServerPort Port number of the Portlet managed servers in the wcpPortletCluster. 8889   wlsMonitoringExporterTowcpPortletCluster Boolean value indicating whether to deploy WebLogic Monitoring Exporter to wcpPortletCluster. false   exposeMonitoringNodePort Boolean value indicating if the Monitoring Services (Prometheus, Grafana and Alertmanager) is exposed outside of the Kubernetes cluster. false   prometheusNodePort Port number of the Prometheus outside the Kubernetes cluster. 32101   grafanaNodePort Port number of the Grafana outside the Kubernetes cluster. 32100   alertmanagerNodePort Port number of the Alertmanager outside the Kubernetes cluster. 32102   weblogicCredentialsSecretName Name of the Kubernetes secret which has Administration Server’s user name and password. wcp-domain-domain-credentials    Note that the values specified in the monitoring-inputs.yaml file will be used to install kube-prometheus-stack (Prometheus, Grafana and Alertmanager) and deploying WebLogic Monitoring Exporter into the OracleWebCenterPortal domain. Hence make the domain specific values to be same as that used during domain creation.\nRun the setup monitoring script Update the values in monitoring-inputs.yaml as per your requirement and run the setup-monitoring.sh script, specifying your inputs file:\n$ cd ${WORKDIR}/monitoring-service $ ./setup-monitoring.sh \\  -i monitoring-inputs.yaml The script will perform the following steps:\n Helm install prometheus-community/kube-prometheus-stack of version \u0026ldquo;16.5.0\u0026rdquo; if setupKubePrometheusStack is set to true. Deploys WebLogic Monitoring Exporter to Administration Server. Deploys WebLogic Monitoring Exporter to wcpCluster if wlsMonitoringExporterTowcpCluster is set to true. Deploys WebLogic Monitoring Exporter to wcpPortletCluster if wlsMonitoringExporterTowcpPortletCluster is set to true. Exposes the Monitoring Services (Prometheus at 32101, Grafana at 32100 and Alertmanager at 32102) outside of the Kubernetes cluster if exposeMonitoringNodePort is set to true. Imports the WebLogic Server Grafana Dashboard if setupKubePrometheusStack is set to true.  Verify the results The setup monitoring script will report failure if there was any error. However, verify that required resources were created by the script.\nVerify the kube-prometheus-stack To confirm that prometheus-community/kube-prometheus-stack was installed when setupKubePrometheusStack is set to true, run the following command:\n$ helm ls -n monitoring Sample output:\n$ helm ls -n monitoring NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION monitoring monitoring 1 2021-06-18 12:58:35.177221969 +0000 UTC deployed kube-prometheus-stack-16.5.0 0.48.0 $ Verify the Prometheus, Grafana and Alertmanager setup When exposeMonitoringNodePort was set to true, verify that monitoring services are accessible outside of the Kubernetes cluster:\n 32100 is the external port for Grafana and with credentials admin:admin 32101 is the external port for Prometheus 32102 is the external port for Alertmanager  Verify the service discovery of WebLogic Monitoring Exporter Verify whether prometheus is able to discover wls-exporter and collect the metrics:\n  Access the Prometheus dashboard at http://mycompany.com:32101/\n  Navigate to Status to see the Service Discovery details.\n  Verify that wls-exporter is listed in the discovered services.\n  Verify the WebLogic Server dashoard You can access the Grafana dashboard at http://mycompany.com:32100/.\n  Log in to Grafana dashboard with username: admin and password: admin.\n  Navigate to \u0026ldquo;WebLogic Server Dashboard\u0026rdquo; under General and verify.\nThis displays the WebLogic Server Dashboard.\n  Delete the monitoring setup To delete the monitoring setup created by Run the setup monitoring script, run the below command:\n$ cd ${WORKDIR}/monitoring-service $ ./delete-monitoring.sh \\  -i monitoring-inputs.yaml "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/patch_and_upgrade/patch-an-image/",
	"title": "Patch an image",
	"tags": [],
	"description": "Create a patched Oracle WebCenter Content image using the WebLogic Image Tool.",
	"content": "Oracle aims to release Oracle WebCenter Content images regularly with latest bundle and recommended interim patches in My Oracle Support (MOS). However, if there is a need to create images with new bundle and interim patches, you can build these images using WebLogic Image Tool.\nIf you have access to the Oracle WebCenter Content patches, you can patch an existing Oracle WebCenter Content image with a bundle patch and interim patches. It is recommended to use the WebLogic Image Tool to patch the Oracle WebCenter Content image.\n Recommendations:\n Use the WebLogic Image Tool create feature for patching the Oracle WebCenter Content Docker image with a bundle patch and multiple interim patches. This is the recommended approach because it optimizes the size of the image. Use the WebLogic Image Tool update feature for patching the Oracle WebCenter Content Docker image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Apply the patched image   Update the image: field in domain.yaml configuration file with the patched image.\n  Apply the updated domain.yaml configuration file:\n$ kubectl apply -f domain.yaml    Note: The server pods will be automatically restarted (rolling restart).\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Review the latest changes for Oracle WebCenter Content on Kubernetes.\nRecent changes    Date Version Change     June 28, 2022 22.2.3 Supports Oracle WebCenter Content 12.2.1.4 domains deployment using May 2022 PSU and known bug fixes - certified for Oracle WebLogic Kubernetes Operator version 3.3.0. Oracle WebCenter Content 12.2.1.4 container image for this release can be downloaded from My Oracle Support (MOS patch 34192566).   March 11, 2022 22.1.3 Supports Oracle WebCenter Content 12.2.1.4 domains deployment using January 2022 PSU and known bug fixes - certified for Oracle WebLogic Kubernetes Operator version 3.3.0. Oracle WebCenter Content 12.2.1.4 container image for this release can be downloaded from My Oracle Support (MOS patch 33771196).   December 7, 2021 21.4.3 Supports Oracle WebCenter Content 12.2.1.4 domains deployment using April 2021 PSU and known bug fixes - certified for Oracle WebLogic Kubernetes Operator version 3.2.5. Oracle WebCenter Content 12.2.1.4 container image for this release can be downloaded from My Oracle Support (MOS patch 32822360).   June 16, 2021 21.2.3 Supports Oracle WebCenter Content 12.2.1.4 domains deployment using April 2021 PSU and known bug fixes. Oracle WebCenter Content 12.2.1.4 container image for this release can be downloaded from My Oracle Support (MOS patch 32822360).   February 28, 2021 21.1.2 Certified Oracle WebLogic Kubernetes Operator version 3.1.1. Kubernetes 1.14.8+, 1.15.7+, 1.16.0+, 1.17.0+, and 1.18.0+ support. Flannel is the only supported CNI in this release. SSL enabling for the Administration Server and Managed Servers is supported. For now, only Oracle WebCenter Content 12.2.1.4 is supported.    "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Recent changes    Date Version Change     June 10, 2022 22.2.3 Only Oracle WebCenter Portal 12.2.1.4 is supported and certified with the WebLogic Kubernetes operator version 3.3.0.   June 30, 2021 21.2.3 Only Oracle WebCenter Portal 12.2.1.4 is supported and certified with the WebLogic Kubernetes operator version 3.1.1.    "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/installguide/prerequisites/",
	"title": "Requirements and limitations",
	"tags": [],
	"description": "Understand the system requirements and limitations for deploying and running Oracle WebCenter Content with the WebLogic Kubernetes Operator, including the Oracle WebCenter Content cluster sizing recommendations.",
	"content": "This section provides information about the system requirements and limitations for deploying and running Oracle WebCenter Content domains with the WebLogic Kubernetes Operator.\nSystem requirements for Oracle WebCenter Content domains For the current production release 22.2.3:\n Oracle Linux 7 (UL6+) and Red Hat Enterprise Linux 7 (UL3+ only with standalone Kubernetes) are supported. Supported Kubernetes versions are: 1.16.15+, 1.17.13+ and 1.18.10+ (check with kubectl version). Docker 18.09.1ce, 19.03.1 (check with docker version) or CRI-O 1.14.7 (check with crictl version | grep RuntimeVersion). Flannel networking v0.12.0-amd64 or later (check with docker images | grep flannel). Helm 3.4.1 (check with helm version --client --short). Oracle WebLogic Kubernetes Operator 3.3.0 (see WebLogic Kubernetes Operator releases page). Oracle WebCenter Content 12.2.1.4 Docker image downloaded from My Oracle Support (MOS patch 34192566). This image contains the latest bundle patch and one-off patches for Oracle WebCenter Content. You must have the cluster-admin role to install WebLogic Kubernetes Operator. The WebLogic Kubernetes Operator does not need the cluster-admin role at runtime. We do not currently support running Oracle WebCenter Content in non-Linux containers. Additionally, see the Oracle WebCenter Content documentation for other requirements such as database version.  See here for resourse sizing information for Oracle WebCenter Content domains setup on Kubernetes cluster.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the WebLogic Kubernetes Operator, the following limitations currently exist for Oracle WebCenter Content domains:\n In this release, Oracle WebCenter Content domains are supported using the domain on a persistent volume model only, where the domain home is located in a persistent volume (PV). The \u0026ldquo;domain in image\u0026rdquo; and \u0026ldquo;model in image\u0026rdquo; models are not supported. Also, \u0026ldquo;WebLogic Deploy Tooling (WDT)\u0026rdquo; based deployments are currently not supported. Only configured clusters are supported. Dynamic clusters are not supported for Oracle WebCenter Content domains. Note that you can still use all of the scaling features, but you need to define the maximum size of your cluster at domain creation time. Mixed clusters (configured servers targeted to a dynamic cluster) are not supported. The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs will not be sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Logstash or Fluentd to get logs. The WebLogic Monitoring Exporter currently supports WebLogic MBean trees only. Support for JRF and Oracle WebCenter Content MBeans is not available. Also, a metrics dashboard specific to Oracle WebCenter Content is not available. Instead, use the WebLogic Server dashboard to monitor the Oracle WebCenter Content server metrics in Grafana. Some features such as multicast, multitenancy, production redeployment, and Node Manager (although it is used internally for the liveness probe and to start WebLogic Server instances) are not supported in this release. Features such as Java Messaging Service whole server migration, consensus leasing, and maximum availability architecture (Oracle WebCenter Content setup) are not supported in this release. You can have multiple UCM servers on your domain but you can have only one IBR server. There is a generic limitation with all load-balancers in end-to-end SSL configuration - accessing multiple types of servers (different Managed Servers and/or Administration Server) at the same time, is currently not supported.  For up-to-date information about the features of WebLogic Server that are supported in Kubernetes environments, see My Oracle Support Doc ID 2349228.1.\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/installguide/prerequisites/",
	"title": "Requirements and limitations",
	"tags": [],
	"description": "Understand the system requirements and limitations for deploying and running Oracle WebCenter Portal with the WebLogic Kubernetes operator.",
	"content": "Contents  Introduction System Requirements Limitations  Introduction This document describes the special considerations for deploying and running a WebCenter Portal domain with the WebLogic Kubernetes Operator. Other than those considerations listed here, the WebCenter Portal domain works in the same way as Fusion Middleware Infrastructure and WebLogic Server domains do.\nIn this release, WebCenter Portal domain is based on the domain on a persistent volume model where a WebCenter Portal domain is located in a persistent volume (PV).\nSystem Requirements  Kubernetes 1.18.18+, 1.19.7+, and 1.20.6+ (check with kubectl version). Flannel networking v0.14.0 or later (check with docker images | grep flannel), Calico networking v3.15. Docker 19.03.11+ (check with docker version). Helm 3.4+ (check with helm version). WebLogic Kubernetes operator 3.3.0 (see the operator releases page). Oracle WebCenter Portal 12.2.1.4.0 image. These proxy setups are used for pulling the required binaries and source code from the respective repositories:  export NO_PROXY=\u0026quot;localhost,127.0.0.0/8,$(hostname -i),.your-company.com,/var/run/docker.sock\u0026rdquo; export no_proxy=\u0026quot;localhost,127.0.0.0/8,$(hostname -i),.your-company.com,/var/run/docker.sock\u0026rdquo; export http_proxy=http://www-proxy-your-company.com:80 export https_proxy=http://www-proxy-your-company.com:80 export HTTP_PROXY=http://www-proxy-your-company.com:80 export HTTPS_PROXY=http://www-proxy-your-company.com:80    NOTE: Add your host IP by using hostname -i and nslookup IP addresses to the no_proxy, NO_PROXY list above.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for a WebCenter Portal domain:\n Domain in image model is not supported in this version of the operator. Only configured clusters are supported. Dynamic clusters are not supported on WebCenter Portal domains. Note that you can still use all of the scaling features. You just need to define the maximum size of your cluster at the time when you create a domain. At present, WebCenter Portal doesn\u0026rsquo;t run on non-Linux containers. Deploying and running a WebCenter Portal domain is supported only in the operator versions 3.3.0 and later. The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs are not sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Fluentd to get logs. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet.  "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/adminguide/configure-load-balancer/traefik/",
	"title": "Traefik",
	"tags": [],
	"description": "Configure the ingress-based Traefik load balancer for Oracle WebCenter Content domains.",
	"content": "This section provides information about how to install and configure the ingress-based Traefik load balancer (version 2.2.1 or later for production deployments) to load balance Oracle WebCenter Content domain clusters. You can configure Traefik for non-SSL, SSL termination and end-to-end SSL access of the application URL.\nFollow these steps to set up Traefik as a load balancer for an Oracle WebCenter Content\tdomain in a Kubernetes cluster:\n  Non-SSL and SSL termination\n Install the Traefik (ingress-based) load balancer Configure Traefik to manage ingresses Create an Ingress for the domain Verify domain application URL access Uninstall the Traefik ingress    End-to-end SSL configuration\n Install the Traefik load balancer for End-to-end SSL Configure Traefik to manage domain Create IngressRouteTCP Verify end-to-end SSL access Uninstall Traefik    Non-SSL and SSL termination Install the Traefik (ingress-based) load balancer   Use Helm to install the Traefik (ingress-based) load balancer. For detailed information, see here. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install traefik traefik/traefik \\  --namespace traefik \\  --values kubernetes/samples/scripts/charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=NodePort\u0026#34; --wait    Click here to see the sample output.   NAME: traefik LAST DEPLOYED: Sun Jan 17 23:30:20 2021 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None    A sample values.yaml for deployment of Traefik 2.2.x:\nimage: name: traefik tag: 2.2.8 pullPolicy: IfNotPresent ingressRoute: dashboard: enabled: true # Additional ingressRoute annotations (e.g. for kubernetes.io/ingress.class) annotations: {} # Additional ingressRoute labels (e.g. for filtering IngressRoute by custom labels) labels: {} providers: kubernetesCRD: enabled: true kubernetesIngress: enabled: true # IP used for Kubernetes Ingress endpoints ports: traefik: port: 9000 expose: true # The exposed port for this service exposedPort: 9000 # The port protocol (TCP/UDP) protocol: TCP web: port: 8000 # hostPort: 8000 expose: true exposedPort: 30305 nodePort: 30305 # The port protocol (TCP/UDP) protocol: TCP # Use nodeport if set. This is useful if you have configured Traefik in a # LoadBalancer # nodePort: 32080 # Port Redirections # Added in 2.2, you can make permanent redirects via entrypoints. # https://docs.traefik.io/routing/entrypoints/#redirection # redirectTo: websecure websecure: port: 8443 # # hostPort: 8443 expose: true exposedPort: 30443 # The port protocol (TCP/UDP) protocol: TCP nodePort: 30443   Verify the Traefik status and find the port number of the SSL and non-SSL services:\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-f9cf58697-p57nt 1/1 Running 0 22d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik NodePort 10.96.95.253 \u0026lt;none\u0026gt; 9000:32306/TCP,30305:30305/TCP,30443:30443/TCP 22d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 22d NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-f9cf58697 1 1 1 22d      Access the Traefik dashboard through the URL http://$(hostname -f):32306, with the HTTP host traefik.example.com:\n$ curl -H \u0026#34;host: $(hostname -f)\u0026#34; http://$(hostname -f):32306/dashboard/  Note: Make sure that you specify a fully qualified node name for $(hostname -f)\n   Configure Traefik to manage ingresses Configure Traefik to manage ingresses created in this namespace, where traefik is the Traefik namespace and wccns is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,wccns}\u0026#34;    Click here to see the sample output.   Release \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Sun Jan 17 23:43:02 2021 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create an ingress for the domain Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK , tls is Non-SSL, and domainType is wccinfra. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml based on the type of configuration (non-SSL or SSL). If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. The template YAML file for the Traefik (ingress-based) load balancer is located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/traefik-ingress.yaml\n  Install ingress-per-domain using Helm for non-SSL configuration:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wcc-traefik-ingress \\  kubernetes/samples/charts/ingress-per-domain \\  --set type=TRAEFIK \\  --namespace wccns \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; --set tls=NONSSL Sample output:\nNAME: wcc-traefik-ingress LAST DEPLOYED: Sun Jan 17 23:49:09 2021 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None   For secured access (SSL) to the Oracle WebCenter Content application, create a certificate and generate a Kubernetes secret:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n wccns create secret tls domain1-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Create Traefik Middleware custom resource\nIn case of SSL termination, Traefik must pass a custom header WL-Proxy-SSL:true to the WebLogic Server endpoints. Create the Middleware using the following command:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: wls-proxy-ssl namespace: wccns spec: headers: customRequestHeaders: WL-Proxy-SSL: \u0026#34;true\u0026#34; EOF   Create the Traefik TLSStore custom resource.\nIn case of SSL termination, Traefik should be configured to use the user-defined SSL certificate. If the user-defined SSL certificate is not configured, Traefik will create a default SSL certificate. To configure a user-defined SSL certificate for Traefik, use the TLSStore custom resource. The Kubernetes secret created with the SSL certificate should be referenced in the TLSStore object. Run the following command to create the TLSStore:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: traefik.containo.us/v1alpha1 kind: TLSStore metadata: name: default namespace: wccns spec: defaultCertificate: secretName: domain1-tls-cert EOF   Install ingress-per-domain using Helm for SSL configuration.\nThe Kubernetes secret name should be updated in the template file.\nThe template file also contains the following annotations:\ntraefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.tls: \u0026#34;true\u0026#34; traefik.ingress.kubernetes.io/router.middlewares: wccns-wls-proxy-ssl@kubernetescrd The entry point for SSL access and the Middleware name should be updated in the annotation. The Middleware name should be in the form \u0026lt;namespace\u0026gt;-\u0026lt;middleware name\u0026gt;@kubernetescrd.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wcc-traefik-ingress \\  kubernetes/samples/charts/ingress-per-domain \\  --set type=TRAEFIK \\  --namespace wccns \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; \\  --set tls=SSL Sample output:\nNAME: wcc-traefik-ingress LAST DEPLOYED: Mon Jul 20 11:44:13 2020 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None   For non-SSL access to the Oracle WebCenter Content application, get the details of the services by the ingress:\n$ kubectl describe ingress wccinfra-traefik -n wccns     Click here to see all services supported by the above deployed ingress.    Name: wccinfra-traefik Namespace: wccns Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- domain1.org /console wccinfra-adminserver:7001 (10.244.0.201:7001) /em wccinfra-adminserver:7001 (10.244.0.201:7001) /wls-exporter wccinfra-adminserver:7001 (10.244.0.201:7001) /cs wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /adfAuthentication wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /_ocsh wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /_dav wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /idcws wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /idcnativews wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /wsm-pm wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /ibr wccinfra-cluster-ibr-cluster:16250 (10.244.0.203:16250) /ibr/adfAuthentication wccinfra-cluster-ibr-cluster:16250 (10.244.0.203:16250) /weblogic/ready wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /imaging wccinfra-cluster-ipm-cluster:16000 (10.244.0.206:16000,10.244.0.209:16000,10.244.0.213:16000) /dc-console wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /dc-client wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /wcc wccinfra-cluster-wccadf-cluster:16225 (10.244.0.205:16225,10.244.0.210:16225,10.244.0.214:16225) Annotations: kubernetes.io/ingress.class: traefik meta.helm.sh/release-name: wcc-traefik-ingress meta.helm.sh/release-namespace: wccns Events: \u0026lt;none\u0026gt;      For SSL access to the Oracle WebCenter Content application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress wccinfra-traefik -n wccns     Click here to see all services supported by the above deployed ingress.    Name: wccinfra-traefik Namespace: wccns Address: Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- domain1.org /console wccinfra-adminserver:7001 (10.244.0.201:7001) /em wccinfra-adminserver:7001 (10.244.0.201:7001) /wls-exporter wccinfra-adminserver:7001 (10.244.0.201:7001) /cs wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /adfAuthentication wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /_ocsh wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /_dav wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /idcws wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /idcnativews wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /wsm-pm wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /ibr wccinfra-cluster-ibr-cluster:16250 (10.244.0.203:16250) /ibr/adfAuthentication wccinfra-cluster-ibr-cluster:16250 (10.244.0.203:16250) /weblogic/ready wccinfra-cluster-ucm-cluster:16200 (10.244.0.202:16200,10.244.0.207:16200,10.244.0.211:16200) /imaging wccinfra-cluster-ipm-cluster:16000 (10.244.0.206:16000,10.244.0.209:16000,10.244.0.213:16000) /dc-console wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /dc-client wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /wcc wccinfra-cluster-wccadf-cluster:16225 (10.244.0.205:16225,10.244.0.210:16225,10.244.0.214:16225) Annotations: kubernetes.io/ingress.class: traefik meta.helm.sh/release-name: wcc-traefik-ingress meta.helm.sh/release-namespace: wccns Events: \u0026lt;none\u0026gt;     To confirm that the load balancer noticed the new ingress and is successfully routing to the domain server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;, which should return an HTTP 200 status code, as follows: $ curl -v http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER_PORT}/weblogic/ready * About to connect() to abc.com port 30305 (#0) * Trying 100.111.156.246... * Connected to abc.com (100.111.156.246) port 30305 (#0) \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: domain1.org:30305 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Length: 0 \u0026lt; Date: Thu, 03 Dec 2020 13:16:19 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; * Connection #0 to host abc.com left intact   Verify domain application URL access For non-SSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain application URLs are accessible through the non-SSL load balancer port 30305 for HTTP access. The sample URLs for Oracle WebCenter Content domain of type wcc are:\nhttp://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/weblogic/ready http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/cs http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/ibr http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/imaging http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/dc-console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/wcc\tFor SSL configuration After setting up the Traefik (ingress-based) load balancer, verify that the domain applications are accessible through the SSL load balancer port 30443 for HTTPS access. The sample URLs for Oracle WebCenter Content domain are:\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/weblogic/ready https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/cs https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/ibr https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/imaging https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/dc-console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/wcc Uninstall the Traefik ingress Uninstall and delete the ingress deployment:\n$ helm delete wcc-traefik-ingress -n wccns End-to-end SSL configuration Install the Traefik load balancer for end-to-end SSL   Use Helm to install the Traefik (ingress-based) load balancer. For detailed information, see here. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install traefik traefik/traefik \\  --namespace traefik \\  --values kubernetes/samples/scripts/charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --wait    Click here to see the sample output.   NAME: traefik LAST DEPLOYED: Sun Jan 17 23:30:20 2021 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None      Verify the Traefik operator status and find the port number of the SSL and non-SSL services:\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-operator-676fc64d9c-skppn 1/1 Running 0 78d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik-operator NodePort 10.109.223.59 \u0026lt;none\u0026gt; 443:30443/TCP,80:30305/TCP 78d service/traefik-operator-dashboard ClusterIP 10.110.85.194 \u0026lt;none\u0026gt; 80/TCP 78d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik-operator 1/1 1 1 78d NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-operator-676fc64d9c 1 1 1 78d replicaset.apps/traefik-operator-cb78c9dc9 0 0 0 78d      Access the Traefik dashboard through the URL http://$(hostname -f):32306, with the HTTP host traefik.example.com:\n$ curl -H \u0026#34;host: $(hostname -f)\u0026#34; http://$(hostname -f):32306/dashboard/  Note: Make sure that you specify a fully qualified node name for $(hostname -f).\n   Configure Traefik to manage the domain Configure Traefik to manage the domain application service created in this namespace, where traefik is the Traefik namespace and wccns is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,wccns}\u0026#34;    Click here to see the sample output.   Release \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Sun Jan 17 23:43:02 2021 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create IngressRouteTCP   To enable SSL passthrough in Traefik, you can configure a TCP router. A sample YAML for IngressRouteTCP is available at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls/traefik-tls.yaml. The following should be updated in traefik-tls.yaml:\n The service name and the SSL port should be updated in the Services. The load balancer hostname should be updated in the HostSNI rule.  Sample traefik-tls.yaml:\n  apiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: wcc-ucm-routetcp namespace: wccns spec: entryPoints: - websecure routes: - match: HostSNI(`${LOADBALANCER_HOSTNAME}`) services: - name: wccinfra-cluster-ucm-cluster port: 16201 weight: 3 TerminationDelay: 400 tls: passthrough: true  Create the IngressRouteTCP:  $ kubectl apply -f traefik-tls.yaml Verify end-to-end SSL access Verify the access to application URLs exposed through the configured service. You should be able to access the following Oracle WebCenter Content domain URLs:\nLOADBALANCER-SSLPORT is 30443\nhttps://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/cs https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/ibr https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/imaging https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/dc-console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/wcc Uninstall Traefik $ helm delete traefik -n wccns "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/oracle-cloud/configure-load-balancer/traefik/",
	"title": "Traefik",
	"tags": [],
	"description": "Configure the ingress-based Traefik load balancer for Oracle WebCenter Content domains.",
	"content": "This section provides information about how to install and configure the ingress-based Traefik load balancer (version 2.2.8 or later for production deployments) to load balance Oracle WebCenter Content domain clusters.\nFollow these steps to set up Traefik as a load balancer for an Oracle WebCenter Content\tdomain in a Kubernetes cluster:\nContents  Install the Traefik (ingress-based) load balancer Configure Traefik to manage ingresses Create an Ingress for the domain Verify domain application URL access Uninstall the Traefik ingress  Install the Traefik (ingress-based) load balancer   Use Helm to install the Traefik (ingress-based) load balancer. For detailed information, see here. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ kubectl create namespace traefik $ helm repo add traefik https://containous.github.io/traefik-helm-chart Sample output:\n\u0026#34;traefik\u0026#34; has been added to your repositories   Install Traefik:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install traefik traefik/traefik \\  --namespace traefik \\  --values kubernetes/samples/scripts/charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --set \u0026#34;service.type=LoadBalancer\u0026#34; --wait    Click here to see the sample output.   NAME: traefik-operator LAST DEPLOYED: Mon Jun 1 19:31:20 2020 NAMESPACE: traefik STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get Traefik load balancer IP or hostname: NOTE: It may take a few minutes for this to become available. You can watch the status by running: $ kubectl get svc traefik-operator --namespace traefik -w Once \u0026#39;EXTERNAL-IP\u0026#39; is no longer \u0026#39;\u0026lt;pending\u0026gt;\u0026#39;: $ kubectl describe svc traefik-operator --namespace traefik | grep Ingress | awk \u0026#39;{print $3}\u0026#39; 2. Configure DNS records corresponding to Kubernetes ingress resources to point to the load balancer IP or hostname found in step 1    A sample values.yaml for deployment of Traefik 2.2.x:   Click here to see values.yaml   image: name: traefik tag: 2.2.8 pullPolicy: IfNotPresent ingressRoute: dashboard: enabled: true # Additional ingressRoute annotations (e.g. for kubernetes.io/ingress.class) annotations: {} # Additional ingressRoute labels (e.g. for filtering IngressRoute by custom labels) labels: {} providers: kubernetesCRD: enabled: true kubernetesIngress: enabled: true # IP used for Kubernetes Ingress endpoints ports: traefik: port: 9000 expose: true # The exposed port for this service exposedPort: 9000 # The port protocol (TCP/UDP) protocol: TCP web: port: 8000 # hostPort: 8000 expose: true exposedPort: 30305 nodePort: 30305 # The port protocol (TCP/UDP) protocol: TCP # Use nodeport if set. This is useful if you have configured Traefik in a # LoadBalancer # nodePort: 32080 # Port Redirections # Added in 2.2, you can make permanent redirects via entrypoints. # https://docs.traefik.io/routing/entrypoints/#redirection # redirectTo: websecure websecure: port: 8443 # # hostPort: 8443 expose: true exposedPort: 30443 # The port protocol (TCP/UDP) protocol: TCP nodePort: 30443   \n  Verify the Traefik (load balancer) services:\n  Please note the EXTERNAL-IP of the traefik-operator service. This is the public IP address of the load balancer that you will use to access the WebLogic Server Administration Console and WebCenter Content URLs.\n$ kubectl get service -n traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik LoadBalancer 10.96.8.30 123.456.xx.xx 9000:30734/TCP,30305:30305/TCP,30443:30443/TCP 6d23h To print only the Traefik EXTERNAL-IP, execute this command:\n$ TRAEFIK_PUBLIC_IP=`kubectl describe svc traefik --namespace traefik | grep Ingress | awk \u0026#39;{print $3}\u0026#39;` $ echo $TRAEFIK_PUBLIC_IP 123.456.xx.xx   Verify the helm charts:\n$ helm list -n traefik NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION traefik traefik 2 2021-10-11 12:22:41.122310912 +0000 UTC deployed traefik-9.1.1 2.2.8   Verify the Traefik status and find the port number\n$ kubectl get all -n traefik    Click here to see the sample output.   NAME READY STATUS RESTARTS AGE pod/traefik-f9cf58697-xjhpl 1/1 Running 0 7d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/traefik LoadBalancer 10.96.8.30 123.456.xx.xx 9000:30734/TCP,30305:30305/TCP,30443:30443/TCP 7d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 1/1 1 1 7d NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-f9cf58697 1 1 1 7d      Configure Traefik to manage ingresses Configure Traefik to manage ingresses created in this namespace, where traefik is the Traefik namespace and wccns is the namespace of the domain:\n$ helm upgrade traefik traefik/traefik --namespace traefik --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,wccns}\u0026#34;    Click here to see the sample output.   Release \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Sun Jan 17 23:43:02 2021 NAMESPACE: traefik STATUS: deployed REVISION: 2 TEST SUITE: None    Create an ingress for the domain Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK , tls is Non-SSL, and domainType is wccinfra. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml based on the type of configuration (non-SSL or SSL). If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. The template YAML file for the Traefik (ingress-based) load balancer is located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/traefik-ingress.yaml\n  Install ingress-per-domain using Helm for non-SSL configuration:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wcc-traefik-ingress \\  kubernetes/samples/charts/ingress-per-domain \\  --set type=TRAEFIK \\  --namespace wccns \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=\u0026#34; \\  --set tls=NONSSL Sample output:\nNAME: wcc-traefik-ingress LAST DEPLOYED: Sun Jan 17 23:49:09 2021 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None   Verify domain application URL access After setting up the Traefik (ingress-based) load balancer, verify that the domain application URLs are accessible through the load balancer port 30305 for HTTP access. The sample URLs for Oracle WebCenter Content domain of type wcc are:\nhttp://${TRAEFIK_PUBLIC_IP}:30305/weblogic/ready http://${TRAEFIK_PUBLIC_IP}:30305/console http://${TRAEFIK_PUBLIC_IP}:30305/cs http://${TRAEFIK_PUBLIC_IP}:30305/ibr\tUninstall the Traefik ingress Uninstall and delete the ingress deployment:\n$ helm delete wcc-traefik-ingress -n wccns "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/installguide/",
	"title": "Install Guide",
	"tags": [],
	"description": "",
	"content": "Install the WebLogic Kubernetes Operator to prepare and deploy Oracle WebCenter Content domain.\n Requirements and limitations  Understand the system requirements and limitations for deploying and running Oracle WebCenter Content with the WebLogic Kubernetes Operator, including the Oracle WebCenter Content cluster sizing recommendations.\n Prepare your environment  Prepare for creating Oracle WebCenter Content domain, including required secrets creation, persistent volume and volume claim creation, database creation, and database schema creation.\n Create Oracle WebCenter Content domain  Create Oracle WebCenter Content domain home on an existing PV or PVC and create the domain resource YAML file for deploying the generated Oracle WebCenter Content domain.\n Launch Oracle Webcenter Content Native Applications in Containers  How to launch Oracle WebCenter Content native binaries from inside containerized environment.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/oracle-cloud/prepare-environment-wcc-domain/",
	"title": "Prepare environment for WCC domain",
	"tags": [],
	"description": "Prepare environment for WCC domain on Oracle Kubernetes Engine (OKE).",
	"content": "To create your Oracle WebCenter Content domain in Kubernetes OKE environment, complete the following steps:\nContents   Set up the code repository to deploy Oracle WebCenter Content domain\n  Create a namespace for the Oracle WebCenter Content domain\n  Create the imagePullSecrets\n  Install the WebLogic Kubernetes Operator\n  Prepare the environment for Oracle WebCenter Content domain\na. Upgrade the WebLogic Kubernetes Operator with the Oracle WebCenter Content domain-namespace\nb. Create a persistent storage for the Oracle WebCenter Content domain\nc. Create a Kubernetes secret with domain credentials\nd. Create a Kubernetes secret with the RCU credentials\ne. Install and start the Database\nf. Configure access to your database\ng. Run the Repository Creation Utility to set up your database schemas\n  Create Oracle WebCenter Content domain\n  Set up the code repository to deploy Oracle WebCenter Content domain Oracle WebCenter Content domain deployment on Kubernetes leverages the WebLogic Kubernetes Operator infrastructure. To deploy an Oracle WebCenter Content domain, you must set up the deployment scripts.\n  Create a working directory to set up the source code:\n$ export WORKDIR=$HOME/wcc_3.3.0 $ mkdir ${WORKDIR}   Download the supported version of the WebLogic Kubernetes Operator source code from the WebLogic Kubernetes Operator github project. Currently the supported WebLogic Kubernetes Operator version is 3.3.0:\n$ cd ${WORKDIR} $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch v3.3.0   Download the Oracle WebCenter Content Kubernetes deployment scripts from the WCC repository and copy them to the WebLogic Kubernetes Operator samples location:\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/create-wcc-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/\t$ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/ingress-per-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/charts ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/imagetool-scripts ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ ``\n  Create a namespace for the Oracle WebCenter Content domain Create a Kubernetes namespace (for example, wccns) for the domain unless you intend to use the default namespace. Use the new namespace in the remaining steps in this section. For details, see Prepare to run a domain.\n $ kubectl create namespace wccns Create the imagePullSecrets Create the imagePullSecrets (in wccns namespace) so that Kubernetes Deployment can pull the image automatically from OCIR.\n Note: Create the imagePullSecret as per your environement using a sample command like this -\n $ kubectl create secret docker-registry image-secret -n wccns --docker-server=phx.ocir.io --docker-username=axxxxxxxxxxx/oracleidentitycloudservice/\u0026lt;your_user_name\u0026gt; --docker-password='vUv+xxxxxxxxxxx\u0026lt;KN7z' --docker-email=me@oracle.com The parameter values are:\nOCI Region is phoenix phx.ocir.io OCI Tenancy Name axxxxxxxxxxx ImagePullSecret Name image-secret Username and email address me@oracle.com Auth Token Password vUv+xxxxxxxxxxx\u0026lt;KN7z\nInstall the WebLogic Kubernetes Operator The WebLogic Kubernetes Operator supports the deployment of Oracle WebCenter Content domain in the Kubernetes environment.\nIn the following example commands to install the WebLogic Kubernetes Operator, opns is the namespace and op-sa is the service account created for the WebLogic Kubernetes Operator:\nCreating namespace and service account for WebLogic Kubernetes Operator $ kubectl create namespace opns $ kubectl create serviceaccount -n opns op-sa Install WebLogic Kubernetes Operator $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator --namespace opns --set image=phx.ocir.io/xxxxxxxxxxx/oracle/weblogic-kubernetes-operator:3.3.0 --set imagePullSecret=image-secret --set serviceAccount=op-sa --set \u0026quot;domainNamespaces={}\u0026quot; --set \u0026quot;javaLoggingLevel=FINE\u0026quot; --wait Verify the WebLogic Kubernetes Operator pod $ kubectl get pods -n opns NAME READY STATUS RESTARTS AGE weblogic-operator-779965b66c-d8265 1/1 Running 0 11d # Verify the Operator helm Charts $ helm list -n opns NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION weblogic-kubernetes-operator opns 3 2022-02-24 06:50:29.810106777 +0000 UTC deployed weblogic-operator-3.3.0 3.3.0 Prepare the environment for Oracle WebCenter Content domain Upgrade the WebLogic Kubernetes Operator with the Oracle WebCenter Content domain-namespace  $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace opns --set \u0026quot;domainNamespaces={wccns}\u0026quot; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator Create a persistent storage for the Oracle WebCenter Content domain In the Kubernetes namespace you created, create the PV and PVC for the domain by running the create-pv-pvc.sh script. Follow the instructions for using the script to create a dedicated PV and PVC for the Oracle WebCenter Content domain.\nHere we will use the NFS Server and mount path, created on this page.\n  Review the configuration parameters for PV creation here. Based on your requirements, update the values in the create-pv-pvc-inputs.yaml file located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/. Sample configuration parameter values for the Oracle WebCenter Content domain are:\n baseName: domain domainUID: wccinfra namespace: wccns weblogicDomainStorageType:: NFS weblogicDomainStorageNFSServer:: \u0026lt;your_nfs_server_ip\u0026gt; weblogicDomainStoragePath: /\u0026lt;your_dir_name\u0026gt;   Note: Make sure to update the \u0026ldquo;weblogicDomainStorageNFSServer:\u0026rdquo; with the NFS Server IP as per your Environment\n   Ensure that the path for the weblogicDomainStoragePath property exists (if not, please refer subsection 4 of this document to create it first) and has correct access permissions, and that the folder is empty.\n  Run the create-pv-pvc.sh script:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ rm -rf output/ $ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output   The create-pv-pvc.sh script will create a subdirectory pv-pvcs under the given /path/to/output-directory directory and creates two YAML configuration files for PV and PVC. Apply these two YAML files to create the PV and PVC Kubernetes resources using the kubectl create -f command:\n$ kubectl create -f output/pv-pvcs/wccinfra-domain-pv.yaml -n wccns $ kubectl create -f output/pv-pvcs/wccinfra-domain-pvc.yaml -n wccns   Get the details of PV and PVC:\n$ kubectl describe pv wccinfra-domain-pv $ kubectl describe pvc wccinfra-domain-pvc -n wccns   Create a Kubernetes secret with domain credentials Create the Kubernetes secrets username and password of the administrative account in the same Kubernetes namespace as the domain:\n $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p welcome1 -n wccns -d wccinfra -s wccinfra-domain-credentials For more details, see this document.\nYou can check the secret with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret wccinfra-domain-credentials -o yaml -n wccns apiVersion: v1 data: password: d2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026quot;2021-07-30T06:04:33Z\u0026quot; labels: weblogic.domainName: wccinfra weblogic.domainUID: wccinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2021-07-30T06:04:36Z\u0026quot; name: wccinfra-domain-credentials namespace: wccns resourceVersion: \u0026quot;90770768\u0026quot; selfLink: /api/v1/namespaces/wccns/secrets/wccinfra-domain-credentials uid: 9c5dab09-15f3-4e1f-a40d-457904ddf96b type: Opaque    Create a Kubernetes secret with the RCU credentials You also need to create a Kubernetes secret containing the credentials for the database schemas. When you create your domain, it will obtain the RCU credentials from this secret.\nUse the provided sample script to create the secret:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u weblogic -p welcome1 -a sys -q welcome1 -d wccinfra -n wccns -s wccinfra-rcu-credentials The parameter values are:\n-u username for schema owner (regular user), required.\n-p password for schema owner (regular user), required.\n-a username for SYSDBA user, required.\n-q password for SYSDBA user, required.\n-d domainUID. Example: wccinfra\n-n namespace. Example: wccns\n-s secretName. Example: wccinfra-rcu-credentials\nYou can confirm the secret was created as expected with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret wccinfra-rcu-credentials -o yaml -n wccns apiVersion: v1 data: password: d2VsY29tZTE= sys_password: d2VsY29tZTE= sys_username: c3lz username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-16T08:23:04Z\u0026#34; labels: weblogic.domainName: wccinfra weblogic.domainUID: wccinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-16T08:23:04Z\u0026#34; name: wccinfra-rcu-credentials namespace: wccns resourceVersion: \u0026#34;3277132\u0026#34; selfLink: /api/v1/namespaces/wccns/secrets/wccinfra-rcu-credentials uid: b75f4e13-84e6-40f5-84ba-0213d85bdf30 type: Opaque    Install and start the Database This step is required only when standalone database was not already setup and the user wanted to use the database in a container. The Oracle Database Docker images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1). For production usecase it is suggested to use a standalone db. Sample provides steps to create the database in a container.\nThe database in a container can be created with a PV attached for persisting the data or without attaching the PV. In this setup we will be creating database in a container without PV attached.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service $ ./start-db-service.sh -i phx.ocir.io/xxxxxxxxxxxx/oracle/database/enterprise:x.x.x.x -s image-secret -n wccns    Click here to see the Sample Output   $ ./start-db-service.sh -i phx.ocir.io/xxxxxxxxxxxx/oracle/database/enterprise:x.x.x.x -s image-secret -n wccns Checking Status for NameSpace [wccns] Skipping the NameSpace[wccns] Creation ... NodePort[30011] ImagePullSecret[docker-store] Image[phx.ocir.io/xxxxxxxxxxxx/oracle/database/enterprise:x.x.x.x] NameSpace[wccns] service/oracle-db created deployment.apps/oracle-db created [oracle-db-8598b475c5-cx5nk] already initialized .. Checking Pod READY column for State [1/1] NAME READY STATUS RESTARTS AGE oracle-db-8598b475c5-cx5nk 1/1 Running 0 20s Service [oracle-db] found NAME READY STATUS RESTARTS AGE oracle-db-8598b475c5-cx5nk 1/1 Running 0 25s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oracle-db LoadBalancer 10.96.74.187 \u0026lt;pending\u0026gt; 1521:30011/TCP 28s [1/30] Retrying for Oracle Database Availability... [2/30] Retrying for Oracle Database Availability... [3/30] Retrying for Oracle Database Availability... [4/30] Retrying for Oracle Database Availability... [5/30] Retrying for Oracle Database Availability... [6/30] Retrying for Oracle Database Availability... [7/30] Retrying for Oracle Database Availability... [8/30] Retrying for Oracle Database Availability... [9/30] Retrying for Oracle Database Availability... [10/30] Retrying for Oracle Database Availability... [11/30] Retrying for Oracle Database Availability... [12/30] Retrying for Oracle Database Availability... [13/30] Retrying for Oracle Database Availability... Done ! The database is ready for use . Oracle DB Service is RUNNING with NodePort [30011] Oracle DB Service URL [oracle-db.wccns.svc.cluster.local:1521/devpdb.k8s]    Once database is created successfully, you can use the database connection string, as an rcuDatabaseURL parameter in the create-domain-inputs.yaml file.\nConfigure access to your database Run a container to create rcu pod\nkubectl run rcu --generator=run-pod/v1 \\  --image phx.ocir.io/xxxxxxxxxxx/oracle/wccontent:x.x.x.x \\  --namespace wccns \\  --overrides=\u0026#39;{ \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;image-secret\u0026#34;}] } }\u0026#39; \\  -- sleep infinity # Check the status of rcu pod kubectl get pods -n wccns Run the Repository Creation Utility to set up your database schemas Create or Drop schemas To create the database schemas for Oracle WebCenter Content, run the create-rcu-schema.sh script.\nFor example:\n# Make sure rcu pod status is running before executing this kubectl exec -n wccns -ti rcu /bin/bash # DB details export CONNECTION_STRING=your_db_host:1521/your_db_service export RCUPREFIX=your_schema_prefix echo -e welcome1\u0026#34;\\n\u0026#34;welcome1\u0026gt; /tmp/pwd.txt # Create schemas /u01/oracle/oracle_common/bin/rcu -silent -createRepository -databaseType ORACLE -connectString $CONNECTION_STRING -dbUser sys -dbRole sysdba -useSamePasswordForAllSchemaUsers true -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component CONTENT -component MDS -component STB -component OPSS -component IAU -component IAU_APPEND -component IAU_VIEWER -component WLS -tablespace USERS -tempTablespace TEMP -f \u0026lt; /tmp/pwd.txt # Drop schemas /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component CONTENT -component MDS -component STB -component OPSS -component IAU -component IAU_APPEND -component IAU_VIEWER -component WLS -f \u0026lt; /tmp/pwd.txt # Exit from the container exit Create Oracle WebCenter Content domain Now that you have your Docker images and you have created your RCU schemas, you are ready to create your domain. To continue, follow the Step-3 and Step-4.\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/installguide/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Prepare for creating Oracle WebCenter Content domain, including required secrets creation, persistent volume and volume claim creation, database creation, and database schema creation.",
	"content": "To prepare your Oracle WebCenter Content in Kubernetes environment, complete the following steps:\n  Set up your Kubernetes cluster\n  Install Helm\n  Pull dependent images\n  Set up the code repository to deploy Oracle WebCenter Content domain\n  Obtain the Oracle WebCenter Content Docker image\n  Install the WebLogic Kubernetes Operator\n  Prepare the environment for Oracle WebCenter Content domain\na. Create a namespace for the Oracle WebCenter Content domain\nb. Create a persistent storage for the Oracle WebCenter Content domain\nc. Create a Kubernetes secret with domain credentials\nd. Create a Kubernetes secret with the RCU credentials\ne. Configure access to your database\nf. Run the Repository Creation Utility to set up your database schemas\n  Create Oracle WebCenter Content domain\n  Set up your Kubernetes cluster If you need help setting up a Kubernetes environment, check the cheat sheet.\nInstall Helm The WebLogic Kubernetes Operator uses Helm to create and deploy the necessary resources and then run it in a Kubernetes cluster. For Helm installation and usage information, see here.\nPull dependent images Obtain dependent images and add them to your local registry. Dependent images include WebLogic Kubernetes Operator, Traefik. Pull these images and add them to your local registry:\n Pull these docker images and re-tag them as shown:  To pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nThen, pull these docker images and re-tag them:\ndocker login https://container-registry.oracle.com (enter your Oracle email Id and password) This step is required once at every node to get access to the Oracle Container Registry. WebLogic Kubernetes Operator image:\n$ docker pull container-registry.oracle.com/middleware/weblogic-kubernetes-operator:3.3.0 $ docker tag container-registry.oracle.com/middleware/weblogic-kubernetes-operator:3.3.0 oracle/weblogic-kubernetes-operator:3.3.0 Pull Traefik Image\n$ docker pull traefik:2.2.8 Set up the code repository to deploy Oracle WebCenter Content domain Oracle WebCenter Content domain deployment on Kubernetes leverages the WebLogic Kubernetes Operator infrastructure. To deploy an Oracle WebCenter Content domain, you must set up the deployment scripts.\n  Create a working directory to set up the source code:\n$ export WORKDIR=$HOME/wcc_3.3.0 $ mkdir ${WORKDIR}   Download the supported version of the WebLogic Kubernetes Operator source code from WebLogic Kubernetes Operator github project. Currently the supported WebLogic Kubernetes Operator version is 3.3.0:\n$ git clone https://github.com/oracle/weblogic-kubernetes-operator.git --branch v3.3.0   Download the Oracle WebCenter Content Kubernetes deployment scripts from the WCC repository and copy them to the WebLogic Kubernetes Operator samples location:\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/create-wcc-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/\t$ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/ingress-per-domain ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/charts ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ $ cp -rf ${WORKDIR}/fmw-kubernetes/OracleWebCenterContent/kubernetes/imagetool-scripts ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/ ``\n  Obtain the Oracle WebCenter Content Docker image The Oracle WebCenter Content image with latest bundle patch and required interim patches can be obtained from My Oracle Support (MOS). This is the only image supported for production deployments. Follow the below steps to download the Oracle WebCenter Content image from My Oracle Support.\n  Download patch 34192566 from My Oracle Support (MOS).\n  Unzip the downloaded patch zip file.\nFor example:\n$ unzip p34192566_122140_Linux-x86-64.zip # sample output Archive: p34192566_122140_Linux-x86-64.zip inflating: wccontent-12.2.1.4-jdk8-ol7-220519.2037.tar inflating: README.html   Load the image archive using the docker load command.\nFor example:\n$ docker load \u0026lt; wccontent-12.2.1.4-jdk8-ol7-220519.2037.tar    Click here to see sample output   d0df970fe76a: Loading layer [==================================================\u0026gt;] 138.3MB/138.3MB 3b64a4bdc552: Loading layer [==================================================\u0026gt;] 13.45MB/13.45MB ee5141cc5c13: Loading layer [==================================================\u0026gt;] 20.99kB/20.99kB 51f637dc720f: Loading layer [==================================================\u0026gt;] 334MB/334MB ffc8b247ad07: Loading layer [==================================================\u0026gt;] 3.98GB/3.98GB cd87862f5c14: Loading layer [==================================================\u0026gt;] 4.608kB/4.608kB 12661fb5186c: Loading layer [==================================================\u0026gt;] 137.2kB/137.2kB f84db83c8dfa: Loading layer [==================================================\u0026gt;] 69.12kB/69.12kB Loaded image: oracle/wccontent:12.2.1.4-jdk8-ol7-220519.2037      Run the docker inspect command to verify that the downloaded image is the latest released image. The value of label com.oracle.weblogic.imagetool.buildid must match to f83a7f6c-b564-4367-93e3-f809371dfa79.\nFor example:\n$ docker inspect --format=\u0026#39;{{ index .Config.Labels \u0026#34;com.oracle.weblogic.imagetool.buildid\u0026#34; }}\u0026#39; oracle/wccontent:12.2.1.4-jdk8-ol7-220519.2037 f83a7f6c-b564-4367-93e3-f809371dfa79   Alternatively, if you want to build and use Oracle WebCenter Content Container image, using WebLogic Image Tool, with any additional bundle patch or interim patches, then follow these steps to create the image.\n Note: The default Oracle WebCenter Content image name used for Oracle WebCenter Content domain deployment is oracle/wccontent:12.2.1.4.0. The image created must be tagged as oracle/wccontent:12.2.1.4.0 using the docker tag command. If you want to use a different name for the image, make sure to update the new image tag name in the create-domain-inputs.yaml file and also in other instances where the oracle/wccontent:12.2.1.4.0 image name is used.\n Install the WebLogic Kubernetes Operator The WebLogic Kubernetes Operator supports the deployment of Oracle WebCenter Content domain in the Kubernetes environment. Follow the steps in this document to install WebLogic Kubernetes Operator.\n Note: Optionally, you can execute these steps to send the contents of the operator’s logs to Elasticsearch.\n In the following example commands to install the WebLogic Kubernetes Operator, opns is the namespace and op-sa is the service account created for WebLogic Kubernetes Operator:\nCreating namespace and service account for WebLogic Kubernetes Operator $ kubectl create namespace opns $ kubectl create serviceaccount -n opns op-sa Install WebLogic Kubernetes Operator $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator --namespace opns --set image=oracle/weblogic-kubernetes-operator:3.3.0 --set serviceAccount=op-sa --set \u0026quot;domainNamespaces={}\u0026quot; --set \u0026quot;javaLoggingLevel=FINE\u0026quot; --wait Prepare the environment for Oracle WebCenter Content domain Create a namespace for the Oracle WebCenter Content domain Create a Kubernetes namespace (for example, wccns) for the domain unless you intend to use the default namespace. Use the new namespace in the remaining steps in this section. For details, see Prepare to run a domain.\n $ kubectl create namespace wccns $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm upgrade --reuse-values --namespace opns --set \u0026quot;domainNamespaces={wccns}\u0026quot; --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator Create a persistent storage for the Oracle WebCenter Content domain In the Kubernetes namespace you created, create the PV and PVC for the domain by running the create-pv-pvc.sh script. Follow the instructions for using the script to create a dedicated PV and PVC for the Oracle WebCenter Content domain.\n  Review the configuration parameters for PV creation here. Based on your requirements, update the values in the create-pv-pvc-inputs.yaml file located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/. Sample configuration parameter values for the Oracle WebCenter Content domain are:\n baseName: domain domainUID: wccinfra namespace: wccns weblogicDomainStorageType: HOST_PATH weblogicDomainStoragePath: /net/\u0026lt;your_host_name\u0026gt;/scratch/k8s_dir/wcc    Ensure that the path for the weblogicDomainStoragePath property exists (if not, please refer subsection 4 of this document to create it first) and has full access permissions, and that the folder is empty.\n  Run the create-pv-pvc.sh script:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ rm -rf output/ $ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output   The create-pv-pvc.sh script will create a subdirectory pv-pvcs under the given /path/to/output-directory directory and creates two YAML configuration files for PV and PVC. Apply these two YAML files to create the PV and PVC Kubernetes resources using the kubectl create -f command:\n$ kubectl create -f output/pv-pvcs/wccinfra-domain-pv.yaml -n wccns $ kubectl create -f output/pv-pvcs/wccinfra-domain-pvc.yaml -n wccns   Get the details of PV and PVC:\n$ kubectl describe pv wccinfra-domain-pv $ kubectl describe pvc wccinfra-domain-pvc -n wccns   Create a Kubernetes secret with domain credentials Create the Kubernetes secrets username and password of the administrative account in the same Kubernetes namespace as the domain:\n $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p welcome1 -n wccns -d wccinfra -s wccinfra-domain-credentials For more details, see this document.\nYou can check the secret with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret wccinfra-domain-credentials -o yaml -n wccns apiVersion: v1 data: password: d2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026quot;2020-09-16T08:22:50Z\u0026quot; labels: weblogic.domainName: wccinfra weblogic.domainUID: wccinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026quot;2020-09-16T08:22:50Z\u0026quot; name: wccinfra-domain-credentials namespace: wccns resourceVersion: \u0026quot;3277100\u0026quot; selfLink: /api/v1/namespaces/wccns/secrets/wccinfra-domain-credentials uid: 35a8313f-1ec2-44b0-a2bf-fee381eed57f type: Opaque    Create a Kubernetes secret with the RCU credentials You also need to create a Kubernetes secret containing the credentials for the database schemas. When you create your domain, it will obtain the RCU credentials from this secret.\nUse the provided sample script to create the secret:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh -u weblogic -p welcome1 -a sys -q welcome1 -d wccinfra -n wccns -s wccinfra-rcu-credentials The parameter values are:\n-u username for schema owner (regular user), required.\n-p password for schema owner (regular user), required.\n-a username for SYSDBA user, required.\n-q password for SYSDBA user, required.\n-d domainUID. Example: wccinfra\n-n namespace. Example: wccns\n-s secretName. Example: wccinfra-rcu-credentials\nYou can confirm the secret was created as expected with the kubectl get secret command.\nFor example:\n  Click here to see the sample secret description.   $ kubectl get secret wccinfra-rcu-credentials -o yaml -n wccns apiVersion: v1 data: password: d2VsY29tZTE= sys_password: d2VsY29tZTE= sys_username: c3lz username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: \u0026#34;2020-09-16T08:23:04Z\u0026#34; labels: weblogic.domainName: wccinfra weblogic.domainUID: wccinfra managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:password: {} f:sys_password: {} f:sys_username: {} f:username: {} f:metadata: f:labels: .: {} f:weblogic.domainName: {} f:weblogic.domainUID: {} f:type: {} manager: kubectl operation: Update time: \u0026#34;2020-09-16T08:23:04Z\u0026#34; name: wccinfra-rcu-credentials namespace: wccns resourceVersion: \u0026#34;3277132\u0026#34; selfLink: /api/v1/namespaces/wccns/secrets/wccinfra-rcu-credentials uid: b75f4e13-84e6-40f5-84ba-0213d85bdf30 type: Opaque    Configure access to your database Run a container to create rcu pod\nkubectl run rcu --generator=run-pod/v1 --image oracle/wccontent:12.2.1.4 -n wccns -- sleep infinity #check the status of rcu pod kubectl get pods -n wccns Run the Repository Creation Utility to set up your database schemas Create OR Drop schemas To create the database schemas for Oracle WebCenter Content, run the create-rcu-schema.sh script.\nFor example:\n# make sure rcu pod status is running before executing this kubectl exec -n wccns -ti rcu /bin/bash # DB details export CONNECTION_STRING=your_db_host:1521/your_db_service export RCUPREFIX=your_schema_prefix echo -e welcome1\u0026#34;\\n\u0026#34;welcome1\u0026gt; /tmp/pwd.txt # Create schemas /u01/oracle/oracle_common/bin/rcu -silent -createRepository -databaseType ORACLE -connectString $CONNECTION_STRING -dbUser sys -dbRole sysdba -useSamePasswordForAllSchemaUsers true -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component CONTENT -component MDS -component STB -component OPSS -component IAU -component IAU_APPEND -component IAU_VIEWER -component WLS -tablespace USERS -tempTablespace TEMP -f \u0026lt; /tmp/pwd.txt # Drop schemas /u01/oracle/oracle_common/bin/rcu -silent -dropRepository -databaseType ORACLE -connectString $CONNECTION_STRING -dbUser sys -dbRole sysdba -selectDependentsForComponents true -schemaPrefix $RCUPREFIX -component CONTENT -component MDS -component STB -component OPSS -component IAU -component IAU_APPEND -component IAU_VIEWER -component WLS -f \u0026lt; /tmp/pwd.txt #exit from the container exit Create Oracle WebCenter Content domain Now that you have your Docker images and you have created your RCU schemas, you are ready to create your domain. To continue, follow the instructions in Create Oracle WebCenter Content domains.\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/appendix/quickstart-deployment-guide/",
	"title": "Quick start deployment guide",
	"tags": [],
	"description": "Describes how to quickly get an Oracle WebCenter Content domain instance running (using the defaults, nothing special) for development and test purposes.",
	"content": "Use this Quick Start to create an Oracle WebCenter Content domain deployment in a Kubernetes cluster (on-premise environments) with WebLogic Kubernetes Operator. Note that this walkthrough is for demonstration purposes only, not for use in production. These instructions assume that you are already familiar with Kubernetes. If you need more detailed instructions, refer to the Install Guide.\nHardware requirements Supported Linux kernel for deploying and running Oracle WebCenter Content domain with the WebLogic Kubernetes Operator is Oracle Linux 7 (UL6+) and Red Hat Enterprise Linux 7 (UL3+ only with standalone Kubernetes). Refer to the prerequisites for more details.\nFor this exercise the minimum hardware requirement to create a single node Kubernetes cluster and deploy Oracle WebCenter Content domain with one UCM and IBR Cluster each.\n   Hardware Size     RAM 32GB   Disk Space 250GB+   CPU core(s) 6    See here for resourse sizing information for Oracle WebCenter Content domain setup on Kubernetes cluster.\nSet up Oracle WebCenter Content in an on-premise environment Perform the steps in this topic to create a single instance on-premise Kubernetes cluster and create an Oracle WebCenter Content domain which deploys Oracle WebCenter Content Server and Oracle WebCenter Inbound Refinery Server.\n Step 1 - Prepare a virtual machine for the Kubernetes cluster Step 2 - Set up a single instance Kubernetes cluster Step 3 - Get scripts and images Step 4 - Install the WebLogic Kubernetes Operator Step 5 - Install the Traefik (ingress-based) load balancer Step 6 - Create and configure an Oracle WebCenter Content Domain  1. Prepare a virtual machine for the Kubernetes cluster For illustration purposes, these instructions are for Oracle Linux 7u6+. If you are using a different flavor of Linux, you will need to adjust the steps accordingly.\nThese steps must be run with the root user, unless specified otherwise. Any time you see YOUR_USERID in a command, you should replace it with your actual userid.\n 1.1 Prerequisites   Choose the directories where your Docker and Kubernetes files will be stored. The Docker directory should be on a disk with a lot of free space (more than 100GB) because it will be used for the Docker file system, which contains all of your images and containers. The Kubernetes directory is used for the /var/lib/kubelet file system and persistent volume storage.\n$ export docker_dir=/u01/docker $ export kubelet_dir=/u01/kubelet $ mkdir -p $docker_dir $kubelet_dir $ ln -s $kubelet_dir /var/lib/kubelet   Verify that IPv4 forwarding is enabled on your host.\nNote: Replace eth0 with the ethernet interface name of your compute resource if it is different.\n$ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.docker0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.eth0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.lo.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.ip_nonlocal_bind' For example: Verify that all are set to 1\n$ net.ipv4.conf.docker0.forwarding = 1 $ net.ipv4.conf.eth0.forwarding = 1 $ net.ipv4.conf.lo.forwarding = 1 $ net.ipv4.ip_nonlocal_bind = 1 Solution: Set all values to 1 immediately with the following commands:\n$ /sbin/sysctl net.ipv4.conf.docker0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.eth0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.lo.forwarding=1 $ /sbin/sysctl net.ipv4.ip_nonlocal_bind=1 To preserve the settings post-reboot: Update the above values to 1 in files in /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/\n  Verify the iptables rule for forwarding.\nKubernetes uses iptables to handle many networking and port forwarding rules. A standard Docker installation may create a firewall rule that prevents forwarding.\nVerify if the iptables rule to accept forwarding traffic is set:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot; If the output is \u0026ldquo;DROP\u0026rdquo;, then run the following command:\n$ /sbin/iptables -P FORWARD ACCEPT Verify if the iptables rule is set properly to \u0026ldquo;ACCEPT\u0026rdquo;:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot;   Disable and stop firewalld:\n$ systemctl disable firewalld $ systemctl stop firewalld   1.2 Install and configure Docker  Note : If you have already installed Docker with version 18.03+ and configured Docker daemon root to sufficient disk space along with proxy settings, continue to Install and configure Kubernetes\n   Make sure that you have the right operating system version:\n$ uname -a $ more /etc/oracle-release For example:\nLinux xxxxxxx 4.1.12-124.27.1.el7uek.x86_64 #2 SMP Mon May 13 08:56:17 PDT 2019 x86_64 x86_64 x86_64 GNU/Linux Oracle Linux Server release 7.6   Install the latest docker-engine and start the Docker service:\n$ yum-config-manager --enable ol7_addons $ yum install docker-engine $ systemctl enable docker $ systemctl start docker   Add your userid to the Docker group. This will allow you to run the Docker commands without root access:\n$ /sbin/usermod -a -G docker \u0026lt;YOUR_USERID\u0026gt;   Check your Docker version. It must be at least 18.03.\n$ docker version For example:\nClient: Docker Engine - Community Version: 19.03.1-ol API version: 1.40 Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:40:28 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.1-ol API version: 1.40 (minimum version 1.12) Go version: go1.12.5 Git commit: ead9442 Built: Wed Sep 11 06:38:43 2019 OS/Arch: linux/amd64 Experimental: false Default Registry: docker.io containerd: Version: v1.2.0-rc.0-108-gc444666 GitCommit: c4446665cb9c30056f4998ed953e6d4ff22c7c39 runc: Version: 1.0.0-rc5+dev GitCommit: 4bb1fe4ace1a32d3676bb98f5d3b6a4e32bf6c58 docker-init: Version: 0.18.0 GitCommit: fec3683   Update the Docker engine configuration:\n$ mkdir -p /etc/docker $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/docker/daemon.json { \u0026quot;group\u0026quot;: \u0026quot;docker\u0026quot;, \u0026quot;data-root\u0026quot;: \u0026quot;/u01/docker\u0026quot; } EOF   Configure proxy settings if you are behind an HTTP proxy. On some hosts /etc/systemd/system/docker.service.d may not be available. Create this directory if it is not available.\n ### Create the drop-in file /etc/systemd/system/docker.service.d/http-proxy.conf that contains proxy details: $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=\u0026quot;HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock\u0026quot; EOF   Restart the Docker daemon to load the latest changes:\n$ systemctl daemon-reload $ systemctl restart docker   Verify that the proxy is configured with Docker:\n$ docker info|grep -i proxy For example:\nHTTP Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT HTTPS Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT No Proxy: localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock   Verify Docker installation:\n$ docker run hello-world For example:\nHello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   1.3 Install and configure Kubernetes   Add the external Kubernetes repository:\n$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF   Set SELinux in permissive mode (effectively disabling it):\n$ export PATH=/sbin:$PATH $ setenforce 0 $ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config   Export proxy and install kubeadm, kubelet, and kubectl:\n### Get the nslookup IP address of the master node to use with apiserver-advertise-address during setting up Kubernetes master ### as the host may have different internal ip (hostname -i) and nslookup $HOSTNAME $ ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` $ echo $ip_addr ### Set the proxies $ export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT ### install kubernetes 1.18.4-1 $ VERSION=1.18.4-1 $ yum install -y kubelet-$VERSION kubeadm-$VERSION kubectl-$VERSION --disableexcludes=kubernetes ### enable kubelet service so that it auto-restart on reboot $ systemctl enable --now kubelet   Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl to avoid traffic routing issues:\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl --system   Disable swap check:\n$ sed -i 's/KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=\u0026quot;--fail-swap-on=false\u0026quot;/' /etc/sysconfig/kubelet $ cat /etc/sysconfig/kubelet ### Reload and restart kubelet $ systemctl daemon-reload $ systemctl restart kubelet   1.4 Set up Helm   Install Helm v3.x.\na. Download Helm from https://github.com/helm/helm/releases. Example to download Helm v3.2.4:\n$ wget https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz b. Unpack tar.gz:\n$ tar -zxvf helm-v3.2.4-linux-amd64.tar.gz c. Find the Helm binary in the unpacked directory, and move it to its desired destination:\n$ mv linux-amd64/helm /usr/bin/helm   Run helm version to verify its installation:\n$ helm version version.BuildInfo{Version:\u0026quot;v3.2.4\u0026quot;, GitCommit:\u0026quot;0ad800ef43d3b826f31a5ad8dfbb4fe05d143688\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, GoVersion:\u0026quot;go1.13.12\u0026quot;}   2. Set up a single instance Kubernetes cluster  Notes:\n These steps must be run with the root user, unless specified otherwise! If you choose to use a different cidr block (that is, other than 10.244.0.0/16 for the --pod-network-cidr= in the kubeadm init command), then also update NO_PROXY and no_proxy with the appropriate value.  Also make sure to update kube-flannel.yaml with the new value before deploying.   Replace the following with appropriate values:  ADD-YOUR-INTERNAL-NO-PROXY-LIST REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT     2.1 Set up the master node   Create a shell script that sets up the necessary environment variables. You can append this to the user’s .bashrc so that it will run at login. You must also configure your proxy settings here if you are behind an HTTP proxy:\n## grab my IP address to pass into kubeadm init, and to add to no_proxy vars ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` export pod_network_cidr=\u0026quot;10.244.0.0/16\u0026quot; export service_cidr=\u0026quot;10.96.0.0/12\u0026quot; export PATH=$PATH:/sbin:/usr/sbin ### Set the proxies export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT   Source the script to set up your environment variables:\n$ . ~/.bashrc   To implement command completion, add the following to the script:\n$ [ -f /usr/share/bash-completion/bash_completion ] \u0026amp;\u0026amp; . /usr/share/bash-completion/bash_completion $ source \u0026lt;(kubectl completion bash)   Run kubeadm init to create the master node:\n$ kubeadm init \\ --pod-network-cidr=$pod_network_cidr \\ --apiserver-advertise-address=$ip_addr \\ --ignore-preflight-errors=Swap \u0026gt; /tmp/kubeadm-init.out 2\u0026gt;\u0026amp;1   Log in to the terminal with YOUR_USERID:YOUR_GROUP. Then set up the ~/.bashrc similar to steps 1 to 3 with YOUR_USERID:YOUR_GROUP.\n Note that from now on we will be using YOUR_USERID:YOUR_GROUP to execute any kubectl commands and not root.\n   Set up YOUR_USERID:YOUR_GROUP to access the Kubernetes cluster:\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config   Verify that YOUR_USERID:YOUR_GROUP is set up to access the Kubernetes cluster using the kubectl command:\n$ kubectl get nodes  Note: At this step, the node is not in ready state as we have not yet installed the pod network add-on. After the next step, the node will show status as Ready.\n   Install a pod network add-on (flannel) so that your pods can communicate with each other.\n Note: If you are using a different cidr block than 10.244.0.0/16, then download and update kube-flannel.yml with the correct cidr address before deploying into the cluster:\n $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.12.0/Documentation/kube-flannel.yml   Verify that the master node is in Ready status:\n$ kubectl get nodes For example:\nNAME STATUS ROLES AGE VERSION mymasternode Ready master 8m26s v1.18.4 or:\n$ kubectl get pods -n kube-system For example:\nNAME READY STATUS RESTARTS AGE pod/coredns-86c58d9df4-58p9f 1/1 Running 0 3m59s pod/coredns-86c58d9df4-mzrr5 1/1 Running 0 3m59s pod/etcd-mymasternode 1/1 Running 0 3m4s pod/kube-apiserver-node 1/1 Running 0 3m21s pod/kube-controller-manager-mymasternode 1/1 Running 0 3m25s pod/kube-flannel-ds-amd64-6npx4 1/1 Running 0 49s pod/kube-proxy-4vsgm 1/1 Running 0 3m59s pod/kube-scheduler-mymasternode 1/1 Running 0 2m58s   To schedule pods on the master node, taint the node:\n$ kubectl taint nodes --all node-role.kubernetes.io/master-   Congratulations! Your Kubernetes cluster environment is ready to deploy your Oracle WebCenter Content domain.\nFor additional references on Kubernetes cluster setup, check the cheat sheet.\n3. Get scripts and images 3.1 Set up the code repository to deploy Oracle WebCenter Content domains Follow these steps to set up the source code repository required to deploy Oracle WebCenter Content domains.\n3.2 Get required Docker images and add them to your local registry Follow these steps to set up the source code repository required to deploy Oracle WebCenter Content domains.\n3.3 Build Oracle WebCenter Content Docker image and add it to your local registry Follow these steps to set up the source code repository required to deploy Oracle WebCenter Content domains.\n Note: For test and development purposes this Oracle WebCenter Content image need not contain any product patches.\n 4. Install WebLogic Kubernetes Operator 4.1 Prepare for WebLogic Kubernetes Operator.   Create a namespace opns for the WebLogic Kubernetes Operator:\n$ kubectl create namespace opns   Create a service account op-sa for WebLogic Kubernetes Operator in the operator’s namespace:\n$ kubectl create serviceaccount -n opns op-sa   4.2 Install the WebLogic Kubernetes Operator Use Helm to install and start WebLogic Kubernetes Operator from the directory you just cloned:\n $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --namespace opns \\ --set image=oracle/weblogic-kubernetes-operator:3.3.0 \\ --set serviceAccount=op-sa \\ --set \u0026quot;domainNamespaces={}\u0026quot; \\ --wait 4.3 Verify the WebLogic Kubernetes Operator   Verify that the WebLogic Kubernetes Operator\u0026rsquo;s pod is running by listing the pods in the respective namespace. You should see one for the WebLogic Kubernetes Operator:\n$ kubectl get pods -n opns   Verify that the WebLogic Kubernetes Operator is up and running by viewing the operator-pod\u0026rsquo;s logs:\n$ kubectl logs -n opns -c weblogic-operator deployments/weblogic-operator   The WebLogic Kubernetes Operator v3.3.0 has been installed. Continue with the load balancer and Oracle WebCenter Content domain setup.\n5. Install the Traefik (ingress-based) load balancer WebLogic Kubernetes Operator supports these load balancers: Traefik, NGINX and Apache. Samples are provided in the documentation.\nThis Quick Start demonstrates how to install the Traefik ingress controller to provide load balancing for an Oracle WebCenter Content domain.\n  Create a namespace for Traefik:\n$ kubectl create namespace traefik   Set up Helm for 3rd party services:\n$ helm repo add traefik https://containous.github.io/traefik-helm-chart   Install the Traefik operator in the traefik namespace with the provided sample values:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install traefik traefik/traefik \\ --namespace traefik \\ --values kubernetes/samples/scripts/charts/traefik/values.yaml \\ --set \u0026quot;kubernetes.namespaces={traefik}\u0026quot; \\ --set \u0026quot;service.type=NodePort\u0026quot; \\ --wait   6. Create and configure an Oracle WebCenter Content domain 6.1 Prepare for an Oracle WebCenter Content domain   Create a namespace that can host Oracle WebCenter Content domain:\n$ kubectl create namespace wccns   Use Helm to configure the WebLogic Kubernetes Operator to manage Oracle WebCenter Content domains in this namespace:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm upgrade weblogic-kubernetes-operator kubernetes/charts/weblogic-operator \\ --reuse-values \\ --namespace opns \\ --set \u0026quot;domainNamespaces={wccns}\u0026quot; \\ --wait   Create Kubernetes secrets.\na. Create a Kubernetes secret for the domain in the same Kubernetes namespace as the domain. In this example, the username is weblogic, the password in welcome1, and the namespace is wccns:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh \\ -u weblogic \\ -p welcome1 \\ -n wccns \\ -d wccinfra \\ -s wccinfra-domain-credentials b. Create a Kubernetes secret for the RCU in the same Kubernetes namespace as the domain:\n Schema user : WCC1 Schema password : Oradoc_db1 DB sys user password : Oradoc_db1 Domain name : wccinfra Domain Namespace : wccns Secret name : wccinfra-rcu-credentials  $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh \\ -u WCC1 \\ -p Oradoc_db1 \\ -a sys \\ -q Oradoc_db1 \\ -d wccinfra \\ -n wccns \\ -s wccinfra-rcu-credentials   Create the Kubernetes persistence volume and persistence volume claim.\na. Create the Oracle WebCenter Content domain home directory. Determine if a user already exists on your host system with uid:gid of 1000:0:\n$ sudo getent passwd 1000 If this command returns a username (which is the first field), you can skip the following useradd command. If not, create the oracle user with useradd:\n$ sudo useradd -u 1000 -g 0 oracle Create the directory that will be used for the Oracle WebCenter Content domain home:\n$ sudo mkdir /scratch/k8s_dir $ sudo chown -R 1000:0 /scratch/k8s_dir b. Update create-pv-pvc-inputs.yaml with the following values:\n baseName: domain domainUID: wccinfra namespace: wccns weblogicDomainStoragePath: /scratch/k8s_dir  $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ sed -i -e \u0026quot;s:baseName\\: weblogic-sample:baseName\\: domain:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:domainUID\\::domainUID\\: wccinfra:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:namespace\\: default:namespace\\: wccns:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:#weblogicDomainStoragePath\\: /scratch/k8s_dir:weblogicDomainStoragePath\\: /scratch/k8s_dir:g\u0026quot; create-pv-pvc-inputs.yaml c. Run the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output d. Create the PV and PVC using the configuration files created in the previous step:\n$ kubectl create -f output/pv-pvcs/wccinfra-domain-pv.yaml $ kubectl create -f output/pv-pvcs/wccinfra-domain-pvc.yaml   Configure the database and create schemas for the Oracle WebCenter Content domain.\nFollow configure-database-access step and run-RCU step to set up the database connection and configure product schemas required to deploy Oracle WebCenter Content domain.\n  Now the environment is ready to start the Oracle WebCenter Content domain creation.\n6.2 Create an Oracle WebCenter Content domain   The sample scripts for Oracle WebCenter Content domain deployment are available at \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts/create-wcc-domain. You must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain.\n  Run the create-domain.sh script to create a domain:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ ./create-domain.sh -i create-domain-inputs.yaml -o output   Create a Kubernetes domain object:\nOnce the create-domain.sh is successful, it generates the output/weblogic-domains/wccinfra/domain.yaml that you can use to create the Kubernetes resource domain, which starts the domain and servers:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv $ kubectl create -f output/weblogic-domains/wccinfra/domain.yaml   Verify that the Kubernetes domain object named wccinfra is created:\n$ kubectl get domain -n wccns NAME AGE wccinfra 3m18s   Once you create the domain, introspect pod is created. This inspects the domain home and then starts the wccinfra-adminserver pod. Once the wccinfra-adminserver pod starts successfully, then the Managed Server pods are started in parallel. Watch the wccns namespace for the status of domain creation:\n$ kubectl get pods -n wccns   Verify that the Oracle WebCenter Content domain server pods and services are created and in Ready state:\n$ kubectl get all -n wccns   6.3 Configure Traefik to access in Oracle WebCenter Content domain services   Configure Traefik to manage ingresses created in the Oracle WebCenter Content domain namespace (wccns):\n$ helm upgrade traefik traefik/traefik \\ --reuse-values \\ --namespace traefik \\ --set \u0026quot;kubernetes.namespaces={traefik,wccns}\u0026quot; \\ --wait   Create an ingress for the domain in the domain namespace by using the sample Helm chart:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wcc-traefik-ingress kubernetes/samples/charts/ingress-per-domain \\ --namespace wccns \\ --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\ --set \u0026quot;traefik.hostname=$(hostname -f)\u0026quot;   Verify the created ingress per domain details:\n$ kubectl describe ingress wccinfra-traefik -n wccns   6.4 Verify that you can access the Oracle WebCenter Content domain URL   Get the LOADBALANCER_HOSTNAME for your environment:\nexport LOADBALANCER_HOSTNAME=$(hostname -f)   The following URLs are available for Oracle WebCenter Content domain:\nCredentials: username: weblogic password: welcome1\nhttp://${LOADBALANCER_HOSTNAME}:30305/console http://${LOADBALANCER_HOSTNAME}:30305/em http://${LOADBALANCER_HOSTNAME}:30305/cs http://${LOADBALANCER_HOSTNAME}:30305/ibr http://${LOADBALANCER_HOSTNAME}:30305/imaging http://${LOADBALANCER_HOSTNAME}:30305/dc-console http://${LOADBALANCER_HOSTNAME}:30305/wcc   "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/adminguide/configure-load-balancer/nginx/",
	"title": "NGINX",
	"tags": [],
	"description": "Configure the ingress-based NGINX load balancer for Oracle WebCenter Content domain.",
	"content": "This section provides information about how to install and configure the ingress-based NGINX load balancer to load balance Oracle WebCenter Content domain clusters. You can configure NGINX for non-SSL, SSL termination, and end-to-end SSL access of the application URL.\nFollow these steps to set up NGINX as a load balancer for an Oracle WebCenter Content domain in a Kubernetes cluster:\nSee the official installation document for prerequisites.\n  Non-SSL and SSL termination\n Install the NGINX load balancer Configure NGINX to manage ingresses Verify non-SSL and SSL termination access    End-to-end SSL configuration\n Install the NGINX load balancer for End-to-end SSL Deploy tls to access individual Managed Servers Deploy tls to access Administration Server    To get repository information, enter the following Helm commands:\n$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update Non-SSL and SSL termination Install the NGINX load balancer   Deploy the ingress-nginx controller by using Helm on the domain namespace:\nFor Kubernetes versions up to v1.18.x:\n$ helm install nginx-ingress -n wccns \\  --version=3.34.0 \\  --set controller.service.type=NodePort \\  --set controller.admissionWebhooks.enabled=false \\  ingress-nginx/ingress-nginx     Click here to see the sample output.   NAME: nginx-ingress LAST DEPLOYED: Sun Feb 7 23:19:30 2021 NAMESPACE: wccns STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace wccns get services -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export HTTPS_NODE_PORT=$(kubectl --namespace wccns get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace wccns get nodes -o jsonpath=\u0026#34;{.items[0].status.addresses[1].address}\u0026#34;) echo \u0026#34;Visit http://$NODE_IP:$HTTP_NODE_PORTto access your application via HTTP.\u0026#34; echo \u0026#34;Visit https://$NODE_IP:$HTTPS_NODE_PORTto access your application via HTTPS.\u0026#34; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Check the status of the deployed ingress controller:\n$ kubectl --namespace wccns get services | grep ingress-nginx-controller Sample output:\nnginx-ingress-ingress-nginx-controller NodePort 10.97.189.122 \u0026lt;none\u0026gt; 80:30993/TCP,443:30232/TCP 7d2h   Configure NGINX to manage ingresses  Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK, tls is Non-SSL, and domainType is wccinfra. These values can be overridden by passing values through the command line or can be edited in the sample file values.yaml. If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. Update the template YAML file for the NGINX load balancer located at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/templates/nginx-ingress.yaml  $ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wccinfra-nginx-ingress kubernetes/samples/charts/ingress-per-domain \\  --namespace wccns \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=$(hostname -f)\u0026#34; \\  --set type=NGINX \\  --set tls=NONSSL Sample output:\nNAME: wccinfra-nginx-ingress LAST DEPLOYED: Sun Feb 7 23:52:38 2021 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None   For secured access (SSL) to the Oracle WebCenter Content application, create a certificate and generate a Kubernetes secret:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n wccns create secret tls domain1-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Install ingress-per-domain using Helm for SSL configuration:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm install wccinfra-nginx-ingress kubernetes/samples/charts/ingress-per-domain \\  --namespace wccns \\  --values kubernetes/samples/charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=$(hostname -f)\u0026#34; \\  --set type=NGINX --set tls=SSL Sample output:\nNAME: wccinfra-nginx-ingress LAST DEPLOYED: Mon Feb 8 00:01:13 2021 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None   For non-SSL access or SSL to the Oracle WebCenter Content application, get the details of the services by the ingress:\n$ kubectl describe ingress wccinfra-nginx -n wccns     Click here to see the sample output of the services supported by the above deployed ingress.   Name: wccinfra-nginx Namespace: wccns Address: 10.97.189.122 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) TLS: domain1-tls-cert terminates domain1.org Rules: Host Path Backends ---- ---- -------- domain1.org /console wccinfra-adminserver:7001 (10.244.0.58:7001) /em wccinfra-adminserver:7001 (10.244.0.58:7001) /servicebus wccinfra-adminserver:7001 (10.244.0.58:7001) /cs wccinfra-cluster-ucm-cluster:16200 (10.244.0.60:16200,10.244.0.61:16200) /adfAuthentication wccinfra-cluster-ucm-cluster:16200 (10.244.0.60:16200,10.244.0.61:16200) /ibr wccinfra-cluster-ibr-cluster:16250 (10.244.0.59:16250) /ibr/adfAuthentication wccinfra-cluster-ibr-cluster:16250 (10.244.0.59:16250) /weblogic/ready wccinfra-cluster-ucm-cluster:16200 (10.244.0.60:16200,10.244.0.61:16200) /imaging wccinfra-cluster-ipm-cluster:16000 (10.244.0.206:16000,10.244.0.209:16000,10.244.0.213:16000) /dc-console wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /dc-client wccinfra-cluster-capture-cluster:16400 (10.244.0.204:16400,10.244.0.208:16400,10.244.0.212:16400) /wcc wccinfra-cluster-wccadf-cluster:16225 (10.244.0.205:16225,10.244.0.210:16225,10.244.0.214:16225) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: wccinfra-nginx-ingress meta.helm.sh/release-namespace: wccns nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026#34;X-Forwarded-Proto: https\u0026#34;; more_set_input_headers \u0026#34;WL-Proxy-SSL: true\u0026#34;; nginx.ingress.kubernetes.io/ingress.allow-http: false Events: \u0026lt;none\u0026gt;    Verify non-SSL and SSL termination access Non-SSL configuration Verify that the Oracle WebCenter Content domain application URLs are accessible through the LOADBALANCER-Non-SSLPORT:\nhttp://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/cs http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/ibr http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/imaging http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/dc-console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/wcc SSL configuration Verify that the Oracle WebCenter Content domain application URLs are accessible through the LOADBALANCER-SSLPORT:\nhttps://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/weblogic/ready https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/cs https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/ibr https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/imaging https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/dc-console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/wcc Uninstall the ingress Uninstall and delete the ingress-nginx deployment:\n$ helm delete wccinfra-nginx -n wccns End-to-end SSL configuration Install the NGINX load balancer for End-to-end SSL   For secured access (SSL) to the Oracle WebCenter Content application, create a certificate and generate secrets:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n wccns create secret tls domain1-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Deploy the ingress-nginx controller by using Helm on the domain namespace:\nFor Kubernetes versions up to v1.18.x:\n$ helm install nginx-ingress -n wccns \\  --version=3.34.0 \\  --set controller.extraArgs.default-ssl-certificate=wccns/domain1-tls-cert \\  --set controller.service.type=NodePort \\  --set controller.admissionWebhooks.enabled=false \\  --set controller.extraArgs.enable-ssl-passthrough=true \\  ingress-nginx/ingress-nginx\t   Click here to see the sample output.   Release \u0026#34;nginx-ingress\u0026#34; has been upgraded. Happy Helming! NAME: nginx-ingress LAST DEPLOYED: Mon Feb 8 02:07:26 2021 NAMESPACE: wccns STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace wccns get services -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export HTTPS_NODE_PORT=$(kubectl --namespace wccns get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace wccns get nodes -o jsonpath=\u0026#34;{.items[0].status.addresses[1].address}\u0026#34;) echo \u0026#34;Visit http://$NODE_IP:$HTTP_NODE_PORTto access your application via HTTP.\u0026#34; echo \u0026#34;Visit https://$NODE_IP:$HTTPS_NODE_PORTto access your application via HTTPS.\u0026#34; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Check the status of the deployed ingress controller:\n$ kubectl --namespace wccns get services | grep ingress-nginx-controller Sample output:\nnginx-ingress-ingress-nginx-controller NodePort 10.97.189.122 \u0026lt;none\u0026gt; 80:30993/TCP,443:30232/TCP 168m   Deploy tls to access individual Managed Servers   Deploy tls to securely access the services. Only one application can be configured with ssl-passthrough. A sample tls file for NGINX is shown below for the service wccinfra-cluster-ucm-cluster and port 16201. All the applications running on port 16201 can be securely accessed through this ingress. For each backend service, create different ingresses as NGINX does not support multiple path/rules with annotation ssl-passthrough. That is, for wccinfra-cluster-ucm-cluster, wccinfra-cluster-ibr-cluster, wccinfra-cluster-ipm-cluster, wccinfra-cluster-capture-cluster, wccinfra-cluster-wccadf-cluster and wccinfra-adminserver, different ingresses must be created.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls Sample nginx-ucm-tls.yaml:\n  Click here to see the content of the file nginx-ucm-tls.yaml   apiVersion: extensions/v1beta1 kind: Ingress metadata: name: wcc-ucm-ingress namespace: wccns annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; spec: tls: - hosts: - \u0026#39;domain1.org\u0026#39; secretName: domain1-tls-cert rules: - host: \u0026#39;domain1.org\u0026#39; http: paths: - path: backend: serviceName: wccinfra-cluster-ucm-cluster servicePort: 16201     Note: host is the server on which this ingress is deployed.\n   Deploy the secured ingress:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls $ kubectl create -f nginx-ucm-tls.yaml   Check the services supported by the ingress:\n$ kubectl describe ingress wcc-ucm-ingress -n wccns    Click here check the services supported by the ingress.   Name: wcc-ucm-ingress Namespace: wccns Address: 10.102.97.237 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026#34;default-http-backend\u0026#34; not found\u0026gt;) TLS: domain1-tls-cert terminates domain1.org Rules: Host Path Backends ---- ---- -------- domain1.org wccinfra-cluster-ucm-cluster:16201 (10.244.238.136:16201,10.244.253.132:16201) Annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: true Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 62s (x2 over 106s) nginx-ingress-controller Scheduled for sync      Verify end-to-end SSL access Verify that the Oracle WebCenter Content domain application URLs are accessible through the LOADBALANCER-SSLPORT:\nhttps://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/cs Deploy tls to access Administration Server   As ssl-passthrough in NGINX works on the clusterIP of the backing service instead of individual endpoints, you must expose adminserver service created by the WebLogic Kubernetes Operator with clusterIP.\nFor example:\na. Get the name of Administration Server service:\n$ kubectl get svc -n wccns | grep wccinfra-adminserver Sample output:\nwccinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 7 b. Expose the Administration Server service wccinfra-adminserver and use the new service name wccinfra-adminserver-nginx-ssl:\n$ kubectl expose svc wccinfra-adminserver -n wccns --name=wccinfra-adminserver-nginx-ssl --port=7002 c. Deploy the secured ingress:\nSample nginx-admin-tls.yaml:\n  Click here to see the content of the file nginx-admin-tls.yaml   apiVersion: extensions/v1beta1 kind: Ingress metadata: name: wcc-admin-ingress namespace: wccns annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; spec: tls: - hosts: - \u0026#39;domain1.org\u0026#39; secretName: domain1-tls-cert rules: - host: \u0026#39;domain1.org\u0026#39; http: paths: - path: backend: serviceName: wccinfra-adminserver-nginx-ssl servicePort: 7002     Note: host is the server on which this ingress is deployed.\n $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls $ kubectl create -f nginx-admin-tls.yaml   Verify end-to-end SSL access Verify that the Oracle WebCenter Content Administration Server URL is accessible through the LOADBALANCER-SSLPORT:\nhttps://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console Uninstall ingress-nginx tls $ cd weblogic-kubernetes-operator/kubernetes/samples/charts/ingress-per-domain/tls $ kubectl delete -f nginx-ucm-tls.yaml "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/installguide/",
	"title": "Install Guide",
	"tags": [],
	"description": "",
	"content": "Install the WebLogic Kubernetes operator and prepare and deploy the Oracle WebCenter Portal domain.\n Requirements and limitations  Understand the system requirements and limitations for deploying and running Oracle WebCenter Portal with the WebLogic Kubernetes operator.\n Prepare your environment  Prepare for creating the Oracle WebCenter Portal domain, This preparation includes but not limited to creating required secrets, persistent volume and volume claim, and database schema.\n Create WebCenter Portal domain  Create an Oracle WebCenter Portal domain home on an existing PV or PVC, and create the domain resource YAML file for deploying the generated Oracle WebCenter Portal domain.\n Configure WebCenter Portal For Search  Set up search functionality in Oracle WebCenter Portal using Elasticsearch.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/appendix/quickstart-deployment-on-prem/",
	"title": "Quick start deployment on-premise",
	"tags": [],
	"description": "Describes how to quickly get an Oracle WebCenter Portal domain instance running (using the defaults, nothing special) for development and test purposes.",
	"content": "Use this Quick Start to create an Oracle WebCenter Portal domain deployment in a Kubernetes cluster (on-premise environments) with the WebLogic Kubernetes Operator. Note that this walkthrough is for demonstration purposes only, not for use in production. These instructions assume that you are already familiar with Kubernetes. If you need more detailed instructions, refer to the Install Guide.\nHardware requirements The Linux kernel supported for deploying and running Oracle WebCenter Portal domains with the operator is Oracle Linux 7 (UL6+) and Red Hat Enterprise Linux 7 (UL3+ only with standalone Kubernetes). Refer to the prerequisites for more details.\nFor this exercise, the minimum hardware requirements to create a single-node Kubernetes cluster and then deploy the domain type with one Managed Server along with Oracle Database running as a container are:\n   Hardware Size     RAM 32GB   Disk Space 250GB+   CPU core(s) 6    See here for resource sizing information for Oracle WebCenter Portal domain set up on a Kubernetes cluster.\nSet up Oracle WebCenter Portal in an on-premise environment Use the steps in this topic to create a single-instance on-premise Kubernetes cluster and then create an Oracle WebCenter Portal domain.\n Step 1 - Prepare a virtual machine for the Kubernetes cluster Step 2 - Set up a single instance Kubernetes cluster Step 3 - Get scripts and images Step 4 - Install the WebLogic Kubernetes operator Step 5 - Install the Traefik (ingress-based) load balancer Step 6 - Create and configure an Oracle WebCenter Portal domain  1. Prepare a virtual machine for the Kubernetes cluster For illustration purposes, these instructions are for Oracle Linux 7u6+. If you are using a different flavor of Linux, you will need to adjust the steps accordingly.\nThese steps must be run with the root user, unless specified otherwise. Any time you see YOUR_USERID in a command, you should replace it with your actual userid.\n 1.1 Prerequisites   Choose the directories where your Docker and Kubernetes files will be stored. The Docker directory should be on a disk with a lot of free space (more than 100GB) because it will be used for the Docker file system, which contains all of your images and containers. The Kubernetes directory is used for the /var/lib/kubelet file system and persistent volume storage.\n$ export docker_dir=/u01/docker $ export kubelet_dir=/u01/kubelet $ mkdir -p $docker_dir $kubelet_dir $ ln -s $kubelet_dir /var/lib/kubelet   Verify that IPv4 forwarding is enabled on your host.\nNote: Replace eth0 with the ethernet interface name of your compute resource if it is different.\n$ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.docker0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.eth0.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.conf.lo.forwarding' $ /sbin/sysctl -a 2\u0026gt;\u0026amp;1|grep -s 'net.ipv4.ip_nonlocal_bind' For example: Verify that all are set to 1:\n$ net.ipv4.conf.docker0.forwarding = 1 $ net.ipv4.conf.eth0.forwarding = 1 $ net.ipv4.conf.lo.forwarding = 1 $ net.ipv4.ip_nonlocal_bind = 1 Solution: Set all values to 1 immediately:\n$ /sbin/sysctl net.ipv4.conf.docker0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.eth0.forwarding=1 $ /sbin/sysctl net.ipv4.conf.lo.forwarding=1 $ /sbin/sysctl net.ipv4.ip_nonlocal_bind=1   To preserve the settings permanently: Update the above values to 1 in files in /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/.\n  Verify the iptables rule for forwarding.\nKubernetes uses iptables to handle many networking and port forwarding rules. A standard Docker installation may create a firewall rule that prevents forwarding.\nVerify if the iptables rule to accept forwarding traffic is set:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot; If the output is \u0026ldquo;DROP\u0026rdquo;, then run the following command:\n$ /sbin/iptables -P FORWARD ACCEPT Verify if the iptables rule is properly set to \u0026ldquo;ACCEPT\u0026rdquo;:\n$ /sbin/iptables -L -n | awk '/Chain FORWARD / {print $4}' | tr -d \u0026quot;)\u0026quot;   Disable and stop firewalld:\n$ systemctl disable firewalld $ systemctl stop firewalld   1.2 Install and configure Docker  Note: If you have already installed Docker with version 18.03+ and configured the Docker daemon root to sufficient disk space along with proxy settings, continue to Install and configure Kubernetes.\n   Make sure that you have the right operating system version:\n$ uname -a $ more /etc/oracle-release Example output:\nLinux xxxxxxx 4.1.12-124.27.1.el7uek.x86_64 #2 SMP Mon May 13 08:56:17 PDT 2019 x86_64 x86_64 x86_64 GNU/Linux Oracle Linux Server release 7.6   Install the latest docker-engine and start the Docker service:\n$ yum-config-manager --enable ol7_addons $ docker_version=\u0026quot;19.03.11-ol\u0026quot; $ yum install docker-engine-$docker_version $ systemctl enable docker $ systemctl start docker   Add your user ID to the Docker group to allow you to run Docker commands without root access:\n$ /sbin/usermod -a -G docker \u0026lt;YOUR_USERID\u0026gt;   Check that your Docker version is at least 18.03:\n$ docker version Example output:\nClient: Docker Engine - Community Version: 19.03.11-ol API version: 1.40 Go version: go1.15.5 Git commit: 748876d Built: Thu Dec 3 19:36:03 2020 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.11-ol API version: 1.40 (minimum version 1.12) Go version: go1.15.8 Git commit: f0aae77 Built: Wed Feb 10 16:13:32 2021 OS/Arch: linux/amd64 Experimental: false Default Registry: docker.io containerd: Version: v1.3.9 GitCommit: runc: Version: 1.0.0-rc5+dev GitCommit: 4bb1fe4ace1a32d3676bb98f5d3b6a4e32bf6c58 docker-init: Version: 0.18.0 GitCommit: fec3683   Update the Docker engine configuration:\n$ mkdir -p /etc/docker $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/docker/daemon.json { \u0026quot;group\u0026quot;: \u0026quot;docker\u0026quot;, \u0026quot;data-root\u0026quot;: \u0026quot;/u01/docker\u0026quot; } EOF   Configure proxy settings if you are behind an HTTP proxy:\n ### Create the drop-in file /etc/systemd/system/docker.service.d/http-proxy.conf that contains proxy details: $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=\u0026quot;HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT\u0026quot; Environment=\u0026quot;NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock\u0026quot; EOF  Note: On some hosts /etc/systemd/system/docker.service.d may not be available. Create this directory if it is not available.\n   Restart the Docker daemon to load the latest changes:\n$ systemctl daemon-reload $ systemctl restart docker   Verify that the proxy is configured with Docker:\n$ docker info|grep -i proxy Example output:\nHTTP Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT HTTPS Proxy: http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT No Proxy: localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock   Verify Docker installation:\n$ docker run hello-world Example output:\nHello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   1.3 Install and configure Kubernetes   Add the external Kubernetes repository:\n$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF   Set SELinux in permissive mode (effectively disabling it):\n$ export PATH=/sbin:$PATH $ setenforce 0 $ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config   Export proxy and install kubeadm, kubelet, and kubectl:\n### Get the nslookup IP address of the master node to use with apiserver-advertise-address during setting up Kubernetes master ### as the host may have different internal ip (hostname -i) and nslookup $HOSTNAME $ ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` $ echo $ip_addr ### Set the proxies $ export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr $ export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT $ export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT ### install kubernetes 1.20.10 $ VERSION=1.20.10 $ yum install -y kubelet-$VERSION kubeadm-$VERSION kubectl-$VERSION --disableexcludes=kubernetes ### enable kubelet service so that it auto-restart on reboot $ systemctl enable --now kubelet   Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl to avoid traffic routing issues:\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl --system   Disable swap check:\n$ sed -i 's/KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=\u0026quot;--fail-swap-on=false\u0026quot;/' /etc/sysconfig/kubelet $ cat /etc/sysconfig/kubelet ### Reload and restart kubelet $ systemctl daemon-reload $ systemctl restart kubelet   1.4 Set up Helm   Install Helm v3.4+\na. Download Helm from https://github.com/helm/helm/releases.\nFor example, to download Helm v3.4.1:\n$ wget https://get.helm.sh/helm-v3.4.1-linux-amd64.tar.gz b. Unpack tar.gz:\n$ tar -zxvf helm-v3.4.1-linux-amd64.tar.gz c. Find the Helm binary in the unpacked directory, and move it to its desired destination:\n$ mv linux-amd64/helm /usr/bin/helm   Run helm version to verify its installation:\n$ helm version version.BuildInfo{Version:\u0026quot;v3.4.1\u0026quot;, GitCommit:\u0026quot;0ad800ef43d3b826f31a5ad8dfbb4fe05d143688\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, GoVersion:\u0026quot;go1.13.12\u0026quot;}   2. Set up a single instance Kubernetes cluster  Notes:\n These steps must be run with the root user, unless specified otherwise! If you choose to use a different CIDR block (that is, other than 10.244.0.0/16 for the --pod-network-cidr= in the kubeadm init command), then also update NO_PROXY and no_proxy with the appropriate value.  Also make sure to update kube-flannel.yaml with the new value before deploying.   Replace the following with appropriate values:  ADD-YOUR-INTERNAL-NO-PROXY-LIST REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT     2.1 Set up the master node   Create a shell script that sets up the necessary environment variables. You can append this to the user’s .bashrc so that it will run at login. You must also configure your proxy settings here if you are behind an HTTP proxy:\n## grab my IP address to pass into kubeadm init, and to add to no_proxy vars ip_addr=`nslookup $(hostname -f) | grep -m2 Address | tail -n1| awk -F: '{print $2}'| tr -d \u0026quot; \u0026quot;` export pod_network_cidr=\u0026quot;10.244.0.0/16\u0026quot; export service_cidr=\u0026quot;10.96.0.0/12\u0026quot; export PATH=$PATH:/sbin:/usr/sbin ### Set the proxies export NO_PROXY=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export no_proxy=localhost,127.0.0.0/8,ADD-YOUR-INTERNAL-NO-PROXY-LIST,/var/run/docker.sock,$ip_addr,$pod_network_cidr,$service_cidr export http_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export https_proxy=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTPS_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT export HTTP_PROXY=http://REPLACE-WITH-YOUR-COMPANY-PROXY-HOST:PORT   Source the script to set up your environment variables:\n$ . ~/.bashrc   To implement command completion, add the following to the script:\n$ [ -f /usr/share/bash-completion/bash_completion ] \u0026amp;\u0026amp; . /usr/share/bash-completion/bash_completion $ source \u0026lt;(kubectl completion bash)   Run kubeadm init to create the master node:\n$ kubeadm init \\ --pod-network-cidr=$pod_network_cidr \\ --apiserver-advertise-address=$ip_addr \\ --ignore-preflight-errors=Swap \u0026gt; /tmp/kubeadm-init.out 2\u0026gt;\u0026amp;1   Log in to the terminal with YOUR_USERID:YOUR_GROUP. Then set up the ~/.bashrc similar to steps 1 to 3 with YOUR_USERID:YOUR_GROUP.\n Note that from now on we will be using YOUR_USERID:YOUR_GROUP to execute any kubectl commands and not root.\n   Set up YOUR_USERID:YOUR_GROUP to access the Kubernetes cluster:\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config   Verify that YOUR_USERID:YOUR_GROUP is set up to access the Kubernetes cluster using the kubectl command:\n$ kubectl get nodes  Note: At this step, the node is not in ready state as we have not yet installed the pod network add-on. After the next step, the node will show status as Ready.\n   Install a pod network add-on (flannel) so that your pods can communicate with each other.\n Note: If you are using a different CIDR block than 10.244.0.0/16, then download and update kube-flannel.yml with the correct CIDR address before deploying into the cluster:\n $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.12.0/Documentation/kube-flannel.yml   Verify that the master node is in Ready status:\n$ kubectl get nodes Sample output:\nNAME STATUS ROLES AGE VERSION mymasternode Ready master 8m26s v1.18.4 or:\n$ kubectl get pods -n kube-system Sample output:\nNAME READY STATUS RESTARTS AGE pod/coredns-86c58d9df4-58p9f 1/1 Running 0 3m59s pod/coredns-86c58d9df4-mzrr5 1/1 Running 0 3m59s pod/etcd-mymasternode 1/1 Running 0 3m4s pod/kube-apiserver-node 1/1 Running 0 3m21s pod/kube-controller-manager-mymasternode 1/1 Running 0 3m25s pod/kube-flannel-ds-amd64-6npx4 1/1 Running 0 49s pod/kube-proxy-4vsgm 1/1 Running 0 3m59s pod/kube-scheduler-mymasternode 1/1 Running 0 2m58s   To schedule pods on the master node, taint the node:\n$ kubectl taint nodes --all node-role.kubernetes.io/master-   Congratulations! Your Kubernetes cluster environment is ready to deploy your Oracle WebCenter Portal domain.\nFor additional references on Kubernetes cluster setup, check the cheat sheet.\n3. Get scripts and images 3.1 Set up the code repository to deploy Oracle WebCenter Portal Follow these steps to set up the source code repository required to deploy Oracle WebCenter Portal domain.\n3.2 Get required Docker images and add them to your local registry   Pull the operator image:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.3.0   Obtain the Oracle Database image from the Oracle Container Registry:\na. For first time users, to pull an image from the Oracle Container Registry, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On (SSO) authentication service. If you do not already have SSO credentials, you can create an Oracle Account using:\nhttps://profile.oracle.com/myprofile/account/create-account.jspx.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nTo obtain the image, log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com b. Find and then pull the Oracle Database image for 12.2.0.1:\n$ docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim c. Build Oracle WebCenter Portal 12.2.1.4 Image by following steps from this document.\n  4. Install the WebLogic Kubernetes operator 4.1 Prepare for the WebLogic Kubernetes operator.   Create a namespace operator-ns for the operator:\n$ kubectl create namespace operator-ns   Create a service account operator-sa for the operator in the operator’s namespace:\n$ kubectl create serviceaccount -n operator-ns operator-sa   4.2 Install the WebLogic Kubernetes operator Use Helm to install and start the operator from the directory you just cloned:\n $ cd ${WORKDIR} $ helm install weblogic-kubernetes-operator charts/weblogic-operator \\ --namespace operator-ns \\ --set image=oracle/weblogic-kubernetes-operator:3.3.0 \\ --set serviceAccount=operator-sa \\ --set \u0026quot;domainNamespaces={}\u0026quot; \\ --wait 4.3 Verify the WebLogic Kubernetes operator   Verify that the operator’s pod is running by listing the pods in the operator’s namespace. You should see one for the operator:\n$ kubectl get pods -n operator-ns   Verify that the operator is up and running by viewing the operator pod\u0026rsquo;s logs:\n$ kubectl logs -n operator-ns -c weblogic-operator deployments/weblogic-operator   The WebLogic Kubernetes operator v3.3.0 has been installed. Continue with the load balancer and Oracle WebCenter Portal domain setup.\n5. Install the Traefik (ingress-based) load balancer The WebLogic Kubernetes Operator supports three load balancers: Traefik, NGINX and Apache. Samples are provided in the documentation.\nThis Quick Start demonstrates how to install the Traefik ingress controller to provide load balancing for an Oracle WebCenter Portal domain.\n  Create a namespace for Traefik:\n$ kubectl create namespace traefik   Set up Helm for 3rd party services:\n$ helm repo add traefik https://containous.github.io/traefik-helm-chart   Install the Traefik operator in the traefik namespace with the provided sample values:\n$ cd ${WORKDIR} $ helm install traefik traefik/traefik \\ --namespace traefik \\ --values charts/traefik/values.yaml \\ --set \u0026quot;kubernetes.namespaces={traefik}\u0026quot; \\ --set \u0026quot;service.type=NodePort\u0026quot; \\ --wait   6. Create and configure an Oracle WebCenter Portal domain 6.1 Prepare for an Oracle WebCenter Portal domain   Create a namespace that can host Oracle WebCenter Portal domain:\n$ kubectl create namespace wcpns   Use Helm to configure the operator to manage Oracle WebCenter Portal domain in this namespace:\n$ cd ${WORKDIR} $ helm upgrade weblogic-kubernetes-operator charts/weblogic-operator \\ --reuse-values \\ --namespace operator-ns \\ --set \u0026quot;domainNamespaces={wcpns}\u0026quot; \\ --wait   Create Kubernetes secrets.\na. Create a Kubernetes secret for the domain in the same Kubernetes namespace as the domain. In this example, the username is weblogic, the password is welcome1, and the namespace is wcpns:\n$ cd ${WORKDIR}/create-weblogic-domain-credentials $ sh create-weblogic-credentials.sh -u weblogic -p welcome1 -n wcpns -d wcp-domain -s wcp-domain-domain-credentials b. Create a Kubernetes secret for the RCU in the same Kubernetes namespace as the domain:\n Schema user : WCP1 Schema password : Oradoc_db1 DB sys user password : Oradoc_db1 Domain name : wcp-domain Domain Namespace : wcpns Secret name : wcp-domain-rcu-credentials  $ cd ${WORKDIR}/create-rcu-credentials $ sh create-rcu-credentials.sh -u WCP1 -p Oradoc_db1 -a sys -q Oradoc_db1 -n wcpns -d wcp-domain -s wcp-domain-rcu-credentials   Create the Kubernetes persistence volume and persistence volume claim.\na. Create the Oracle WebCenter Portal domain home directory. Determine if a user already exists on your host system with uid:gid of 1000:\n$ sudo getent passwd 1000 If this command returns a username (which is the first field), you can skip the following useradd command. If not, create the oracle user with useradd:\n$ sudo useradd -u 1000 -g 1000 oracle Create the directory that will be used for the Oracle WebCenter Portal domain home:\n$ sudo mkdir /scratch/k8s_dir $ sudo chown -R 1000:1000 /scratch/k8s_dir b. Update create-pv-pvc-inputs.yaml with the following values:\n baseName: domain domainUID: wcp-domain namespace: wcpns weblogicDomainStoragePath: /scratch/k8s_dir  $ cd ${WORKDIR}/create-weblogic-domain-pv-pvc $ cp create-pv-pvc-inputs.yaml create-pv-pvc-inputs.yaml.orig $ sed -i -e \u0026quot;s:baseName\\: weblogic-sample:baseName\\: domain:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:domainUID\\::domainUID\\: wcp-domain:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:namespace\\: default:namespace\\: wcpns:g\u0026quot; create-pv-pvc-inputs.yaml $ sed -i -e \u0026quot;s:#weblogicDomainStoragePath\\: /scratch/k8s_dir:weblogicDomainStoragePath\\: /scratch/k8s_dir:g\u0026quot; create-pv-pvc-inputs.yaml c. Run the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output d. Create the PV and PVC using the configuration files created in the previous step:\n $ kubectl create -f output/pv-pvcs/wcp-domain-domain-pv.yaml $ kubectl create -f output/pv-pvcs/wcp-domain-domain-pvc.yaml   Install and configure the database for the Oracle WebCenter Portal domain.\nThis step is required only when a standalone database is not already set up and you want to use the database in a container.\nThe Oracle Database Docker images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1). For production, it is suggested to use a standalone database. This example provides steps to create the database in a container.\n a. Create a database in a container:\n$ cd ${WORKDIR}/create-oracle-db-service $ ./start-db-service.sh -i container-registry.oracle.com/database/enterprise:12.2.0.1-slim -p none Once the database is successfully created, you can use the database connection string oracle-db.default.svc.cluster.local:1521/devpdb.k8s as an rcuDatabaseURL parameter in the create-domain-inputs.yaml file.\nb. Create Oracle WebCenter Portal schemas.\nTo create the Oracle WebCenter Portal schemas, run the following commands:\n $ ./create-rcu-schema.sh \\ -s WCP1 \\ -t wcp \\ -d oracle-db.default.svc.cluster.local:1521/devpdb.k8s \\ -i oracle/wcportal:12.2.1.4\\ -n wcpns \\ -q Oradoc_db1 \\ -r welcome1   Now the environment is ready to start the Oracle WebCenter Portal domain creation.\n6.2 Create an Oracle WebCenter Portal domain   The sample scripts for Oracle WebCenter Portal domain deployment are available at create-wcp-domain. You must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain.\nUpdate create-domain-inputs.yaml with the following values for domain creation:\n rcuDatabaseURL: oracle-db.default.svc.cluster.local:1521/devpdb.k8s    Run the create-domain.sh script to create a domain:\n$ cd ${WORKDIR}/create-wcp-domain/domain-home-on-pv/ $ ./create-domain.sh -i create-domain-inputs.yaml -o output   Create a Kubernetes domain object:\nOnce the create-domain.sh is successful, it generates output/weblogic-domains/wcp-domain/domain.yaml, which you can use to create the Kubernetes resource domain to start the domain and servers:\n$ cd ${WORKDIR}/create-wcp-domain/domain-home-on-pv $ kubectl create -f output/weblogic-domains/wcp-domain/domain.yaml   Verify that the Kubernetes domain object named wcp-domain is created:\n$ kubectl get domain -n wcpns NAME AGE wcp-domain 3m18s   Once you create the domain, the introspect pod is created. This inspects the domain home and then starts the wcp-domain-adminserver pod. Once the wcp-domain-adminserver pod starts successfully, the Managed Server pods are started in parallel. Watch the wcpns namespace for the status of domain creation:\n$ kubectl get pods -n wcpns -w   Verify that the Oracle WebCenter Portal domain server pods and services are created and in Ready state:\n$ kubectl get all -n wcpns   6.3 Configure Traefik to access Oracle WebCenter Portal domain services   Configure Traefik to manage ingresses created in the Oracle WebCenter Portal domain namespace (wcpns):\n$ helm upgrade traefik traefik/traefik \\ --reuse-values \\ --namespace traefik \\ --set \u0026quot;kubernetes.namespaces={traefik,wcpns}\u0026quot; \\ --wait   Create an ingress for the domain in the domain namespace by using the sample Helm chart:\n$ cd ${WORKDIR} helm install wcp-traefik-ingress \\ charts/ingress-per-domain \\ --namespace wcpns \\ --values charts/ingress-per-domain/values.yaml \\ --set \u0026quot;traefik.hostname=$(hostname -f)\u0026quot;   Verify the created ingress per domain details:\n$ kubectl describe ingress wcp-domain-traefik -n wcpns   6.4 Verify that you can access the Oracle WebCenter Portal domain URL   Get the LOADBALANCER_HOSTNAME for your environment:\nexport LOADBALANCER_HOSTNAME=$(hostname -f)   Verify the following URLs are available for Oracle WebCenter Portal domain.\nCredentials:\nusername: weblogic password: welcome1\nhttp://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenter http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rsscrawl http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rest http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenterhelp   "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/",
	"title": "Monitor a domain and publish logs",
	"tags": [],
	"description": "Monitor Oracle WebCenter Portal and publishing logs to Elasticsearch.",
	"content": "  Monitor a WebCenter Portal domain  Monitor an WebCenter Portal instance using Prometheus and Grafana.\n Publish WebLogic Server logs into Elasticsearch  Publish WebLogic Server logs into Elasticsearch.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/manage-wcportal-domains/configure-load-balancer/nginx/",
	"title": "NGINX",
	"tags": [],
	"description": "Configure the ingress-based NGINX load balancer for an Oracle WebCenter Portal domain.",
	"content": "To load balance Oracle WebCenter Portal domain clusters, you can install the ingress-based NGINX load balancer and configure NGINX for non-SSL, SSL termination, and end-to-end SSL access of the application URL. Follow these steps to set up NGINX as a load balancer for an Oracle WebCenter Portal domain in a Kubernetes cluster:\nSee the official installation document for prerequisites.\n  Non-SSL and SSL termination\n Install the NGINX load balancer Configure NGINX to manage ingresses Verify non-SSL and SSL termination access    End-to-end SSL configuration\n Install the NGINX load balancer for End-to-end SSL Deploy tls to access the services Verify end-to-end SSL access    Non-SSL and SSL termination To get repository information, enter the following Helm commands:\n$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update Install the NGINX load balancer   Deploy the ingress-nginx controller by using Helm on the domain namespace:\n$ helm install nginx-ingress ingress-nginx/ingress-nginx -n wcpns \\ --set controller.service.type=NodePort \\ --set controller.admissionWebhooks.enabled=false    Click here to see the sample output.    NAME: nginx-ingress LAST DEPLOYED: Tue Jan 12 21:13:54 2021 NAMESPACE: wcpns STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=30305 export HTTPS_NODE_PORT=$(kubectl --namespace wcpns get services -o jsonpath=\u0026quot;{.spec.ports[1].nodePort}\u0026quot; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace wcpns get nodes -o jsonpath=\u0026quot;{.items[0].status.addresses[1].address}\u0026quot;) echo \u0026quot;Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP.\u0026quot; echo \u0026quot;Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS.\u0026quot; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Check the status of the deployed ingress controller:\n$ kubectl --namespace wcpns get services | grep ingress-nginx-controller Sample output:\nnginx-ingress-ingress-nginx-controller NodePort 10.101.123.106 \u0026lt;none\u0026gt; 80:30305/TCP,443:31856/TCP 2m12s   Configure NGINX to manage ingresses  Create an ingress for the domain in the domain namespace by using the sample Helm chart. Here path-based routing is used for ingress. Sample values for default configuration are shown in the file ${WORKDIR}/charts/ingress-per-domain/values.yaml. By default, type is TRAEFIK, tls is Non-SSL. You can override these values by passing values through the command line or edit them in the sample values.yaml file.   NOTE: This is not an exhaustive list of rules. You can enhance it based on the application URLs that need to be accessed externally.\n If needed, you can update the ingress YAML file to define more path rules (in section spec.rules.host.http.paths) based on the domain application URLs that need to be accessed. Update the template YAML file for the NGINX load balancer located at ${WORKDIR}/charts/ingress-per-domain/templates/nginx-ingress.yaml You can add new path rules like shown below .\n- path: /NewPathRule backend: serviceName: \u0026#39;Backend Service Name\u0026#39; servicePort: \u0026#39;Backend Service Port\u0026#39; $ cd ${WORKDIR} $ helm install wcp-domain-nginx charts/ingress-per-domain \\  --namespace wcpns \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=$(hostname -f)\u0026#34; \\  --set type=NGINX ``` Sample output: ```bash NAME: wcp-domain-nginx LAST DEPLOYED: Fri Jul 24 09:34:03 2020 NAMESPACE: wcpns STATUS: deployed REVISION: 1 TEST SUITE: None ``` 1. For secured access (SSL) to the Oracle WebCenter Portal application, create a certificate and generate a Kubernetes secret: ```bash $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=*\u0026#34; $ kubectl -n wcpns create secret tls wcp-domain-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt   Install ingress-per-domain using Helm for SSL configuration:\n$ cd ${WORKDIR} $ helm install wcp-domain-nginx charts/ingress-per-domain \\  --namespace wcpns \\  --values charts/ingress-per-domain/values.yaml \\  --set \u0026#34;nginx.hostname=$(hostname -f)\u0026#34; \\  --set type=NGINX --set sslType=SSL   For non-SSL access to the Oracle WebCenter Portal application, get the details of the services by the ingress:\n$ kubectl describe ingress wcp-domain-nginx -n wcpns    Click here to see the sample output of the services supported by the above deployed ingress.    Name: wcp-domain-nginx Namespace: wcpns Address: 10.101.123.106 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) Rules: Host Path Backends ---- ---- -------- * /webcenter wcp-domain-cluster-wcp-cluster:8888 (10.244.0.52:8888,10.244.0.53:8888) /console wcp-domain-adminserver:7001 (10.244.0.51:7001) /rsscrawl wcp-domain-cluster-wcp-cluster:8888 (10.244.0.53:8888) /rest wcp-domain-cluster-wcp-cluster:8888 (10.244.0.53:8888) /webcenterhelp wcp-domain-cluster-wcp-cluster:8888 (10.244.0.53:8888) /wsrp-tools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.53:8889) /portalTools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.53:8889) /em wcp-domain-adminserver:7001 (10.244.0.51:7001) Annotations: meta.helm.sh/release-name: wcp-domain-nginx meta.helm.sh/release-namespace: wcpns nginx.com/sticky-cookie-services: serviceName=wcp-domain-cluster-wcp-cluster srv_id expires=1h path=/; nginx.ingress.kubernetes.io/proxy-connect-timeout: 1800 nginx.ingress.kubernetes.io/proxy-read-timeout: 1800 nginx.ingress.kubernetes.io/proxy-send-timeout: 1800 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 48m (x2 over 48m) nginx-ingress-controller Scheduled for sync      For SSL access to the Oracle WebCenter Portal application, get the details of the services by the above deployed ingress:\n$ kubectl describe ingress wcp-domain-nginx -n wcpns    Click here to see the sample output of the services supported by the above deployed ingress.   Name: wcp-domain-nginx Namespace: wcpns Address: 10.106.220.140 Default backend: default-http-backend:80 (\u0026lt;error: endpoints \u0026quot;default-http-backend\u0026quot; not found\u0026gt;) TLS: wcp-domain-tls-cert terminates mydomain.com Rules: Host Path Backends ---- ---- -------- * /webcenter wcp-domain-cluster-wcp-cluster:8888 (10.244.0.43:8888,10.244.0.44:8888) /console wcp-domain-adminserver:7001 (10.244.0.42:7001) /rsscrawl wcp-domain-cluster-wcp-cluster:8888 (10.244.0.43:8888,10.244.0.44:8888) /webcenterhelp wcp-domain-cluster-wcp-cluster:8888 (10.244.0.43:8888,10.244.0.44:8888) /rest wcp-domain-cluster-wcp-cluster:8888 (10.244.0.43:8888,10.244.0.44:8888) /em wcp-domain-adminserver:7001 (10.244.0.42:7001) /wsrp-tools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.43:8889,10.244.0.44:8889) /portalTools wcp-domain-cluster-wcportlet-cluster:8889 (10.244.0.43:8889,10.244.0.44:8889) Annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: wcp-domain-nginx meta.helm.sh/release-namespace: wcpns nginx.ingress.kubernetes.io/affinity: cookie nginx.ingress.kubernetes.io/affinity-mode: persistent nginx.ingress.kubernetes.io/configuration-snippet: more_set_input_headers \u0026quot;X-Forwarded-Proto: https\u0026quot;; more_set_input_headers \u0026quot;WL-Proxy-SSL: true\u0026quot;; nginx.ingress.kubernetes.io/ingress.allow-http: false nginx.ingress.kubernetes.io/proxy-connect-timeout: 1800 nginx.ingress.kubernetes.io/proxy-read-timeout: 1800 nginx.ingress.kubernetes.io/proxy-send-timeout: 1800 nginx.ingress.kubernetes.io/session-cookie-expires: 172800 nginx.ingress.kubernetes.io/session-cookie-max-age: 172800 nginx.ingress.kubernetes.io/session-cookie-name: stickyid nginx.ingress.kubernetes.io/ssl-redirect: false Events: \u0026lt;none\u0026gt;      Verify non-SSL and SSL termination access Verify that the Oracle WebCenter Portal domain application URLs are accessible through the nginx NodePort LOADBALANCER-NODEPORT 30305:\nhttp://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/webcenter http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/rsscrawl http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/rest http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/webcenterhelp http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/wsrp-tools http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-NODEPORT}/portalTools Uninstall the ingress Uninstall and delete the ingress-nginx deployment:\n$ helm delete wcp-domain-nginx -n wcpns $ helm delete nginx-ingress -n wcpns End-to-end SSL configuration Install the NGINX load balancer for End-to-end SSL   For secured access (SSL) to the Oracle WebCenter Portal application, create a certificate and generate secrets:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=domain1.org\u0026#34; $ kubectl -n wcpns create secret tls wcp-domain-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt  Note: The value of CN is the host on which this ingress is to be deployed.\n   Deploy the ingress-nginx controller by using Helm on the domain namespace:\n$ helm install nginx-ingress -n wcpns \\  --set controller.extraArgs.default-ssl-certificate=wcpns/wcp-domain-tls-cert \\  --set controller.service.type=NodePort \\  --set controller.admissionWebhooks.enabled=false \\  --set controller.extraArgs.enable-ssl-passthrough=true \\  ingress-nginx/ingress-nginx    Click here to see the sample output.   NAME: nginx-ingress LAST DEPLOYED: Tue Sep 15 08:40:47 2020 NAMESPACE: wcpns STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=$(kubectl --namespace wcpns get services -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export HTTPS_NODE_PORT=$(kubectl --namespace wcpns get services -o jsonpath=\u0026#34;{.spec.ports[1].nodePort}\u0026#34; nginx-ingress-ingress-nginx-controller) export NODE_IP=$(kubectl --namespace wcpns get nodes -o jsonpath=\u0026#34;{.items[0].status.addresses[1].address}\u0026#34;) echo \u0026#34;Visit http://$NODE_IP:$HTTP_NODE_PORTto access your application via HTTP.\u0026#34; echo \u0026#34;Visit https://$NODE_IP:$HTTPS_NODE_PORTto access your application via HTTPS.\u0026#34; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls      Check the status of the deployed ingress controller:\n$ kubectl --namespace wcpns get services | grep ingress-nginx-controller Sample output:\nnginx-ingress-ingress-nginx-controller NodePort 10.96.177.215 \u0026lt;none\u0026gt; 80:32748/TCP,443:31940/TCP 23s   Deploy tls to access services   Deploy tls to securely access the services. Only one application can be configured with ssl-passthrough. A sample tls file for NGINX is shown below for the service wcp-domain-cluster-wcp-cluster and port 8889. All the applications running on port 8889 can be securely accessed through this ingress.\n  For each backend service, create different ingresses, as NGINX does not support multiple paths or rules with annotation ssl-passthrough. For example, for wcp-domain-adminserver and wcp-domain-cluster-wcp-cluster, different ingresses must be created.\n  As ssl-passthrough in NGINX works on the clusterIP of the backing service instead of individual endpoints, you must expose wcp-domain-cluster-wcp-cluster created by the operator with clusterIP.\nFor example:\na. Get the name of wcp-domain cluster service:\n$ kubectl get svc -n wcpns | grep wcp-domain-cluster-wcp-cluster Sample output:\nwcp-domain-cluster-wcp-cluster ClusterIP 10.102.128.124 \u0026lt;none\u0026gt; 8888/TCP,8889/TCP 62m   Deploy the secured ingress:\n$ cd ${WORKDIR}/charts/ingress-per-domain/tls $ kubectl create -f nginx-tls.yaml  Note: The default nginx-tls.yaml contains the backend for WebCenter Portal service with domainUID wcp-domain. You need to create similar tls configuration YAML files separately for each backend service.\n   Click here to check the content of the file nginx-tls.yaml    apiVersion: extensions/v1beta1 kind: Ingress metadata: name: wcpns-ingress namespace: wcpns annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: \u0026quot;true\u0026quot; spec: tls: - hosts: - domain1.org secretName: wcp-domain-tls-cert rules: - host: domain1.org http: paths: - path: backend: serviceName: wcp-domain-cluster-wcp-cluster servicePort: 8889     Note: Host is the server on which this ingress is deployed.\n   Check the services supported by the ingress:\n$ kubectl describe ingress wcpns-ingress -n wcpns   Verify end-to-end SSL access Verify that the Oracle WebCenter Portal domain application URLs are accessible through the LOADBALANCER-SSLPORT 30233:\nhttps://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenter https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/rsscrawl https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenterhelp https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/rest https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/wsrp-tools https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/portalTools Uninstall ingress-nginx tls $ cd ${WORKDIR}/charts/ingress-per-domain/tls $ kubectl delete -f nginx-tls.yaml $ helm delete nginx-ingress -n wcpns "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/publishing-logs/",
	"title": "Publish WebLogic Server logs into Elasticsearch",
	"tags": [],
	"description": "Publish WebLogic Server logs into Elasticsearch.",
	"content": "To publish WebLogic Server logs into Elasticsearch, you can configure your WebCenter Portal domain to use Fluentd, WebLogic Logging Exporter or Logstash.  Fluentd  Describes how to configure a WebCenter Portal domain to use Fluentd to send log information to Elasticsearch.\n WebLogic logging exporter  Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.\n Logstash  Describes how to configure a WebCenter Portal domain to use logstash and publish the WebLogic Server logs to Elasticsearch.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/adminguide/weblogic-monitoring-exporter-setup/",
	"title": "Monitor an Oracle WebCenter Content domain",
	"tags": [],
	"description": "Use the WebLogic Monitoring Exporter to monitor an Oracle WebCenter Content instance using Prometheus and Grafana.",
	"content": "You can monitor a WebCenter Content domain using Prometheus and Grafana by exporting the metrics from the domain instance using the WebLogic Monitoring Exporter. This sample shows you how to set up the WebLogic Monitoring Exporter to push the data to Prometheus.\nPrerequisites This document assumes that the Prometheus Operator is deployed on the Kubernetes cluster. If it is not already deployed, follow the steps below for deploying the Prometheus Operator.\nDeploy Prometheus and Grafana Refer to the compatibility matrix of Kube Prometheus and clone the release version of the kube-prometheus repository according to the Kubernetes version of your cluster.\nClone the kube-prometheus project $ git clone https://github.com/coreos/kube-prometheus.git Label the nodes Kube-Prometheus requires all the exporter nodes to be labelled with kubernetes.io/os=linux. If a node is not labelled, then you must label it using the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux Create Prometheus and Grafana resources Change to the kube-prometheus directory and execute the following commands to create the namespace and CRDs:\nNOTE: Wait for a minute for each command to process.\n$ cd kube-prometheus $ kubectl create -f manifests/setup $ until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \u0026#34;\u0026#34;; done $ kubectl create -f manifests/ Provide external access To provide external access for Grafana, Prometheus, and Alertmanager, execute the commands below:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; NOTE:\n 32100 is the external port for Grafana 32101 is the external port for Prometheus 32102 is the external port for Alertmanager   Set Up the WebLogic Monitoring Exporter Set up the WebLogic Monitoring Exporter that will collect WebLogic Server metrics and monitor your Oracle WebCenter Content domain.\nGenerate the WebLogic Monitoring Exporter Deployment Package Two packages are required as the listening ports are different for the Administration Server and Managed Servers. One binary required for the Admin Server (wls-exporter-as.war) and one for Managed Cluster (wls-exporter-ms.war). Set the required proxies and then run the script getX.X.X.sh to generate two binaries:\nDownload WebLogic Monitoring Exporter Download WebLogic Monitoring Exporter package from https://github.com/oracle/weblogic-monitoring-exporter/releases Download wls-exporter.war and getX.X.X.sh\nCreate configuration file for WebLogic Monitoring Exporter In this step we will create the configuration file for WebLogic Monitoring Exporter.The configuration will have the server port for serving the webapp, metrics to be scraped from the WebLogic server etc.\n  Click here to see sample content for config-admin `config-admin.yaml`.   metricsNameSnakeCase: true restPort: 7001 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name    In this step we will generate the deployemt package. We have to generate three separate packages with restPort as 7001 16200 and 16250 in config.yaml. The three packages are required as the listening ports for AdminServer, Oracle WebCenter Content server \u0026amp; Oracle WebCenter Inbound Refinery server.\nUse getX.X.X.sh script to update the configuration file into wls-exporter package. Below a sample usage\nSet the required proxies and then run the script getX.X.X.sh\n$ cd kubernetes/samples/scripts/create-wcc-domain/utils/weblogic-monitoring-exporter $ sh get1.2.0.sh config-admin.yaml Output:\n./get1.2.0.sh config-admin.yaml % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 642 100 642 0 0 1186 0 --:--:-- --:--:-- --:--:-- 1184 100 2033k 100 2033k 0 0 846k 0 0:00:02 0:00:02 --:--:-- 1527k created /tmp/ci-6gGPjopn3l /tmp/ci-6gGPjopn3l /\u0026lt;your_path\u0026gt;/prometheus/weblogic_monitor_exporter in temp dir adding: config.yml (deflated 63%) Generate the packages for Managed Servers/clusters with the different configuration file.\nDeploy the WebLogic Monitoring Exporter Follow these steps to deploy the package in the WebLogic Server instances:\n  In the Administration Server and Managed Servers, deploy the WebLogic Monitoring Exporter (wls-exporter.war) separately using the Oracle Enterprise Manager.\n  Select the servers to which the Exporter WAR should be deployed:\n  Set the application name. The application name must be different if it is deployed separately in the Administration Server and Managed Servers. Make sure the context-root for both the deployments is wls-exporter:\n  Click Install and start application.\n  Then deploy the WebLogic Monitoring Exporter application.\n  Activate the changes to start the application. If the application is started and the port is exposed, then you can access the WebLogic Monitoring Exporter console using this URL: http://\u0026lt;server:port\u0026gt;/wls-exporter.\n  Repeat same steps for ucm, ibr, ipm, capture and wccadf servers.\n  Configure Prometheus Operator Prometheus enables you to collect metrics from the WebLogic Monitoring Exporter. The Prometheus Operator identifies the targets using service discovery. To get the WebLogic Monitoring Exporter end point discovered as a target, you must create a service monitor pointing to the service.\nSee the following sample service monitor deployment YAML configuration file located at\nkubernetes/samples/scripts/create-wccontent-domains/utils/weblogic-monitoring-exporter/wls-exporter.yaml.\nServiceMonitor for wls-exporter:\n  Click here to see sample content for wls-exporter.yaml   apiVersion: v1 kind: Secret metadata: name: basic-auth namespace: monitoring data: password: d2VsY29tZTE= # welcome1 i.e.'WebLogic password' user: d2VibG9naWM= # weblogic i.e. 'WebLogic username' type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter-wccinfra namespace: monitoring labels: k8s-app: wls-exporter spec: namespaceSelector: matchNames: - wccns selector: matchLabels: weblogic.domainName: wccinfra endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics    The exporting of metrics from wls-exporter requires basicAuth so a Kubernetes Secret is created with the user name and password that are base64 encoded. This Secret will be used in the ServiceMonitor deployment.\nWhen generating the base64 encoded strings for the user name and password, observe if a new line character is appended in the encoded string. This line character causes an authentication failure. To avoid a new line string, use the following example:\n$ echo -n \u0026quot;welcome1\u0026quot; | base64 d2VsY29tZTE= In the deployment YAML configuration for wls-exporter shown above, weblogic.domainName: wccinfra is used as a label under spec.selector.matchLabels, so all the services will be selected for the service monitor. If you don\u0026rsquo;t use this label, you should create separate service monitors for each server - if the server name is used as matching labels in spec.selector.matchLabels. Doing so will require you to relabel the configuration because Prometheus, by default, ignores the labels provided in the wls-exporter.\nBy default, Prometheus does not store all the labels provided by the target. In the service monitor deployment YAML configuration, you must mention the relabeling configuration (spec.endpoints.relabelings) so that certain labels provided by weblogic-monitoring-exporter (required for the Grafana dashboard) are stored in Prometheus. Do not delete the following section from the configuration YAML file:\nrelabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) Add RoleBinding and Role for the WebLogic Domain Namespace The RoleBinding is required for Prometheus to access the endpoints provided by the WebLogic Monitoring Exporter. You need to add RoleBinding for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. Edit the kube-prometheus/manifests/prometheus-roleBindingSpecificNamespaces.yaml file in the Prometheus Operator deployment manifests and add the RoleBinding for the namespace (wccns) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: wccns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring Similarly, add the Role for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. Edit kube-prometheus/manifests/prometheus-roleSpecificNamespaces.yaml in the Prometheus Operator deployment manifests and add the Role for the namespace (wccns) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: wccns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch Then apply prometheus-roleBindingSpecificNamespaces.yaml and prometheus-roleSpecificNamespaces.yaml for the RoleBinding and Role to take effect in the cluster.\n$ kubectl apply -f kube-prometheus/manifests/prometheus-roleBindingSpecificNamespaces.yaml $ kubectl apply -f kube-prometheus/manifests/prometheus-roleSpecificNamespaces.yaml Deploy the Service Monitor To deploy the service monitor, use the above wls-exporter.yaml deployment YAML and run the following command:\n$ cd kubernetes/samples/scripts/create-wccontent-domains/utils/weblogic-monitoring-exporter/ $ kubectl create -f wls-exporter.yaml Enable Prometheus to Discover the Service After the deployment of the service monitor, Prometheus should be able to discover wls-exporter and export metrics.\nYou can access the Prometheus dashboard at http://mycompany.com:32101/.\nDeploy Grafana Dashboard To view the domain metrics, deploy the Grafana dashboard provided in the WebLogic Monitoring Exporter.\nYou can access the Grafana dashboard at http://mycompany.com:32100/.\n  Log in to Grafana dashboard with admin/admin.\n  Go to Settings, then select DataSources, and then Add Data Source.\nHTTP URL: Prometheus URL http://mycompany.com:32101/\nAuth: Enable Basic Auth\nBasic Auth Details: WebLogic credentials provided in step Configure Prometheus Operator\n  Download the weblogic_dashboard.json file from here.\n  Click Add and then Import. Paste the modified JSON in the Paste JSON block, and then load it.\nThis displays the WebLogic Server Dashboard.\n  "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/installguide/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "Prepare for creating the Oracle WebCenter Portal domain, This preparation includes but not limited to creating required secrets, persistent volume and volume claim, and database schema.",
	"content": "Set up the environment, including setting up a Kubernetes cluster and the Weblogic Kubernetes Operator.\n  Install Helm\n  Set Up your Kubernetes Cluster\n  Obtain the Oracle WebCenter Portal Docker Image\n  Pull Other Dependent Images\n  Set Up the Code Repository to Deploy Oracle WebCenter Portal Domain\n  Grant Roles and Clear Stale Resources\n  Install the WebLogic Kubernetes Operator\n  Prepare the Environment for the WebCenter Portal Domain\na. Create a namespace for an Oracle WebCenter Portal domain\nb. Create a Kubernetes secret with domain credentials\nc. Create a Kubernetes secret with the RCU credentials\nd. Create a persistent storage for an Oracle WebCenter Portal domain\ne. Configure access to your database\nf. Run the Repository Creation Utility to set up your database schemas\n  Install Helm The operator uses Helm to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, see here.\nSet Up your Kubernetes Cluster If you need help in setting up a Kubernetes environment, check our cheat sheet.\nAfter creating Kubernetes clusters, you can optionally:\n Create load balancers to direct traffic to backend domain Configure Kibana and Elasticsearch for your operator logs  Obtain the Oracle WebCenter Portal Docker Image The Oracle WebCenter Portal image with latest bundle patch and required interim patches can be obtained from My Oracle Support (MOS). This is the only image supported for production deployments. Follow the below steps to download the Oracle WebCenter Portal image from My Oracle Support.\n  Download patch 33807917 from My Oracle Support (MOS).\n  Unzip the downloaded patch zip file.\n  Load the image archive using the docker load command.\nFor example:\n$ docker load \u0026lt; wcportal-12.2.1.4-jdk8-ol7-220203.0823.tar Loaded image: oracle/wcportal:12.2.1.4-jdk8-ol7-220203.0823   If you want to build and use an Oracle WebCenter Portal Docker image with any additional bundle patch or interim patches that are not part of the image obtained from My Oracle Support, then follow these steps to create the image.\n Note: The default Oracle WebCenter Portal image name used for Oracle WebCenter Portal domain deployment is oracle/wcportal:12.2.1.4. The image obtained must be tagged as oracle/wcportal:12.2.1.4 using the docker tag command. If you want to use a different name for the image, make sure to update the new image tag name in the create-domain-inputs.yaml file and also in other instances where the oracle/wcportal:12.2.1.4 image name is used.\n Pull Other Dependent Images Dependent images include WebLogic Kubernetes Operator, database, and Traefik. Pull these images and add them to your local registry:\n Pull these docker images and re-tag them as shown:  To pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nThen, pull these docker images:\n$ docker login https://container-registry.oracle.com (enter your Oracle email Id and password) #This step is required once at every node to get access to the Oracle Container Registry. WebLogic Kubernetes Operator image:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.3.0 Copy all the built and pulled images to all the nodes in your cluster or add to a Docker registry that your cluster can access.  NOTE: If you\u0026rsquo;re not running Kubernetes on your development machine, you\u0026rsquo;ll need to make the Docker image available to a registry visible to your Kubernetes cluster. Upload your image to a machine running Docker and Kubernetes as follows:\n# on your build machine $ docker save Image_Name:Tag \u0026gt; Image_Name-Tag.tar $ scp Image_Name-Tag.tar YOUR_USER@YOUR_SERVER:/some/path/Image_Name-Tag.tar # on the Kubernetes server $ docker load \u0026lt; /some/path/Image_Name-Tag.tar Set Up the Code Repository to Deploy Oracle WebCenter Portal Domain Oracle WebCenter Portal domain deployment on Kubernetes leverages the WebLogic Kubernetes Operator infrastructure. For deploying the Oracle WebCenter Portal domain, you need to set up the deployment scripts as below:\n  Create a working directory to set up the source code.\n$ mkdir $HOME/wcp_22.2.3 $ cd $HOME/wcp_22.2.3   Download the Oracle WebCenter Portal Kubernetes deployment scripts from the Github repository. Required artifacts are available at OracleWeCenterPortal/kubernetes.\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ export WORKDIR=$HOME/wcp_22.2.3/fmw-kubernetes/OracleWebCenterPortal/kubernetes/   You can now use the deployment scripts from \u0026lt;$WORKDIR\u0026gt; to set up the WebCenter Portal domain as described later in this document.\nGrant Roles and Clear Stale Resources   To confirm if there is already a WebLogic custom resource definition, execute the following command:\n$ kubectl get crd NAME CREATED AT domains.weblogic.oracle 2020-03-14T12:10:21Z   Delete the WebLogic custom resource definition, if you find any, by executing the following command:\n$ kubectl delete crd domains.weblogic.oracle customresourcedefinition.apiextensions.k8s.io \u0026#34;domains.weblogic.oracle\u0026#34; deleted   Install the WebLogic Kubernetes Operator   Create a namespace for the WebLogic Kubernetes Operator:\n$ kubectl create namespace operator-ns namespace/operator-ns created NOTE: In this procedure, the namespace is called “operator-ns”. You can use any name.\nYou can use:\n domainUID/domainname as wcp-domain Domain namespace as wcpns Operator namespace as operator-ns traefik namespace as traefik    Create a service account for the WebLogic Kubernetes Operator in the operator\u0026rsquo;s namespace:\n$ kubectl create serviceaccount -n operator-ns operator-sa serviceaccount/operator-sa created   To be able to set up the log-stash and Elasticsearch after creating the domain, set the value of the field elkIntegrationEnabled to true in the file kubernetes/charts/weblogic-operator/values.yaml.\n  Use helm to install and start the WebLogic Kubernetes Operator from the downloaded repository:\n Helm install weblogic-operator\n $ cd ${WORKDIR} $ helm install weblogic-kubernetes-operator charts/weblogic-operator --namespace operator-ns --set serviceAccount=operator-sa --set \u0026#34;domainNamespaces={}\u0026#34; --wait NAME: weblogic-kubernetes-operator LAST DEPLOYED: Wed Jan 6 01:47:33 2021 NAMESPACE: operator-ns STATUS: deployed REVISION: 1 TEST SUITE: None ``\n  To verify that the operator\u0026rsquo;s pod is running, list the pods in the operator\u0026rsquo;s namespace. You should see one for the WebLogic Kubernetes Operator:\n$ kubectl get pods -n operator-ns NAME READY STATUS RESTARTS AGE weblogic-operator-67df5fddc5-tlc4b 2/2 Running 0 3m15s   Then, check by viewing the Operator pod\u0026rsquo;s log as shown in the following sample log snippet:\n$ kubectl logs -n operator-ns -c weblogic-operator deployments/weblogic-operator Launching Oracle WebLogic Server Kubernetes Operator... Importing keystore /operator/internal-identity/temp/weblogic-operator.jks to /operator/internal-identity/temp/weblogic-operator.p12... Entry for alias weblogic-operator-alias successfully imported. Import command completed: 1 entries successfully imported, 0 entries failed or cancelled Warning: The -srcstorepass option is specified multiple times. All except the last one will be ignored. MAC verified OK % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 4249 0 2394 100 1855 6884 5334 --:--:-- --:--:-- --:--:-- 6899 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 5558 0 3028 100 2530 22704 18970 --:--:-- --:--:-- --:--:-- 22766 OpenJDK 64-Bit Server VM warning: Option MaxRAMFraction was deprecated in version 10.0 and will likely be removed in a future release. VM settings: Max. Heap Size (Estimated): 14.08G Using VM: OpenJDK 64-Bit Server VM {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.438+0000\u0026#34;,\u0026#34;thread\u0026#34;:1,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.TuningParametersImpl\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;update\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593438,\u0026#34;message\u0026#34;:\u0026#34;Reloading tuning parameters from Operator\u0026#39;s config map\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.944+0000\u0026#34;,\u0026#34;thread\u0026#34;:1,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;main\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593944,\u0026#34;message\u0026#34;:\u0026#34;Oracle WebLogic Server Kubernetes Operator, version: 3.0.4, implementation: master.4d4fe0a, build time: 2019-11-15T21:19:56-0500\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.972+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593972,\u0026#34;message\u0026#34;:\u0026#34;Operator namespace is: operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.009+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594009,\u0026#34;message\u0026#34;:\u0026#34;Operator target namespaces are: operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.013+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594013,\u0026#34;message\u0026#34;:\u0026#34;Operator service account is: operator-sa\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.031+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;performK8sVersionCheck\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594031,\u0026#34;message\u0026#34;:\u0026#34;Verifying Kubernetes minimum version\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.286+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.ClientPool\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getApiClient\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594286,\u0026#34;message\u0026#34;:\u0026#34;The Kuberenetes Master URL is set to https://10.96.0.1:443\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.673+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;createAndValidateKubernetesVersion\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594673,\u0026#34;message\u0026#34;:\u0026#34;Kubernetes version is: v1.13.7\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.259+0000\u0026#34;,\u0026#34;thread\u0026#34;:12,\u0026#34;fiber\u0026#34;:\u0026#34;engine-operator-thread-2-fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.CrdHelper$CrdContext$CreateResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;onSuccess\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595259,\u0026#34;message\u0026#34;:\u0026#34;Create Custom Resource Definition: oracle.kubernetes.operator.calls.CallResponse@470b40c\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.356+0000\u0026#34;,\u0026#34;thread\u0026#34;:16,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1-child-2\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;performSecurityChecks\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595356,\u0026#34;message\u0026#34;:\u0026#34;Verifying that operator service account can access required operations on required resources in namespace operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.598+0000\u0026#34;,\u0026#34;thread\u0026#34;:18,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1-child-2\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.ConfigMapHelper$ScriptConfigMapContext$CreateResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;onSuccess\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595598,\u0026#34;message\u0026#34;:\u0026#34;Creating domain config map, operator-ns, for namespace: {1}.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.937+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;WARNING\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.utils.Certificates\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getCertificate\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595937,\u0026#34;message\u0026#34;:\u0026#34;Can\u0026#39;t read certificate at /operator/external-identity/externalOperatorCert\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\\njava.nio.file.NoSuchFileException: /operator/external-identity/externalOperatorCert\\n\\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\\n\\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\\n\\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\\n\\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:215)\\n\\tat java.base/java.nio.file.Files.newByteChannel(Files.java:370)\\n\\tat java.base/java.nio.file.Files.newByteChannel(Files.java:421)\\n\\tat java.base/java.nio.file.Files.readAllBytes(Files.java:3205)\\n\\tat oracle.kubernetes.operator.utils.Certificates.getCertificate(Certificates.java:48)\\n\\tat oracle.kubernetes.operator.utils.Certificates.getOperatorExternalCertificateData(Certificates.java:39)\\n\\tat oracle.kubernetes.operator.rest.RestConfigImpl.getOperatorExternalCertificateData(RestConfigImpl.java:52)\\n\\tat oracle.kubernetes.operator.rest.RestServer.isExternalSslConfigured(RestServer.java:383)\\n\\tat oracle.kubernetes.operator.rest.RestServer.start(RestServer.java:199)\\n\\tat oracle.kubernetes.operator.Main.startRestServer(Main.java:353)\\n\\tat oracle.kubernetes.operator.Main.completeBegin(Main.java:198)\\n\\tat oracle.kubernetes.operator.Main$NullCompletionCallback.onCompletion(Main.java:701)\\n\\tat oracle.kubernetes.operator.work.Fiber.completionCheck(Fiber.java:475)\\n\\tat oracle.kubernetes.operator.work.Fiber.run(Fiber.java:448)\\n\\tat oracle.kubernetes.operator.work.ThreadLocalContainerResolver.lambda$wrapExecutor$0(ThreadLocalContainerResolver.java:87)\\n\\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\\n\\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\\n\\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\tat java.base/java.lang.Thread.run(Thread.java:834)\\n\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.967+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595967,\u0026#34;message\u0026#34;:\u0026#34;Did not start the external ssl REST server because external ssl has not been configured.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:57.910+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168597910,\u0026#34;message\u0026#34;:\u0026#34;Started the internal ssl REST server on https://0.0.0.0:8082/operator\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:57.913+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;markReadyAndStartLivenessThread\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168597913,\u0026#34;message\u0026#34;:\u0026#34;Starting Operator Liveness Thread\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}   Prepare the Environment for the WebCenter Portal Domain Create a namespace for an Oracle WebCenter Portal domain Unless you want to use the default namespace, create a Kubernetes namespace that can host one or more domains:\n$ kubectl create namespace wcpns namespace/wcpns created To manage domain in this namespace, configure the operator using helm:\n Helm upgrade weblogic-operator\n $ helm upgrade --reuse-values --set \u0026#34;domainNamespaces={wcpns}\u0026#34; \\  --wait weblogic-kubernetes-operator charts/weblogic-operator --namespace operator-ns NAME: weblogic-kubernetes-operator LAST DEPLOYED: Wed Jan 6 01:52:58 2021 NAMESPACE: operator-ns STATUS: deployed REVISION: 2 Create a Kubernetes secret with domain credentials Using the create-weblogic-credentials script, create a Kubernetes secret that contains the user name and password for the domain in the same Kubernetes namespace as the domain:\n$ cd ${WORKDIR}/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p welcome1 -n wcpns -d wcp-domain -s wcp-domain-domain-credentials secret/wcp-domain-domain-credentials created secret/wcp-domain-domain-credentials labeled The secret wcp-domain-domain-credentials has been successfully created in the wcpns namespace. Where: * weblogic is the weblogic username * welcome1 is the weblogic password * wcp-domain is the domain name * wcpns is the domain namespace * wcp-domain-domain-credentials is the secret name Note: You can inspect the credentials as follows:  $ kubectl get secret wcp-domain-domain-credentials -o yaml -n wcpns Create a Kubernetes secret with the RCU credentials Create a Kubernetes secret for the Repository Configuration Utility (user name and password) using the create-rcu-credentials.sh script in the same Kubernetes namespace as the domain:\n$ cd ${WORKDIR}/create-rcu-credentials $ sh create-rcu-credentials.sh \\  -u WCP1 -p welcome1 -a sys -q Oradoc_db1 -n wcpns \\  -d wcp-domain -s wcp-domain-rcu-credentials secret/wcp-domain-rcu-credentials created secret/wcp-domain-rcu-credentials labeled The secret wcp-domain-rcu-credentials has been successfully created in the wcpns namespace. Where: * WCP1 is the schema user * welcome1 is the schema password * Oradoc_db1 is the database SYS users password * wcp-domain is the domain name * wcpns is the domain namespace * wcp-domain-rcu-credentials is the secret name Note: You can inspect the credentials as follows:  $ kubectl get secret wcp-domain-rcu-credentials -o yaml -n wcpns Create a persistent storage for an Oracle WebCenter Portal domain Create a Kubernetes PV and PVC (Persistent Volume and Persistent Volume Claim):\nIn the Kubernetes namespace you created, create the PV and PVC for the domain by running the create-pv-pvc.sh script. Follow the instructions for using the script to create a dedicated PV and PVC for the Oracle WebCenter Portal domain.\n  Review the configuration parameters for PV creation here. Based on your requirements, update the values in the create-pv-pvc-inputs.yaml file located at ${WORKDIR}/create-weblogic-domain-pv-pvc/. Sample configuration parameter values for an Oracle WebCenter Portal domain are:\n baseName: domain domainUID: wcp-domain namespace: wcpns weblogicDomainStorageType: HOST_PATH weblogicDomainStoragePath: /scratch/kubevolume    Ensure that the path for the weblogicDomainStoragePath property exists (create one if it doesn\u0026rsquo;t exist), that it has full access permissions, and that the folder is empty.\n  Run the create-pv-pvc.sh script:\n$ cd ${WORKDIR}/create-weblogic-domain-pv-pvc $ ./create-pv-pvc.sh -i create-pv-pvc-inputs.yaml -o output Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-pv-pvc-inputs-v1\u0026#34; export baseName=\u0026#34;domain\u0026#34; export domainUID=\u0026#34;wcp-domain\u0026#34; export namespace=\u0026#34;wcpns\u0026#34; export weblogicDomainStorageType=\u0026#34;HOST_PATH\u0026#34; export weblogicDomainStoragePath=\u0026#34;/scratch/kubevolume\u0026#34; export weblogicDomainStorageReclaimPolicy=\u0026#34;Retain\u0026#34; export weblogicDomainStorageSize=\u0026#34;10Gi\u0026#34; Generating output/pv-pvcs/wcp-domain-domain-pv.yaml Generating output/pv-pvcs/wcp-domain-domain-pvc.yaml The following files were generated: output/pv-pvcs/wcp-domain-domain-pv.yaml output/pv-pvcs/wcp-domain-domain-pvc.yaml   The create-pv-pvc.sh script creates a subdirectory pv-pvcs under the given /path/to/output-directory directory and creates two YAML configuration files for PV and PVC. Apply these two YAML files to create the PV and PVC Kubernetes resources using the kubectl create -f command:\n$ kubectl create -f output/pv-pvcs/wcp-domain-domain-pv.yaml $ kubectl create -f output/pv-pvcs/wcp-domain-domain-pvc.yaml   Configure access to your database Oracle WebCenter Portal domain requires a database which is configured with the necessary schemas. The Repository Creation Utility (RCU) allows you to create those schemas. You must set up the database before you create your domain.\nFor production deployments, you must set up and use a standalone (non-container) database running outside of Kubernetes.\nBefore creating a domain, you need to set up the necessary schemas in your database.\nRun the Repository Creation Utility to set up your database schemas To create the database schemas for Oracle WebCenter Portal domain, run the create-rcu-schema.sh script.\n$ cd ${WORKDIR}/create-rcu-schema $ sh create-rcu-schema.sh -h usage: create-rcu-schema.sh -s \u0026lt;schemaPrefix\u0026gt; -t \u0026lt;schemaType\u0026gt; -d \u0026lt;dburl\u0026gt; -i \u0026lt;image\u0026gt; -u \u0026lt;imagePullPolicy\u0026gt; -p \u0026lt;docker-store\u0026gt; -n \u0026lt;namespace\u0026gt; -q \u0026lt;sysPassword\u0026gt; -r \u0026lt;schemaPassword\u0026gt; -o \u0026lt;rcuOutputDir\u0026gt; [-h] -s RCU Schema Prefix (required) -t RCU Schema Type (optional) (supported values: wcp(default), wcpp) -d RCU Oracle Database URL (optional) (default: oracle-db.default.svc.cluster.local:1521/devpdb.k8s) -p FMW Infrastructure ImagePullSecret (optional) (default: none) -i Oracle WebCenter Portal Image (optional) (default: oracle/wcportal:12.2.1.4) -u FMW Infrastructure ImagePullPolicy (optional) (default: IfNotPresent) -n Namespace for RCU pod (optional) (default: default) -q password for database SYSDBA user. (optional) (default: Oradoc_db1) -r password for all schema owner (regular user). (optional) (default: Oradoc_db1) -o Output directory for the generated YAML file. (optional) (default: rcuoutput) -c Comma-separated variables in the format variablename=value. (optional). (default: none) -h Help $ ./create-rcu-schema.sh \\  -s WCP1 \\  -t wcp \\  -d oracle-db.default.svc.cluster.local:1521/devpdb.k8s \\  -i oracle/wcportal:12.2.1.4\\  -n wcpns \\  -q Oradoc_db1 \\  -r welcome1  Where RCU Schema type wcp generates webcenter portal related schema and wcpp generates webcenter portal plus portlet schemas.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/patch_and_upgrade/upgrade-operator-release/",
	"title": "Upgrade an WebLogic Kubernetes Operator release",
	"tags": [],
	"description": "Upgrade the WebLogic Kubernetes Operator release to a newer version.",
	"content": "These instructions apply to upgrading WebLogic Kubernetes Operators within the 3.x release family as additional versions are released.\nTo upgrade WebLogic Kubernetes Operator, use the helm upgrade command. Make sure that the weblogic-kubernetes-operator repository on your local machine is at the WebLogic Kubernetes Operator release to which you are upgrading. When upgrading the WebLogic Kubernetes Operator, the helm upgrade command requires that you supply a new Helm chart and image. For example:\n$ helm upgrade \\ --reuse-values \\ --set image=oracle/weblogic-kubernetes-operator:3.3.0 \\ --namespace weblogic-operator-namespace \\ --wait \\ weblogic-kubernetes-operator \\ kubernetes/charts/weblogic-operator "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/publishing-logs/weblogiclogging/",
	"title": "WebLogic logging exporter",
	"tags": [],
	"description": "Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.",
	"content": "The WebLogic Logging Exporter adds a log event handler to WebLogic Server which enables WebLogic Server to push the logs to Elasticsearch in Kubernetes by using the Elasticsearch REST API. For more details, see to the WebLogic Logging Exporter project.\nThis sample shows you how to publish WebLogic Server logs to Elasticsearch and view them in Kibana. For publishing operator logs, see this sample.\nPrerequisites This document assumes that you have already set up Elasticsearch and Kibana for logs collection. If you have not, please see this document.\n Download the WebLogic Logging Exporter binaries The pre-built binaries are available on the WebLogic Logging Exporter Releases page.\nDownload:\n weblogic-logging-exporter-1.0.0.jar from the release page snakeyaml-1.25.jar from Maven Central  These identifiers are used in the sample commands.\n wcpns: WebCenter Portal domain namespace wcp-domain: domainUID wcp-domain-adminserver: Administration Server pod name   Copy the JAR Files to the WebLogic Domain Home Copy the weblogic-logging-exporter-1.0.0.jar and snakeyaml-1.25.jar files to the domain home directory in the Administration Server pod.\n$ kubectl cp \u0026lt;file-to-copy\u0026gt; \u0026lt;namespace\u0026gt;/\u0026lt;Administration-Server-pod\u0026gt;:\u0026lt;domainhome\u0026gt; $ kubectl cp snakeyaml-1.25.jar wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains/wcp-domain/ $ kubectl cp weblogic-logging-exporter-1.0.0.jar wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains/wcp-domain/ Add a Startup Class to the Domain Configuration   In the WebLogic Server Administration Console, in the left navigation pane, expand Environment, and then select Startup and Shutdown Classes.\n  Add a new startup class. You may choose any descriptive name, however, the class name must be weblogic.logging.exporter.Startup.\n  Target the startup class to each server from which you want to export logs.\n  In your config.xml file located at, /u01/oracle/user_projects/domains/wcp-domain/config/config.xml the newly added startup-class must exist as shown below:\n$ kubectl exec -it wcp-domain-adminserver -n wcpns cat /u01/oracle/user_projects/domains/wcp-domain/config/config.xml \u0026lt;startup-class\u0026gt; \u0026lt;name\u0026gt;weblogic-logging-exporter\u0026lt;/name\u0026gt; \u0026lt;target\u0026gt;AdminServer,wcp_cluster\u0026lt;/target\u0026gt; \u0026lt;class-name\u0026gt;weblogic.logging.exporter.Startup\u0026lt;/class-name\u0026gt; \u0026lt;/startup-class\u0026gt;   Update the WebLogic Server CLASSPATH  Copy the setDomainEnv.sh file from the pod to a local folder:  $ kubectl cp wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains/wcp-domain/bin/setDomainEnv.sh $PWD/setDomainEnv.sh tar: Removing leading `/\u0026#39; from member names Ignore exception: tar: Removing leading '/' from member names\n Update the server class path in setDomainEnv.sh:  CLASSPATH=/u01/oracle/user_projects/domains/wcp-domain/weblogic-logging-exporter-1.0.0.jar:/u01/oracle/user_projects/domains/wcp-domain/snakeyaml-1.25.jar:${CLASSPATH} export CLASSPATH  Copy back the modified setDomainEnv.sh file to the pod:  $ kubectl cp setDomainEnv.sh wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains/wcp-domain/bin/setDomainEnv.sh Create a Configuration File for the WebLogic Logging Exporter   Specify the Elasticsearch server host and port number in the file: \u0026lt;$WORKDIR\u0026gt;/logging-services/weblogic-logging-exporter/WebLogicLoggingExporter.yaml\nExample:\nweblogicLoggingIndexName: wls publishHost: elasticsearch.default.svc.cluster.local publishPort: 9300 domainUID: wcp-domain weblogicLoggingExporterEnabled: true weblogicLoggingExporterSeverity: TRACE weblogicLoggingExporterBulkSize: 1   Copy the WebLogicLoggingExporter.yaml file to the domain home directory in the WebLogic Administration Server pod:\n  $ kubectl cp \u0026lt;$WORKDIR\u0026gt;/logging-services/weblogic-logging-exporter/WebLogicLoggingExporter.yaml wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains/wcp-domain/config/ Restart the Servers in the Domain To restart the servers, stop and then start them using the following commands:\nTo stop the servers:\n$ kubectl patch domain wcp-domain -n wcpns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; To start the servers:\n$ kubectl patch domain wcp-domain -n wcpns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;IF_NEEDED\u0026#34; }]\u0026#39; After all the servers are restarted, see their server logs to check that the weblogic-logging-exporter class is called, as shown below:\n======================= WebLogic Logging Exporter Startup class called Reading configuration from file name: /u01/oracle/user_projects/domains/wcp-domain/config/WebLogicLoggingExporter.yaml Config{weblogicLoggingIndexName='wls', publishHost='domain.host.com', publishPort=9200, weblogicLoggingExporterSeverity='Notice', weblogicLoggingExporterBulkSize='2', enabled=true, weblogicLoggingExporterFilters=FilterConfig{expression='NOT(MSGID = 'BEA-000449')', servers=[]}], domainUID='wcp-domain'} Create an Index Pattern in Kibana Create an index pattern wls* in Kibana by navigating to the dashboard through the Management option. After the servers are started, the log data is displayed on the Kibana dashboard:\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/adminguide/",
	"title": "Administration Guide",
	"tags": [],
	"description": "Describes how to use some of the common utility tools and configurations to administer Oracle WebCenter Content domains.",
	"content": "Administer Oracle WebCenter Content domains in Kubernetes.\n Set up a load balancer  Configure different load balancers for Oracle WebCenter Content domains.\n Monitor an Oracle WebCenter Content domain  Use the WebLogic Monitoring Exporter to monitor an Oracle WebCenter Content instance using Prometheus and Grafana.\n Elasticsearch integration for logs  Monitor an Oracle WebCenter Sites domain and publish the WebLogic Server logs to Elasticsearch.\n Publish logs to Elasticsearch  Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.\n Publish logs to Elasticsearch Using Fluentd  Configure a WebLogic domain to use Fluentd to send log information to Elasticsearch.\n Configure an additional mount or shared space to a domain for Imaging and Capture  Configure an additional mount or shared space to a domain, for WebCenter Imaging and WebCenter Capture\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/installguide/create-wccontent-domains/",
	"title": "Create Oracle WebCenter Content domain",
	"tags": [],
	"description": "Create Oracle WebCenter Content domain home on an existing PV or PVC and create the domain resource YAML file for deploying the generated Oracle WebCenter Content domain.",
	"content": "The WebCenter Content deployment scripts demonstrate the creation of Oracle WebCenter Content domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, complete the following steps:\n Review the Domain resource documentation. Review the requirements and limitations. Ensure that you have executed all the preliminary steps in Prepare your environment. Ensure that the database schemas were created and the WebLogic Kubernetes Operator are running.  Prepare to use the create domain script The sample scripts for Oracle WebCenter Content domain deployment are available at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain.\nYou must edit create-domain-inputs.yaml (or a copy of it) located under ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domian-home-on-pv to provide the details for your domain. Refer to the configuration parameters below to understand the information that you must provide in this file.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     sslEnabled Boolean indicating whether to enable SSL for each WebLogic Server instance. false   adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminServerSSLPort SSL port number of the Administration Server inside the Kubernetes cluster. 7002   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is ucm_cluster \u0026amp; ibr_cluster for the WebCenter Content domain. ucm_cluster   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebCenter Content domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/wccinfra   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. wccinfra   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   image WebCenter Content Docker image. WebLogic Kubernetes Operator requires Oracle WebCenter Content 12.2.1.4.0 Refer to Obtain the Oracle WebCenter Content Docker image for details on how to obtain or create the image. oracle/wccontent:12.2.1.4.0   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never. IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 3   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). If sslEnabled is set to true and the WebLogic demo certificate is used, add -Dweblogic.security.SSL.ignoreHostnameVerification=true to allow the Managed Servers to connect to the Administration Server while booting up. The WebLogic generated demo certificate in this environment typically contains a host name that is different from the runtime container\u0026rsquo;s host name. -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/wccinfra   managedServerNameBase Base string used to generate Managed Server names. ucm_server   managedServerPort Port number for each Managed Server. By default the managedServerPort is 16200 for the ucm_server \u0026amp; managedServerPort is 16250 for the ibr_server. 16200   managedServerSSLPort SSL port number for each Managed Server. By default the managedServerSSLPort is 16201 for the ucm_server \u0026amp; managedServerSSLPort is 16251 for the ibr_server. 16201   namespace Kubernetes namespace in which to create the domain. wccns   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. wccinfra-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the t3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. wccinfra-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example WCC1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. WCC1   rcuDatabaseURL The database URL. \u0026lt;YOUR DATABASE CONNECTION DETAILS\u0026gt;   rcuCredentialsSecret The Kubernetes secret containing the database credentials. wccinfra-rcu-credentials   ipmEnabled Boolean indicating whether to enable WebCenter Imaging application false   captureEnabled Boolean indicating whether to enable WebCenter Capture application false   adfuiEnabled Boolean indicating whether to enable WebCenter ADF UI application false    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\n Note: The properties ipmEnabled, captureEnabled, adfuiEnabled are set to false by default and should be updated to true if you need to enable the respective applications.\n The sample demonstrates how to create the Oracle WebCenter Content domain home and associated Kubernetes resources for that domain. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the create domain script Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o \u0026lt;path to output-directory\u0026gt; The script will perform the following steps:\n  Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is \u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script.\n  Create a Kubernetes job that will start up a utility Oracle WebCenter Content container and run offline WLST scripts to create the domain on the shared storage.\n  Run and wait for the job to finish.\n  Create a Kubernetes domain YAML file, domain.yaml, in the \u0026ldquo;output\u0026rdquo; directory that was created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command.\n  Run managed-server-wrapper script, which intrenally applies the domain YAML. This script also applies initial configurations for Managed Server containers and readies Managed Servers for future inter-container communications.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ ./start-managed-servers-wrapper.sh -o \u0026lt;path_to_output_directory\u0026gt; -p \u0026lt;load_balancer_port\u0026gt;   Create a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n  The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured cluster named ucm_cluster of size 3. A configured cluster named ibr_cluster of size 1. A configured cluster named ipm_cluster of size 3. A configured cluster named capture_cluster of size 3. A configured cluster named wccadf_cluster of size 3. Managed Servers, named ucm_cluster listening on port 16200. Managed Servers, named ibr_cluster listening on port 16250. Managed Servers, named ipm_cluster listening on port 16000. Managed Servers, named capture_cluster listening on port 16400. Managed Servers, named wccadf_cluster listening on port 16225. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;.  Verify the results The create domain script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nGenerated YAML files with the default inputs   Click here to see sample content of the generated `domain.yaml`.   $ cat output/weblogic-domains/wccinfra/domain.yaml # Copyright (c) 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v8\u0026quot; kind: Domain metadata: name: wccinfra namespace: wccns labels: weblogic.domainUID: wccinfra spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/wccinfra maxClusterConcurrentStartup: 1 # The domain home source type # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server Docker image that WebLogic Kubernetes Operator uses to start the domain image: \u0026quot;oracle/wccontent:12.2.1.4.0\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: wccinfra-domain-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # Whether to write HTTP access log file to log home httpAccessLogInLogHome: true # The in-pod location for domain log, server logs, server out, and Node Manager log files logHome: /u01/oracle/user_projects/domains/logs/wccinfra # An (optional) in-pod location for data storage of default and custom file stores. # If not specified or the value is either not set or empty (e.g. dataHome: \u0026quot;\u0026quot;) then the # data storage directories are determined from the WebLogic domain home configuration. dataHome: \u0026quot;\u0026quot; # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the WebLogic Kubernetes Operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx512m \u0026quot; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wccinfra-domain-pvc volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; adminService: channels: # The Admin Server's NodePort - channelName: default nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: ibr_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 1 serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; # The number of managed servers to start for unlisted clusters # replicas: 1 # Istio # configuration: # istio: # enabled: # readinessPort: - clusterName: ucm_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/service.sticky.cookie: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 3 serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; # The number of managed servers to start for unlisted clusters # replicas: 1 - clusterName: ipm_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/service.sticky.cookie: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 3 # The number of managed servers to start for unlisted clusters # replicas: 1 - clusterName: capture_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/service.sticky.cookie: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 3 # The number of managed servers to start for unlisted clusters # replicas: 1 - clusterName: wccadf_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/service.sticky.cookie: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/session-cookie-name: WCCSID serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 3 # The number of managed servers to start for unlisted clusters # replicas: 1    Verify the domain To confirm that the domain was created, enter the following command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\n  Click here to see a sample domain description.   $ kubectl describe domain wccinfra -n wccns Name: wccinfra Namespace: wccns Labels: weblogic.domainUID=wccinfra Annotations: API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2020-11-23T12:48:13Z Generation: 7 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:weblogic.domainUID: Manager: kubectl Operation: Update Time: 2020-11-23T13:50:28Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:servers: f:startTime: Manager: OpenAPI-Generator Operation: Update Time: 2020-12-03T10:20:52Z Resource Version: 18267402 Self Link: /apis/weblogic.oracle/v8/namespaces/wccns/domains/wccinfra UID: 1a866c30-9b29-4281-bd2b-df80914efdff Spec: Admin Server: Admin Service: Channels: Channel Name: default Node Port: 30701 Server Start State: RUNNING Clusters: Cluster Name: ibr_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start Policy: IF_NEEDED Server Start State: RUNNING Cluster Name: ucm_cluster Cluster Service: Annotations: traefik.ingress.kubernetes.io/affinity: true traefik.ingress.kubernetes.io/service.sticky.cookie: true traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID Replicas: 3 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start Policy: IF_NEEDED Server Start State: RUNNING Cluster Name: ipm_cluster Cluster Service: Annotations: traefik.ingress.kubernetes.io/affinity: true traefik.ingress.kubernetes.io/service.sticky.cookie: true traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID Replicas: 3 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: capture_cluster Cluster Service: Annotations: traefik.ingress.kubernetes.io/affinity: true traefik.ingress.kubernetes.io/service.sticky.cookie: true traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID Replicas: 3 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: wccadf_cluster Cluster Service: Annotations: traefik.ingress.kubernetes.io/affinity: true traefik.ingress.kubernetes.io/service.sticky.cookie: true traefik.ingress.kubernetes.io/session-cookie-name: WCCSID Replicas: 3 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/wccinfra Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: oracle/wccontent_ora_final_it:12.2.1.4.0 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/wccinfra Log Home Enabled: true Max Cluster Concurrent Startup: 1 Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx512m Volume Mounts: Mount Path: /u01/oracle/user_projects/domains Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: wccinfra-domain-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: wccinfra-domain-credentials Status: Clusters: Cluster Name: ibr_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Cluster Name: ucm_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 3 Replicas: 3 Replicas Goal: 3 Cluster Name: ipm_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 3 Replicas: 3 Replicas Goal: 3 Cluster Name: capture_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 3 Replicas: 3 Replicas Goal: 3 Cluster Name: wccadf_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 3 Replicas: 3 Replicas Goal: 3 Conditions: Last Transition Time: 2020-11-23T13:58:41.070Z Reason: ServersReady Status: True Type: Available Servers: Desired State: RUNNING Health: Activation Time: 2020-11-25T16:55:24.930Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: AdminServer State: RUNNING Cluster Name: ibr_cluster Desired State: RUNNING Health: Activation Time: 2020-11-30T12:23:27.603Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: ibr_server1 State: RUNNING Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server2 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server3 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server4 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server5 Cluster Name: ucm_cluster Desired State: RUNNING Health: Activation Time: 2020-12-02T14:10:37.992Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: ucm_server1 State: RUNNING Cluster Name: ucm_cluster Desired State: RUNNING Health: Activation Time: 2020-12-01T04:51:19.886Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: ucm_server2 State: RUNNING Cluster Name: ucm_cluster Desired State: SHUTDOWN Server Name: ucm_server3 Cluster Name: ucm_cluster Desired State: SHUTDOWN Server Name: ucm_server4 Cluster Name: ucm_cluster Desired State: SHUTDOWN Server Name: ucm_server5 Cluster Name: ipm_cluster Desired State: RUNNING Health: Activation Time: 2020-12-01T04:51:19.886Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: ipm_server1 State: RUNNING Cluster Name: ipm_cluster Desired State: SHUTDOWN Server Name: ipm_server2 Cluster Name: ipm_cluster Desired State: SHUTDOWN Server Name: ipm_server3 Cluster Name: ipm_cluster Desired State: SHUTDOWN Server Name: ipm_server4 Cluster Name: ipm_cluster Desired State: SHUTDOWN Server Name: ipm_server5 Cluster Name: capture_cluster Desired State: RUNNING Health: Activation Time: 2020-12-01T04:51:19.886Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: capture_server1 State: RUNNING Cluster Name: capture_cluster Desired State: SHUTDOWN Server Name: capture_server2 Cluster Name: capture_cluster Desired State: SHUTDOWN Server Name: capture_server3 Cluster Name: capture_cluster Desired State: SHUTDOWN Server Name: capture_server4 Cluster Name: capture_cluster Desired State: SHUTDOWN Server Name: capture_server5 Cluster Name: wccadf_cluster Desired State: RUNNING Health: Activation Time: 2020-12-01T04:51:19.886Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: MyNodeName Server Name: wccadf_server1 State: RUNNING Cluster Name: wccadf_cluster Desired State: SHUTDOWN Server Name: wccadf_server2 Cluster Name: wccadf_cluster Desired State: SHUTDOWN Server Name: wccadf_server3 Cluster Name: wccadf_cluster Desired State: SHUTDOWN Server Name: wccadf_server4 Cluster Name: wccadf_cluster Desired State: SHUTDOWN Server Name: wccadf_server5 Start Time: 2020-11-23T12:48:13.756Z Events: \u0026lt;none\u0026gt;    In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. WebLogic Kubernetes Operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Enter the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command. You can verify that an Administration Server and Managed Servers for ucm, ibr, ipm, capture and wccadf cluster are running.\n$ kubectl get pod -n wccns NAME READY STATUS RESTARTS AGE rcu 1/1 Running 0 78d wccinfra-adminserver 1/1 Running 0 9d wccinfra-create-fmw-infra-sample-domain-job-l8r9d 0/1 Completed 0 9d wccinfra-ibr-server1 1/1 Running 0 9d wccinfra-ucm-server1 1/1 Running 0 9d wccinfra-ucm-server2 1/1 Running 0 9d wccinfra-ucm-server3 1/1 Running 0 9d wccinfra-ipm-server1 1/1 Running 0 9d wccinfra-ipm-server2 1/1 Running 0 9d wccinfra-ipm-server3 1/1 Running 0 9d wccinfra-capture-server1 1/1 Running 0 9d wccinfra-capture-server2 1/1 Running 0 9d wccinfra-capture-server3 1/1 Running 0 9d wccinfra-wccadf-server1 1/1 Running 0 9d wccinfra-wccadf-server2 1/1 Running 0 9d wccinfra-wccadf-server3 1/1 Running 0 9d Verify the services Enter the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command.\n  Click here to see a sample list of services.   $ kubectl get services -n wccns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wccinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 9d wccinfra-adminserver-external NodePort 10.104.100.193 \u0026lt;none\u0026gt; 7001:30701/TCP 9d wccinfra-cluster-ibr-cluster ClusterIP 10.98.100.212 \u0026lt;none\u0026gt; 16250/TCP 114s wccinfra-cluster-ucm-cluster ClusterIP 10.108.47.178 \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-cluster-ipm-cluster ClusterIP 10.108.217.111 \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-cluster-capture-cluster ClusterIP 10.110.193.252 \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-cluster-wccadf-cluster ClusterIP 10.109.191.247 \u0026lt;none\u0026gt; 16225/TCP 9d wccinfra-ibr-server1 ClusterIP None \u0026lt;none\u0026gt; 16250/TCP 9d wccinfra-ibr-server2 ClusterIP 10.97.253.44 \u0026lt;none\u0026gt; 16250/TCP 9d wccinfra-ibr-server3 ClusterIP 10.110.183.48 \u0026lt;none\u0026gt; 16250/TCP 9d wccinfra-ibr-server4 ClusterIP 10.108.228.158 \u0026lt;none\u0026gt; 16250/TCP 9d wccinfra-ibr-server5 ClusterIP 10.101.29.140 \u0026lt;none\u0026gt; 16250/TCP 9d wccinfra-ucm-server1 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-ucm-server2 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-ucm-server3 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-ucm-server4 ClusterIP 10.109.25.242 \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-ucm-server5 ClusterIP 10.109.193.26 \u0026lt;none\u0026gt; 16200/TCP 9d wccinfra-ipm-server1 ClusterIP None \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-ipm-server2 ClusterIP None \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-ipm-server3 ClusterIP None \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-ipm-server4 ClusterIP 10.111.215.108 \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-ipm-server5 ClusterIP 10.109.220.10 \u0026lt;none\u0026gt; 16000/TCP 9d wccinfra-capture-server1 ClusterIP None \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-capture-server2 ClusterIP None \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-capture-server3 ClusterIP None \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-capture-server4 ClusterIP 10.109.72.216 \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-capture-server5 ClusterIP 10.102.90.234 \u0026lt;none\u0026gt; 16400/TCP 9d wccinfra-wccadf-server1 ClusterIP None \u0026lt;none\u0026gt; 16225/TCP 9d wccinfra-wccadf-server2 ClusterIP None \u0026lt;none\u0026gt; 16225/TCP 9d wccinfra-wccadf-server3 ClusterIP None \u0026lt;none\u0026gt; 16225/TCP 9d wccinfra-wccadf-server4 ClusterIP 10.99.91.229 \u0026lt;none\u0026gt; 16225/TCP 9d wccinfra-wccadf-server5 ClusterIP 10.105.114.38 \u0026lt;none\u0026gt; 16225/TCP 9d    Configure an additional mount or shared space to a domain for Imaging and Capture Optionally, if you want to configure an additional mount or shared space to a domain, for WebCenter Imaging and WebCenter Capture applications for file imports, refer to the Configure an Additional Mount or Shared-Space to a Domain for Imaging and Capture.\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/appendix/docker-k8s-hardening/",
	"title": "Security hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster\u0026rsquo;s security posture. Since Kubernetes is evolving rapidly refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    Security best practices for Oracle WebLogic Server Running in Docker and Kubernetes\n https://blogs.oracle.com/weblogicserver/security-best-practices-for-weblogic-server-running-in-docker-and-kubernetes    "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/adminguide/elasticsearch-integration/",
	"title": "Elasticsearch integration for logs",
	"tags": [],
	"description": "Monitor an Oracle WebCenter Sites domain and publish the WebLogic Server logs to Elasticsearch.",
	"content": "1. Integrate Elasticsearch to WebLogic Kubernetes Operator For reference information, see Elasticsearch integration for the WebLogic Kubernetes Operator.\nTo enable elasticsearch integration, you must edit file kubernetes/charts/weblogic-operator/values.yaml before deploying the WebLogic Kubernetes Operator.\n# elkIntegrationEnabled specifies whether or not ELK integration is enabled. elkIntegrationEnabled: true # logStashImage specifies the docker image containing logstash. # This parameter is ignored if 'elkIntegrationEnabled' is false. logStashImage: \u0026quot;logstash:6.6.0\u0026quot; # elasticSearchHost specifies the hostname of where Elasticsearch is running. # This parameter is ignored if 'elkIntegrationEnabled' is false. elasticSearchHost: \u0026quot;elasticsearch.default.svc.cluster.local\u0026quot; # elasticSearchPort specifies the port number of where Elasticsearch is running. # This parameter is ignored if 'elkIntegrationEnabled' is false. elasticSearchPort: 9200 After you\u0026rsquo;ve deployed WebLogic Kubernetes Operator and made the above changes, the weblogic-operator pod will have additional Logstash container. The Logstash container will push the weblogic-operator logs to the configured Elasticsearch server.\n2. Publish WebLogic Server and WebCenter Content Logs using Logstash Pod You can publish the WebLogic Server logs to Elasticsearch Server using Logstash pod. This Logstash pod must have access to the shared domain home. For the WebCenter Content wccinfra, you can use the persistent volume of the domain home in the Logstash pod. The steps to create the Logstash pod are as follows:\nGet the persistent volume details of the domain home of the WebLogic Server(s). The following command will list the persistent volume details in the namespace - \u0026ldquo;wccns\u0026rdquo;:\n$ kubectl get pv -n wccns NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE wccinfra-domain-pv 10Gi RWX Retain Bound wccns/wccinfra-domain-pvc wccinfra-domain-storage-class 33d Create the deployment yaml for Logstash pod. The mounted persistent volume of the domain home will provide access to the WebLogic server logs to Logstash pod. Given below is a sample Logstash deployment yaml.\napiVersion: apps/v1 kind: Deployment metadata: name: logstash-wls namespace: wccns spec: selector: matchLabels: app: \u0026quot;logstash-wls\u0026quot; template: # create pods using pod definition in this template metadata: labels: app: logstash-wls spec: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wccinfra-domain-pvc - name: shared-logs emptyDir: {} containers: - name: logstash image: logstash:6.6.0 command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logstash.conf\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - name: shared-logs mountPath: /shared-logs ports: - containerPort: 5044 name: logstash Sample Logstash configuration file is located at kubernetes/samples/scripts/create-wcc-domain/logstash/logstash.conf\n$ vi kubernetes/samples/scripts/create-wcc-domain/logstash/logstash.conf input { file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/AdminServer.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ucm_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ibr_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ipm_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/capture_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/wccadf_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/AdminServer.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ucm_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ibr_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/ipm_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/capture_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wccinfra/wccadf_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/AdminServer/logs/AdminServer-diagnostic.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/**/logs/ucm_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/**/logs/ibr_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/**/logs/ipm_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/**/logs/capture_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wccinfra/servers/**/logs/wccadf_server*.log\u0026quot; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ] } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;elasticsearch.default.svc.cluster.local:9200\u0026quot;] } } Here ** means that all ucm_server.log and ibr_server.log from any servers under wccinfra will be pushed to Logstash.\n$ kubectl cp kubernetes/samples/scripts/create-wcc-domain/logstash/logstash.conf wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/logstash.conf Deploy Logstash pod After you have created the Logstash deployment yaml and Logstash configuration file, deploy Logstash using following command:\n$ kubectl create -f kubernetes/samples/scripts/create-wcc-domain/logstash/logstash.yaml 3. Test the deployment of Elasticsearch and Kibana The WebLogic Kubernetes Operator also provides a sample deployment of Elasticsearch and Kibana for testing purpose. You can deploy Elasticsearch and Kibana on the Kubernetes cluster as shown below:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/ $ kubectl create -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml Get the Kibana dashboard port information as shown below: Wait for pods to start:\n-bash-4.2$ kubectl get pods -w NAME READY STATUS RESTARTS AGE elasticsearch-8bdb7cf54-mjs6s 1/1 Running 0 4m3s kibana-dbf8964b6-n8rcj 1/1 Running 0 4m3s -bash-4.2$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.105.205.157 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 10d kibana NodePort 10.98.104.41 \u0026lt;none\u0026gt; 5601:30412/TCP 10d kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 42d You can access the Kibana dashboard at http://\u0026lt;your_hostname\u0026gt;:30412/. In our example, the node port would be 30412.\nCreate an Index Pattern in Kibana Create an index pattern logstash-* in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard.\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/appendix/docker-k8s-hardening/",
	"title": "Security hardening",
	"tags": [],
	"description": "Review resources for the Docker and Kubernetes cluster hardening.",
	"content": "Securing a Kubernetes cluster involves hardening on multiple fronts - securing the API servers, etcd, nodes, container images, container run-time, and the cluster network. Apply principles of defense in depth, principle of least privilege, and minimize the attack surface. Use security tools such as Kube-Bench to verify the cluster\u0026rsquo;s security posture. Since Kubernetes is evolving rapidly refer to Kubernetes Security Overview for the latest information on securing a Kubernetes cluster. Also ensure the deployed Docker containers follow the Docker Security guidance.\nThis section provides references on how to securely configure Docker and Kubernetes.\nReferences   Docker hardening\n https://docs.docker.com/engine/security/security/ https://blog.aquasec.com/docker-security-best-practices    Kubernetes hardening\n https://kubernetes.io/docs/concepts/security/overview/ https://kubernetes.io/docs/concepts/security/pod-security-standards/ https://blogs.oracle.com/developers/5-best-practices-for-kubernetes-security    Security best practices for Oracle WebLogic Server Running in Docker and Kubernetes\n https://blogs.oracle.com/weblogicserver/security-best-practices-for-weblogic-server-running-in-docker-and-kubernetes    "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/installguide/create-wcp-domain/",
	"title": "Create WebCenter Portal domain",
	"tags": [],
	"description": "Create an Oracle WebCenter Portal domain home on an existing PV or PVC, and create the domain resource YAML file for deploying the generated Oracle WebCenter Portal domain.",
	"content": "Contents  Introduction Prerequisites Prepare the WebCenter Portal Domain Creation Input File Create the WebCenter Portal Domain Initialize the WebCenter Portal Domain Verify the WebCenter Portal Domain Managing WebCenter Portal  Introduction You can use the sample scripts to create a WebCenter Portal domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC).The scripts also generate the domain YAML file, which helps you start the Kubernetes artifacts of the corresponding domain.\nPrerequisites  Ensure that you have completed all of the steps under prepare-your-environment. Ensure that the database and the WebLogic Kubernetes operator is up.  Prepare the WebCenter Portal Domain Creation Input File If required, you can customize the parameters used for creating a domain in the create-domain-inputs.yaml file.\nPlease note that the sample scripts for the WebCenter Portal domain deployment are available from the previously downloaded repository at ${WORKDIR}/create-wcp-domain/domain-home-on-pv/.\nMake a copy of the create-domain-inputs.yaml file before updating the default values.\nThe default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured cluster named wcp-cluster of size 5. Managed Server, named wcpserver, listening on port 8888. If configurePortletServer is set to true . It configures a cluster named wcportlet-cluster of size 5 and Managed Server, named wcportletserver, listening on port 8889. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;.  Configuration parameters The following parameters can be provided in the inputs file:\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   sslEnabled SSL mode enabling flag false   configurePortletServer Configure portlet server cluster false   adminServerSSLPort SSL Port number for the Administration Server inside the Kubernetes cluster. 7002   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is wcp-cluster for the WebCenter Portal domain. wcp-cluster   portletClusterName Name of the Portlet cluster instance to generate for the domain. By default the cluster name is wcportlet-cluster for the Portlet. wcportlet-cluster   configuredManagedServerCount Number of Managed Server instances for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files that you need to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script uses the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod that creates a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script that creates a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-in scripts, you must use this property to set the name of the script to that which you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebCenter Portal domain. This field cannot be modified. /u01/oracle/user_projects/domains/wcp-domain   domainPVMountPath Mount path of the domain persistent volume. This field cannot be modified. /u01/oracle/user_projects/domains   domainUID Unique ID that identifies this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. wcp-domain   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   image WebCenter Portal Docker image. The WebLogic Kubernetes Operator requires WebCenter Portal release 12.2.1.4. Refer to WebCenter Portal Docker Image for details on how to obtain or create the image. oracle/wcportal:12.2.1.4   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret is validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include server.out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Server to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can include references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. This field cannot be modified. /u01/oracle/user_projects/logs/wcp-domain   managedServerNameBase Base string used to generate Managed Server names. wcpserver   portletServerNameBase Base string used to generate Portlet Server names. wcportletserver   managedServerPort Port number for each Managed Server. By default the managedServerPort is 8888 for the wcpserver and managedServerPort is 8889 for the wcportletserver. 8888   managedServerSSLPort SSL port number for each Managed Server. By default the managedServerPort is 8788 for the wcpserver and managedServerPort is 8789 for the wcportletserver. 8788   portletServerPort Port number for each Portlet Server. By default the portletServerPort is 8889 for the wcportletserver. 8888   portletServerSSLPort SSL port number for each Portlet Server. By default the portletServerSSLPort is 8789 for the wcportletserver. 8789   namespace Kubernetes namespace in which to create the domain. wcpns   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. wcp-domain-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances are to be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. wcp-domain-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed and minimum amount of compute resources required for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified. Refer to WebCenter Portal Cluster Sizing Recommendations for more details.   rcuSchemaPrefix The schema prefix to use in the database, for example WCP1. You may wish to make this the same as the domainUID in order to simplify matching domain to their RCU schemas. WCP1   rcuDatabaseURL The database URL. dbhostname:dbport/servicename   rcuCredentialsSecret The Kubernetes secret containing the database credentials. wcp-domain-rcu-credentials   loadBalancerHostName Host name for the final url accessible outside K8S environment. abc.def.com   loadBalancerPortNumber Port for the final url accessible outside K8S environment. 30305   loadBalancerProtocol Protocol for the final url accessible outside K8S environment. http   loadBalancerType Loadbalancer name. Example: Traefik or \u0026quot;\u0026rdquo; traefik   unicastPort Starting range of unicast port that application will use. 50000    You can form the names of the Kubernetes resources in the generated YAML files with the value of these properties specified in the create-domain-inputs.yaml file: adminServerName, clusterName and managedServerNameBase. Characters that are invalid in a Kubernetes service name are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;) .\nThe sample demonstrates how to create a WebCenter Portal domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides users with the capability to supply their own scripts to create the domain home for other use cases. You can modify the generated domain YAML file to include more use cases.\nCreate the WebCenter Portal Domain The syntax of the create-domain.sh script is as follows:\n $ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o /\u0026lt;path to output-directory\u0026gt; The script performs the following functions:\n Creates a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;.If the directory already exists, remove its content before using this script. Creates a Kubernetes job to start the WebCenter Portal Container utility and run offline WLST scripts that create the domain on the shared storage. Runs and waits for the job to finish. Creates a Kubernetes domain YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:  $ kubectl apply -f ../\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml  Creates a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.    Run the create-domain.sh sample script, pointing it at the create-domain-inputs.yaml inputs file and an output directory like below:\n$ cd ${WORKDIR}/create-wcp-domain/ $ sh create-domain.sh -i create-domain-inputs.yaml -o output Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-inputs-v1\u0026#34; export sslEnabled=\u0026#34;false\u0026#34; export adminPort=\u0026#34;7001\u0026#34; export adminServerSSLPort=\u0026#34;7002\u0026#34; export adminServerName=\u0026#34;AdminServer\u0026#34; export domainUID=\u0026#34;wcp-domain\u0026#34; export domainHome=\u0026#34;/u01/oracle/user_projects/domains/$domainUID\u0026#34; export serverStartPolicy=\u0026#34;IF_NEEDED\u0026#34; export clusterName=\u0026#34;wcp-cluster\u0026#34; export configuredManagedServerCount=\u0026#34;5\u0026#34; export initialManagedServerReplicas=\u0026#34;2\u0026#34; export managedServerNameBase=\u0026#34;wcpserver\u0026#34; export managedServerPort=\u0026#34;8888\u0026#34; export managedServerSSLPort=\u0026#34;8788\u0026#34; export portletServerPort=\u0026#34;8889\u0026#34; export portletServerSSLPort=\u0026#34;8789\u0026#34; export image=\u0026#34;oracle/wcportal:12.2.1.4\u0026#34; export imagePullPolicy=\u0026#34;IfNotPresent\u0026#34; export productionModeEnabled=\u0026#34;true\u0026#34; export weblogicCredentialsSecretName=\u0026#34;wcp-domain-domain-credentials\u0026#34; export includeServerOutInPodLog=\u0026#34;true\u0026#34; export logHome=\u0026#34;/u01/oracle/user_projects/domains/logs/$domainUID\u0026#34; export httpAccessLogInLogHome=\u0026#34;true\u0026#34; export t3ChannelPort=\u0026#34;30012\u0026#34; export exposeAdminT3Channel=\u0026#34;false\u0026#34; export adminNodePort=\u0026#34;30701\u0026#34; export exposeAdminNodePort=\u0026#34;false\u0026#34; export namespace=\u0026#34;wcpns\u0026#34; javaOptions=-Dweblogic.StdoutDebugEnabled=false export persistentVolumeClaimName=\u0026#34;wcp-domain-domain-pvc\u0026#34; export domainPVMountPath=\u0026#34;/u01/oracle/user_projects/domains\u0026#34; export createDomainScriptsMountPath=\u0026#34;/u01/weblogic\u0026#34; export createDomainScriptName=\u0026#34;create-domain-job.sh\u0026#34; export createDomainFilesDir=\u0026#34;wlst\u0026#34; export rcuSchemaPrefix=\u0026#34;WCP1\u0026#34; export rcuDatabaseURL=\u0026#34;oracle-db.wcpns.svc.cluster.local:1521/devpdb.k8s\u0026#34; export rcuCredentialsSecret=\u0026#34;wcp-domain-rcu-credentials\u0026#34; export loadBalancerHostName=\u0026#34;abc.def.com\u0026#34; export loadBalancerPortNumber=\u0026#34;30305\u0026#34; export loadBalancerProtocol=\u0026#34;http\u0026#34; export loadBalancerType=\u0026#34;traefik\u0026#34; export unicastPort=\u0026#34;50000\u0026#34; Generating output/weblogic-domains/wcp-domain/create-domain-job.yaml Generating output/weblogic-domains/wcp-domain/delete-domain-job.yaml Generating output/weblogic-domains/wcp-domain/domain.yaml Checking to see if the secret wcp-domain-domain-credentials exists in namespace wcpns configmap/wcp-domain-create-wcp-infra-sample-domain-job-cm created Checking the configmap wcp-domain-create-wcp-infra-sample-domain-job-cm was created configmap/wcp-domain-create-wcp-infra-sample-domain-job-cm labeled Checking if object type job with name wcp-domain-create-wcp-infra-sample-domain-job exists Deleting wcp-domain-create-wcp-infra-sample-domain-job using output/weblogic-domains/wcp-domain/create-domain-job.yaml job.batch \u0026#34;wcp-domain-create-wcp-infra-sample-domain-job\u0026#34; deleted $loadBalancerType is NOT empty Creating the domain by creating the job output/weblogic-domains/wcp-domain/create-domain-job.yaml job.batch/wcp-domain-create-wcp-infra-sample-domain-job created Waiting for the job to complete... status on iteration 1 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 2 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 3 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 4 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 5 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 6 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Running status on iteration 7 of 20 pod wcp-domain-create-wcp-infra-sample-domain-job-b5l6c status is Completed Domain wcp-domain was created and will be started by the WebLogic Kubernetes Operator The following files were generated: output/weblogic-domains/wcp-domain/create-domain-inputs.yaml output/weblogic-domains/wcp-domain/create-domain-job.yaml output/weblogic-domains/wcp-domain/domain.yaml Completed   To monitor the above domain creation logs:\n$ kubectl get pods -n wcpns |grep wcp-domain-create wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 1/1 Running 0 6s $ kubectl get pods -n wcpns | grep wcp-domain-create | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl -n wcpns logs -f SAMPLE OUTPUT:\nThe domain will be created using the script /u01/weblogic/create-domain-script.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands ================================================================= WebCenter Portal Weblogic Operator Domain Creation Script 12.2.1.4.0 ================================================================= Creating Base Domain... Creating Admin Server... Creating cluster... managed server name is wcpserver1 managed server name is wcpserver2 managed server name is wcpserver3 managed server name is wcpserver4 managed server name is wcpserver5 ['wcpserver1', 'wcpserver2', 'wcpserver3', 'wcpserver4', 'wcpserver5'] Creating porlet cluster... managed server name is wcportletserver1 managed server name is wcportletserver2 managed server name is wcportletserver3 ['wcportletserver1', 'wcportletserver2', 'wcportletserver3', 'wcportletserver4', 'wcportletserver5'] Managed servers created... Creating Node Manager... Will create Base domain at /u01/oracle/user_projects/domains/wcp-domain Writing base domain... Base domain created at /u01/oracle/user_projects/domains/wcp-domain Extending Domain... Extending domain at /u01/oracle/user_projects/domains/wcp-domain Database oracle-db.wcpns.svc.cluster.local:1521/devpdb.k8s ExposeAdminT3Channel false with 100.111.157.155:30012 Applying JRF templates... Applying WCPortal templates... Extension Templates added... WC_Portal Managed server deleted... Configuring the Service Table DataSource... fmwDatabase jdbc:oracle:thin:@oracle-db.wcpns.svc.cluster.local:1521/devpdb.k8s Getting Database Defaults... Targeting Server Groups... Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcpserver1 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcpserver2 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcpserver3 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcpserver4 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcpserver5 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcportletserver1 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcportletserver2 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcportletserver3 Targeting Cluster ... Set CoherenceClusterSystemResource to defaultCoherenceCluster for cluster:wcp-cluster Set WLS clusters as target of defaultCoherenceCluster:wcp-cluster Set CoherenceClusterSystemResource to defaultCoherenceCluster for cluster:wcportlet-cluster Set WLS clusters as target of defaultCoherenceCluster:wcportlet-cluster Preparing to update domain... Jan 12, 2021 10:30:09 AM oracle.security.jps.az.internal.runtime.policy.AbstractPolicyImpl initializeReadStore INFO: Property for read store in parallel: oracle.security.jps.az.runtime.readstore.threads = null Domain updated successfully Domain Creation is done... Successfully Completed   Initialize the WebCenter Portal Domain To start the domain, apply the above domain.yaml:\n$ kubectl apply -f output/weblogic-domains/wcp-domain/domain.yaml domain.weblogic.oracle/wcp-domain created Verify the WebCenter Portal Domain Verify that the domain and servers pods and services are created and in the READY state:\nSample run below:\n-bash-4.2$ kubectl get pods -n wcpns -w NAME READY STATUS RESTARTS\tAGE wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 15m wcp-domain-adminserver 1/1 Running 0 8m9s wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 3h6m wcp-domain-wcp-server1 0/1 Running 0 6m5s wcp-domain-wcp-server2 0/1 Running 0 6m4s wcp-domain-wcp-server2 1/1 Running 0 6m18s wcp-domain-wcp-server1 1/1 Running 0 6m54s -bash-4.2$ kubectl get all -n wcpns NAME READY STATUS RESTARTS AGE pod/wcp-domain-adminserver 1/1 Running 0 13m pod/wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 3h12m pod/wcp-domain-wcp-server1 1/1 Running 0 11m pod/wcp-domain-wcp-server2 1/1 Running 0 11m pod/wcp-domain-wcportletserver1 1/1 Running 1 21h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/wcp-domain-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 13m service/wcp-domain-cluster-wcp-cluster ClusterIP 10.98.145.173 \u0026lt;none\u0026gt; 8888/TCP 11m service/wcp-domain-wcp-server1 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP 11m service/wcp-domain-wcp-server2 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP 11m service/wcp-domain-cluster-wcportlet-cluster ClusterIP 10.98.145.173 \u0026lt;none\u0026gt; 8889/TCP 11m service/wcp-domain-wcportletserver1 ClusterIP None \u0026lt;none\u0026gt; 8889/TCP 11m NAME COMPLETIONS DURATION AGE job.batch/wcp-domain-create-fmw-infra-sample-domain-job 1/1 16m 3h12m To see the Admin and Managed Servers logs, you can check the pod logs:\n$ kubectl logs -f wcp-domain-adminserver -n wcpns $ kubectl logs -f wcp-domain-wcp-server1 -n wcpns Verify the Pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n-bash-4.2$ kubectl get pods -n wcpns NAME READY STATUS RESTARTS AGE rcu 1/1 Running 1 14d wcp-domain-adminserver 1/1 Running 0 16m wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 3h14m wcp-domain-wcp-server1 1/1 Running 0 14m wcp-domain-wcp-server2 1/1 Running 0 14m wcp-domain-wcportletserver1 1/1 Running 1 14m Verify the Services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n-bash-4.2$ kubectl get services -n wcpns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wcp-domain-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 17m wcp-domain-cluster-wcp-cluster ClusterIP 10.98.145.173 \u0026lt;none\u0026gt; 8888/TCP 14m wcp-domain-wcp-server1 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP 14m wcp-domain-wcp-server2 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP 14m wcp-domain-cluster-wcportlet-cluster ClusterIP 10.98.145.173 \u0026lt;none\u0026gt; 8889/TCP 14m wcp-domain-wcportletserver1 ClusterIP None \u0026lt;none\u0026gt; 8889/TCP 14m Managing WebCenter Portal To stop Managed Servers:\n$ kubectl patch domain wcp-domain -n wcpns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/clusters/0/replicas\u0026#34;, \u0026#34;value\u0026#34;: 0 }]\u0026#39; To start all configured Managed Servers:\n$ kubectl patch domain wcp-domain -n wcpns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/clusters/0/replicas\u0026#34;, \u0026#34;value\u0026#34;: 3 }]\u0026#39; -bash-4.2$ kubectl get pods -n wcpns -w NAME READY STATUS RESTARTS\tAGE wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 15m wcp-domain-adminserver 1/1 Running 0 8m9s wcp-domain-create-fmw-infra-sample-domain-job-8jr6k 0/1 Completed 0 3h6m wcp-domain-wcp-server1 0/1 Running 0 6m5s wcp-domain-wcp-server2 0/1 Running 0 6m4s wcp-domain-wcp-server2 1/1 Running 0 6m18s wcp-domain-wcp-server1 1/1 Running 0 6m54s "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/manage-wcportal-domains/monitoring-and-publishing-logs/publishing-logs/logstash/",
	"title": "Logstash",
	"tags": [],
	"description": "Describes how to configure a WebCenter Portal domain to use logstash and publish the WebLogic Server logs to Elasticsearch.",
	"content": "Install Elasticsearch and Kibana To install Elasticsearch and Kibana, run the following command:\n$ cd ${WORKDIR}/elasticsearch-and-kibana $ kubectl create -f elasticsearch_and_kibana.yaml Publish to Elasticsearch The diagnostics or other logs can be pushed to Elasticsearch server using logstash pod. The logstash pod should have access to the shared domain home or the log location. In case of the Oracle WebCenter Portal domain, the persistent volume of the domain home can be used in the logstash pod. The steps to create the logstash pod are,\n  Get domain home persistence volume claim details of the Oracle WebCenter Portal domain. The following command will list the persistent volume claim details in the namespace - wcpns. In the example below the persistent volume claim is wcp-domain-domain-pvc.\n$ kubectl get pv -n wcpns NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE wcp-domain-domain-pv 10Gi RWX Retain Bound wcpns/wcp-domain-domain-pvc wcp-domain-domain-storage-class 175d   Create logstash configuration file logstash.conf. Below is a sample Logstash configuration file is located at ${WORKDIR}/logging-services/logstash. Below configuration pushes diagnostic and all domains logs.\ninput { file { path =\u0026gt; \u0026#34;/u01/oracle/user_projects/domains/wcp-domain/servers/**/logs/*-diagnostic.log\u0026#34; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026#34;/u01/oracle/user_projects/domains/logs/wcp-domain/*.log\u0026#34; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026#34;message\u0026#34;, \u0026#34;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026#34; ] } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;elasticsearch.default.svc.cluster.local:9200\u0026#34;] } }   Copy the logstash.conf into say /u01/oracle/user_projects/domains so that it can be used for logstash deployment, using Administration Server pod ( For example wcp-domain-adminserver pod in namespace wcpns):\n$ kubectl cp ${WORKDIR}/logging-services/logstash/logstash.conf wcpns/wcp-domain-adminserver:/u01/oracle/user_projects/domains -n wcpns   Create deployment YAML logstash.yaml for logstash pod using the domain home persistence volume claim. Make sure to point the logstash configuration file to correct location ( For example: we copied logstash.conf to /u01/oracle/user_projects/domains/logstash.conf) and also correct domain home persistence volume claim. Sample Logstash deployment is located at kubernetes/samples/scripts/create-wcp-domain/utils/logstash/logstash.yaml:\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: logstash\rnamespace: wcpns\rspec:\rselector:\rmatchLabels:\rapp: logstash\rtemplate: metadata:\rlabels:\rapp: logstash\rspec:\rvolumes:\r- name: domain-storage-volume\rpersistentVolumeClaim:\rclaimName: wcp-domain-domain-pvc\r- name: shared-logs\remptyDir: {}\rcontainers:\r- name: logstash\rimage: logstash:6.6.0\rcommand: [\u0026quot;/bin/sh\u0026quot;]\rargs: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logstash.conf\u0026quot;]\rimagePullPolicy: IfNotPresent\rvolumeMounts:\r- mountPath: /u01/oracle/user_projects/domains\rname: domain-storage-volume\r- name: shared-logs\rmountPath: /shared-logs\rports:\r- containerPort: 5044\rname: logstash\r  Deploy logstash to start publish logs to Elasticsearch\n$ kubectl create -f ${WORKDIR}/logging-services/logstash/logstash.yaml   Create an Index Pattern in Kibana Create an index pattern logstash* in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard:\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/oracle-cloud/configure-load-balancer/",
	"title": "Set up a load balancer",
	"tags": [],
	"description": "Configure different load balancers for Oracle WebCenter Content domains.",
	"content": "WebLogic Kubernetes Operator supports ingress-based load balancers such as Traefik.\n Traefik  Configure the ingress-based Traefik load balancer for Oracle WebCenter Content domains.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/patch_and_upgrade/upgrade-k8s-cluster/",
	"title": "Upgrade a Kubernetes cluster",
	"tags": [],
	"description": "Upgrade the underlying Kubernetes cluster version in a running WebCenter Content Kubernetes environment.",
	"content": "These instructions describe how to upgrade a Kubernetes cluster created using kubeadm on which an Oracle WebCenter Content domain is deployed. A rolling upgrade approach is used to upgrade nodes (master and worker) of the Kubernetes cluster.\nIt is expected that there will be a down time during the upgrade of the Kubernetes cluster as the nodes need to be drained as part of the upgrade process.\n Prerequisites  Review Prerequisites and ensure that your Kubernetes cluster is ready for upgrade. Make sure your environment meets all prerequisites. Make sure the database used for the WebCenter Content domain deployment is up and running during the upgrade process.  Upgrade the Kubernetes version An upgrade of Kubernetes is supported from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. For example, you can upgrade from 1.x to 1.x+1, but not from 1.x to 1.x+2. To upgrade a Kubernetes version, first all the master nodes of the Kubernetes cluster must be upgraded sequentially, followed by the sequential upgrade of each worker node.\n See here for Kubernetes official documentation to upgrade from v1.14.x to v1.15.x. See here for Kubernetes official documentation to upgrade from v1.15.x to v1.16.x. See here for Kubernetes official documentation to upgrade from v1.16.x to v1.17.x. See here for Kubernetes official documentation to upgrade from v1.17.x to v1.18.x.  "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/oracle-cloud/create-wccontent-domains/",
	"title": "Create Oracle WebCenter Content domain",
	"tags": [],
	"description": "Create Oracle WebCenter Content domain on Oracle Kubernetes Engine (OKE).",
	"content": "Contents  Run the create domain script Create Container Clusters (OKE) Verify the results Verify the pods Verify the services Expose service for IBR intradoc port  Run the create domain script Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o \u0026lt;path to output-directory\u0026gt; The script will perform the following steps:\n  Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is \u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script.\n  Create a Kubernetes job that will start up a utility Oracle WebCenter Content container and run offline WLST scripts to create the domain on the shared storage.\n  Run and wait for the job to finish.\n  Create a Kubernetes domain YAML file, domain.yaml, in the \u0026ldquo;output\u0026rdquo; directory that was created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command.\n  Run oke-start-managed-server-wrapper.sh script, which intrenally applies the domain YAML. This script also applies initial configurations for Managed Server containers and readies Managed Servers for future inter-container communications.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ ./oke-start-managed-servers-wrapper.sh -o \u0026lt;path_to_output_directory\u0026gt; -l \u0026lt;load_balancer_external_ip\u0026gt; -p \u0026lt;load_balancer_port\u0026gt;   Verify the results The create domain script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nGenerated YAML files with the default inputs   Click here to see sample content of the generated `domain.yaml`.   $ cat output/weblogic-domains/wccinfra/domain.yaml # Copyright (c) 2017, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v8\u0026quot; kind: Domain metadata: name: wccinfra namespace: wccns labels: weblogic.domainUID: wccinfra spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/wccinfra maxClusterConcurrentStartup: 1 # The domain home source type # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server image that the WebLogic Kubernetes Operator uses to start the domain image: \u0026quot;phx.ocir.io/xxxxxxxxxx/oracle/wccontent/oracle/wccontent:x.x.x.x\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image imagePullSecrets: - name: image-secret # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: wccinfra-domain-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # Whether to write HTTP access log file to log home httpAccessLogInLogHome: true # The in-pod location for domain log, server logs, server out, introspector out, and Node Manager log files logHome: /u01/oracle/user_projects/domains/logs/wccinfra # An (optional) in-pod location for data storage of default and custom file stores. # If not specified or the value is either not set or empty (e.g. dataHome: \u0026quot;\u0026quot;) then the # data storage directories are determined from the WebLogic domain home configuration. dataHome: \u0026quot;\u0026quot; # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the WebLogic Kubernetes Operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026quot; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wccinfra-domain-pvc volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; # adminService: # channels: # The Admin Server's NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: ibr_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 1 # The number of managed servers to start for unlisted clusters # replicas: 1 # Istio # configuration: # istio: # enabled: # readinessPort: - clusterName: ucm_cluster clusterService: annotations: traefik.ingress.kubernetes.io/affinity: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/service.sticky.cookie: \u0026quot;true\u0026quot; traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; replicas: 3 # The number of managed servers to start for unlisted clusters # replicas: 1    Verify the domain To confirm that the domain was created, enter the following command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\n  Click here to see a sample domain description.   [opc@bastionhost domain-home-on-pv]$ kubectl describe domain wccinfra -n wccns Name: wccinfra Namespace: wccns Labels: weblogic.domainUID=wccinfra Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;weblogic.oracle/v8\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Domain\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;weblogic.domainUID\u0026quot;:\u0026quot;wccinfra\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;wccinfr... API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2021-08-24T12:26:19Z Generation: 33 Managed Fields: API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:weblogic.domainUID: Manager: kubectl Operation: Update Time: 2021-09-30T10:56:07Z API Version: weblogic.oracle/v8 Fields Type: FieldsV1 fieldsV1: f:status: .: f:clusters: f:conditions: f:introspectJobFailureCount: f:servers: f:startTime: Manager: Kubernetes Java Client Operation: Update Time: 2021-10-04T20:06:17Z Resource Version: 115422662 Self Link: /apis/weblogic.oracle/v8/namespaces/wccns/domains/wccinfra UID: e283c968-b80b-404b-aa1e-711080d7cc38 Spec: Admin Server: Server Start State: RUNNING Clusters: Cluster Name: ibr_cluster Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Cluster Name: ucm_cluster Cluster Service: Annotations: traefik.ingress.kubernetes.io/affinity: true traefik.ingress.kubernetes.io/service.sticky.cookie: true traefik.ingress.kubernetes.io/session-cookie-name: JSESSIONID Replicas: 3 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Service: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/wccinfra Domain Home Source Type: PersistentVolume Http Access Log In Log Home: true Image: phx.ocir.io/xxxxxxxxxx/oracle/wccontent:x.x.x.x Image Pull Policy: IfNotPresent Image Pull Secrets: Name: image-secret Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/wccinfra Log Home Enabled: true Max Cluster Concurrent Startup: 1 Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m Volume Mounts: Mount Path: /u01/oracle/user_projects/domains Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: wccinfra-domain-pvc Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: wccinfra-domain-credentials Status: Clusters: Cluster Name: ibr_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Cluster Name: ucm_cluster Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 3 Replicas: 3 Replicas Goal: 3 Conditions: Last Transition Time: 2021-09-30T11:04:35.889547Z Reason: ServersReady Status: True Type: Available Introspect Job Failure Count: 0 Servers: Desired State: RUNNING Health: Activation Time: 2021-09-30T10:58:38.381000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.0.10.135 Server Name: adminserver State: RUNNING Cluster Name: ibr_cluster Desired State: RUNNING Health: Activation Time: 2021-09-30T11:01:09.987000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.0.10.135 Server Name: ibr_server1 State: RUNNING Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server2 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server3 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server4 Cluster Name: ibr_cluster Desired State: SHUTDOWN Server Name: ibr_server5 Cluster Name: ucm_cluster Desired State: RUNNING Health: Activation Time: 2021-09-30T11:00:36.369000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.0.10.142 Server Name: ucm-server1 State: RUNNING Cluster Name: ucm_cluster Desired State: RUNNING Health: Activation Time: 2021-09-30T11:02:35.448000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.0.10.135 Server Name: ucm-server2 State: RUNNING Cluster Name: ucm_cluster Desired State: RUNNING Health: Activation Time: 2021-09-30T11:04:32.314000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: 10.0.10.142 Server Name: ucm-server3 State: RUNNING Cluster Name: ucm_cluster Desired State: SHUTDOWN Server Name: ucm-server4 Cluster Name: ucm_cluster Desired State: SHUTDOWN Server Name: ucm-server5 Start Time: 2021-08-24T12:26:20.033714Z Events: \u0026lt;none\u0026gt;    In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The WebLogic Kubernetes Operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Enter the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command. You can verify that an Administration Server and Managed Servers for ucm and ibr cluster are running.\n$ kubectl get pod -n wccns NAME READY STATUS RESTARTS AGE rcu 1/1 Running 0 54d wccinfra-adminserver 1/1 Running 0 18d wccinfra-create-fmw-infra-sample-domain-job-xqnn4 0/1 Completed 0 54d wccinfra-ibr-server1 1/1 Running 0 18d wccinfra-ucm-server1 1/1 Running 0 18d wccinfra-ucm-server2 1/1 Running 0 18d wccinfra-ucm-server3 1/1 Running 0 18d Verify the services Enter the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command.\n  Click here to see a sample list of services.   $ kubectl get services -n wccns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oracle-db LoadBalancer 10.96.74.187 123.45.xxx.xxx 1521:30011/TCP 80d wccinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 18d wccinfra-cluster-ibr-cluster ClusterIP 10.96.206.89 \u0026lt;none\u0026gt; 16250/TCP 119s wccinfra-cluster-ucm-cluster ClusterIP 10.96.180.150 \u0026lt;none\u0026gt; 16200/TCP 54d wccinfra-ibr-server1 ClusterIP None \u0026lt;none\u0026gt; 16250/TCP 18d wccinfra-ibr-server2 ClusterIP 10.96.185.209 \u0026lt;none\u0026gt; 16250/TCP 18d wccinfra-ibr-server3 ClusterIP 10.96.43.99 \u0026lt;none\u0026gt; 16250/TCP 18d wccinfra-ibr-server4 ClusterIP 10.96.77.52 \u0026lt;none\u0026gt; 16250/TCP 18d wccinfra-ibr-server5 ClusterIP 10.96.63.174 \u0026lt;none\u0026gt; 16250/TCP 18d wccinfra-ucm-server1 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 18d wccinfra-ucm-server2 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 18d wccinfra-ucm-server3 ClusterIP None \u0026lt;none\u0026gt; 16200/TCP 18d wccinfra-ucm-server4 ClusterIP 10.96.141.251 \u0026lt;none\u0026gt; 16200/TCP 18d wccinfra-ucm-server5 ClusterIP 10.96.85.52 \u0026lt;none\u0026gt; 16200/TCP 18d    Expose service for IBR intradoc port  Get the IP address for the node, hosting ibr managed server pod. In this sample, node running wccinfra-ibr-server1 pod has ip \u0026lsquo;10.0.10.xx\u0026rsquo; $ kubectl get pods -n wccns -o wide #output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES wccinfra-adminserver 1/1 Running 0 4h50m 10.244.0.150 10.0.10.xxx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; wccinfra-create-fmw-infra-sample-domain-job-zbsxr 0/1 Completed 0 7d22h 10.244.1.25 10.0.10.xx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; wccinfra-ibr-server1 1/1 Running 0 4h48m 10.244.1.38 10.0.10.xx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; wccinfra-ucm-server1 1/1 Running 0 4h48m 10.244.1.39 10.0.10.xx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; wccinfra-ucm-server2 1/1 Running 0 4h46m 10.244.0.151 10.0.10.xxx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; wccinfra-ucm-server3 1/1 Running 0 4h44m 10.244.1.40 10.0.10.xx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  Expose service for IBR intradoc port $ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-wcc-domain/domain-home-on-pv/ $ kubectl expose service/wccinfra-cluster-ibr-cluster --name wccinfra-cluster-ibr-cluster-ext --port=5555 --target-port=5555 --external-ip=\u0026lt;your-ibr-managed-server-node-ip\u0026gt; -n wccns #sample $ kubectl expose service/wccinfra-cluster-ibr-cluster --name wccinfra-cluster-ibr-cluster-ext --port=5555 --target-port=5555 --external-ip=10.0.10.xx -n wccns $ kubectl get service/wccinfra-cluster-ibr-cluster-ext -n wccns -o yaml \u0026gt; wccinfra-cluster-ibr-cluster-ext.yaml $ sed -i \u0026#34;0,/5555/s//16250/\u0026#34; wccinfra-cluster-ibr-cluster-ext.yaml $ kubectl -n wccns apply -f wccinfra-cluster-ibr-cluster-ext.yaml  Verify ibr service name \u0026lsquo;wccinfra-cluster-ibr-cluster-ext\u0026rsquo; $ kubectl get svc -n wccns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oracle-db LoadBalancer 10.96.74.187 123.45.xxx.xxx 1521:30011/TCP 13d wccinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 5h10m wccinfra-cluster-ibr-cluster ClusterIP 10.96.155.21 \u0026lt;none\u0026gt; 16250/TCP 20s wccinfra-cluster-ibr-cluster-ext ClusterIP 10.96.152.184 10.0.10.xx 5555/TCP 7d3h wccinfra-cluster-ucm-cluster ClusterIP 10.96.136.224 \u0026lt;none\u0026gt; 16200/TCP 7d4h   "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/patch_and_upgrade/",
	"title": "Patch and upgrade",
	"tags": [],
	"description": "",
	"content": "Patch an existing Oracle WebCenter Content image or upgrade the infrastructure, such as upgrading the underlying Kubernetes cluster to a new release and upgrading the WebLogic Kubernetes Operator release.\n Patch an image  Create a patched Oracle WebCenter Content image using the WebLogic Image Tool.\n Upgrade an WebLogic Kubernetes Operator release  Upgrade the WebLogic Kubernetes Operator release to a newer version.\n Upgrade a Kubernetes cluster  Upgrade the underlying Kubernetes cluster version in a running WebCenter Content Kubernetes environment.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/adminguide/weblogic-logging-exporter-setup/",
	"title": "Publish logs to Elasticsearch",
	"tags": [],
	"description": "Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.",
	"content": "The WebLogic Logging Exporter adds a log event handler to WebLogic Server. WebLogic Server logs can be pushed to Elasticsearch in Kubernetes directly by using the Elasticsearch REST API. For more details, see to the WebLogic Logging Exporter project.\nThis sample shows you how to publish WebLogic Server logs to Elasticsearch and view them in Kibana. For publishing WebLogic Kubernetes Operator logs, see this sample.\nPrerequisites This document assumes that you have already set up Elasticsearch and Kibana for logs collection. If you have not, please see this document.\n Download the WebLogic Logging Exporter binaries The pre-built binaries are available on the WebLogic Logging Exporter Releases page.\nDownload:\n weblogic-logging-exporter-1.0.0.jar from the Releases page. snakeyaml-1.25.jar from Maven Central.  These identifiers are used in the sample commands in this document.\n wccns: WebCenter Content domain namespace wccinfra: domainUID wccinfra-adminserver: Administration Server pod name   Copy the JAR Files to the WebLogic Domain Home Copy the weblogic-logging-exporter-1.0.0.jar and snakeyaml-1.25.jar files to the domain home directory in the Administration Server pod.\n$ kubectl cp \u0026lt;file-to-copy\u0026gt; \u0026lt;namespace\u0026gt;/\u0026lt;Administration-Server-pod\u0026gt;:\u0026lt;domainhome\u0026gt; $ kubectl cp weblogic-logging-exporter-1.0.0.jar wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/wccinfra/ $ kubectl cp snakeyaml-1.25.jar wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/wccinfra/ Add a Startup Class to the Domain Configuration In this step, we configure weblogic-logging-exporter JAR as a startup class in the WebLogic servers where we intend to collect the logs.\n  In the WebLogic Server Administration Console, in the left navigation pane, expand Environment, and then select Startup and Shutdown Classes.\n  Add a new startup class. You may choose any descriptive name, however, the class name must be weblogic.logging.exporter.Startup.\n  Target the startup class to each server from which you want to export logs.\n  You can verify this by checking for the update in your config.xml file(/u01/oracle/user_projects/domains/wccinfra/config/config.xml) which should be similar to this example:\n$ kubectl exec -n wccns -it wccinfra-adminserver cat /u01/oracle/user_projects/domains/wccinfra/config/config.xml \u0026lt;startup-class\u0026gt; \u0026lt;name\u0026gt;weblogic-logging-exporter\u0026lt;/name\u0026gt; \u0026lt;target\u0026gt;AdminServer,ucm_cluster,ibr_cluster,ipm_cluster,capture_cluster,wccadf_cluster\u0026lt;/target\u0026gt; \u0026lt;class-name\u0026gt;weblogic.logging.exporter.Startup\u0026lt;/class-name\u0026gt; \u0026lt;/startup-class\u0026gt;   Update the WebLogic Server CLASSPATH   Copy the setDomainEnv.sh file from the pod to a local folder:\n$ kubectl cp wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/wccinfra/bin/setDomainEnv.sh $PWD/setDomainEnv.sh tar: Removing leading `/' from member names Ignore exception: tar: Removing leading '/' from member names\n  Modify setDomainEnv.sh to update the Server Class path, add below code at the end of file:\nCLASSPATH=/u01/oracle/user_projects/domains/wccinfra/weblogic-logging-exporter-1.0.0.jar:/u01/oracle/user_projects/domains/wccinfra/snakeyaml-1.25.jar:${CLASSPATH} export CLASSPATH   Copy back the modified setDomainEnv.sh file to the pod:\n$ kubectl cp setDomainEnv.sh wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/wccinfra/bin/setDomainEnv.sh ``\n  Create a Configuration File for the WebLogic Logging Exporter In this step, we will be creating the configuration file for weblogic-logging-exporter.\n  Specify the Elasticsearch server host and port number in file kubernetes/samples/scripts/create-wcc-domain/utils/weblogic-logging-exporter/WebLogicLoggingExporter.yaml:\nExample:\nweblogicLoggingIndexName: wls publishHost: elasticsearch.default.svc.cluster.local publishPort: 9200 domainUID: wccinfra weblogicLoggingExporterEnabled: true weblogicLoggingExporterSeverity: Notice weblogicLoggingExporterBulkSize: 2 weblogicLoggingExporterFilters: - FilterExpression: NOT(MSGID = 'BEA-000449')   Copy the WebLogicLoggingExporter.yaml file to the domain home directory in the WebLogic Administration Server pod:\n$ kubectl cp kubernetes/samples/scripts/create-wcc-domain/utils/weblogic-logging-exporter/WebLogicLoggingExporter.yaml wccns/wccinfra-adminserver:/u01/oracle/user_projects/domains/wccinfra/config/   Restart All the Servers in the Domain To restart the servers, stop and then start them using the following commands:\nTo STOP the servers: $ kubectl patch domain wccinfra -n wccns --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;NEVER\u0026quot; }]' To START the servers: $ kubectl patch domain wccinfra -n wccns --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;IF_NEEDED\u0026quot; }]' After all the servers are restarted, see their server logs to check that the weblogic-logging-exporter class is called, as shown below:\n======================= Weblogic Logging Exporter Startup class called ================== Reading configuration from file name: /u01/oracle/user_projects/domains/wccinfra/config/WebLogicLoggingExporter.yaml Config{weblogicLoggingIndexName='wls', publishHost='elasticsearch.default.svc.cluster.local', publishPort=9200, weblogicLoggingExporterSeverity='Notice', weblogicLoggingExporterBulkSize='1', enabled=true, weblogicLoggingExporterFilters=[ FilterConfig{expression='NOT(MSGID = 'BEA-000449')', servers=[]}], domainUID='wccinfra'} ====================== WebLogic Logging Exporter is ebled ================= publishHost in initialize: elasticsearch.default.svc.cluster.local ================= publishPort in initialize: 9200 ================= url in executePutOrPostOnUrl: http://elasticsearch.default.svc.cluster.local:9200/wls Create an Index Pattern in Kibana Create an appropriate index pattern in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard.\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/manage-wcportal-domains/",
	"title": "Administration Guide",
	"tags": [],
	"description": "Describes how to use some common utility tools and configurations to administer  WebCenter Portal domain.",
	"content": "Administer Oracle WebCenter Portal domain in Kubernetes.\n Set up a load balancer  Configure different load balancers for the Oracle WebCenter Portal domain.\n Monitor a domain and publish logs  Monitor Oracle WebCenter Portal and publishing logs to Elasticsearch.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle WebCenter Portal Docker image used for deploying Oracle WebCenter Portal domains. An Oracle WebCenter Portal Docker image can be created using the WebLogic Image Tool or using the Dockerfile approach.",
	"content": "You can build an Oracle WebCenter Portal image for production deployments with patches (bundle or interim) using the WebLogic Image Tool, you must have access to the My Oracle Support (MOS) to download (bundle or interim) patches.\n Create or update an Oracle WebCenter Portal Docker image using the WebLogic Image Tool  Set up the WebLogic Image Tool Create an image Update an image   Create an Oracle WebCenter Portal Docker image using Dockerfile  Create or update an Oracle WebCenter Portal Docker image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle WebCenter Portal Docker image (can include patches as well) or update an existing image with one or more patches (bundle patch and interim patches).\n Recommendations:\n Use create for creating a new Oracle WebCenter Portal Docker image:  without any patches or, containing the Oracle WebCenter Portal binaries, bundle , and interim patches. This is the recommended approach if you have access to the Oracle WebCenter Portal patches because it optimizes the size of the image.   Use update for patching an existing Oracle WebCenter Portal Docker image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.    Prerequisites Set up the WebLogic Image Tool Validate the setup WebLogic Image Tool build directory WebLogic Image Tool cache Set up additional build scripts  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location.  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it. In these steps, this directory is imagetool-setup.\n$ mkdir imagetool-setup $ cd imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n  Unzip the release ZIP file to the imagetool-setup directory.\n  Execute the following commands to set up the WebLogic Image Tool on a Linux environment:\n$ cd imagetool-setup/imagetool/bin $ source setup.sh   Validate the setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory is deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts To create an Oracle WebCenter Portal Docker image using the WebLogic Image Tool, additional container scripts for Oracle WebCenter Portal domains are required.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd imagetool-setup $ git clone https://github.com/oracle/docker-images.git   Copy the additional WebLogic Image Tool build files from the operator source repository to the imagetool-setup location:\n$ mkdir -p imagetool-setup/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0 $ cd imagetool-setup/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0 $ cp -rf ${WORKDIR}/imagetool-scripts/* .    Note: To create the image, continue with the following steps. To update the image, see update an image.\n Create an image After setting up the WebLogic Image Tool and configuring the required build scripts, create a new Oracle WebCenter Portal Docker image using the WebLogic Image Tool as described ahead.\nDownload the Oracle WebCenter Portal installation binaries and patches You must download the required Oracle WebCenter Portal installation binaries and patches listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice. In these steps, the directory is download location.\nThe installation binaries and patches required for release 21.2.3 are:\n  JDK:\n jdk-8u281-linux-x64.tar.gz    Fusion Middleware Infrastructure installer:\n fmw_12.2.1.4.0_infrastructure.jar    WCP installers:\n fmw_12.2.1.4.0_wcportal.jar    Fusion Middleware Infrastructure patches:\n p28186730_139425_Generic.zip (OPatch) p32253037_122140_Generic.zip(WLS) p31544353_122140_Linux-x86-64.zip(WLS ADR Patch) p32124456_122140_Generic.zip(Bundle patch for Oracle Coherence Version 12.2.1.4.7) p31666198_122140_Generic.zip(OPSS Bundle Patch 12.2.1.4.200724) p32357288_122140_Generic.zip(ADF BUNDLE PATCH 12.2.1.4.210107)    WCP patches:\n p32224021_122140_Generic.zip(WCP BUNDLE PATCH 12.2.1.4.201126) p31852495_122140_Generic.zip(WEBCENTER CORE BUNDLE PATCH 12.2.1.4.200905))    Update required build files The following files in the code repository location \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0 are used for creating the image:\n additionalBuildCmds.txt buildArgs    In the buildArgs file, update all occurrences of %DOCKER_REPO% with the docker-images repository location, which is the complete path of \u0026lt;imagetool-setup-location\u0026gt;/docker-images.\nFor example, update:\n%DOCKER_REPO%/OracleWebCenterPortal/imagetool/12.2.1.4.0/\nto:\n\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0/\n  Similarly, update the placeholders %JDK_VERSION% and %BUILDTAG% with appropriate values.\n  Update the response file \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleFMWInfrastructure/dockerfiles/12.2.1.4/install.file to add the parameter INSTALL_TYPE=\u0026quot;Fusion Middleware Infrastructure\u0026quot; in the [GENERIC] section.\n  Create the image   Add a JDK package to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type jdk --version 8u281 --path \u0026lt;download location\u0026gt;/jdk-8u281-linux-x64.tar.gz   Add the downloaded installation binaries to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type fmw --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_infrastructure.jar $ imagetool cache addInstaller --type wcp --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_wcportal.jar   Add the downloaded OPatch patch to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.5 --value \u0026lt;download location\u0026gt;/p28186730_139425_Generic.zip   Append the --opatchBugNumber flag and the OPatch patch key to the create command in the buildArgs file:\n--opatchBugNumber 28186730_13.9.4.2.5   Add the downloaded product patches to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key 32253037_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32253037_122140_Generic.zip $ imagetool cache addEntry --key 32124456_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32124456_122140_Generic.zip $ imagetool cache addEntry --key 32357288_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32357288_122140_Generic.zip $ imagetool cache addEntry --key 32224021_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p32224021_122140_Generic.zip $ imagetool cache addEntry --key 31666198_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p31666198_122140_Generic.zip $ imagetool cache addEntry --key 31544353_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p31544353_122140_Linux-x86-64.zip $ imagetool cache addEntry --key 31852495_12.2.1.4.0 --value \u0026lt;download location\u0026gt;/p31852495_122140_Generic.zip   Append the --patches flag and the product patch keys to the create command in the buildArgs file. The --patches list must be a comma-separated collection of patch --key values used in the imagetool cache addEntry commands above.\nSample --patches list for the product patches added in to the cache:\n--patches 32253037_12.2.1.4.0,32124456_12.2.1.4.0,32357288_12.2.1.4.0,32224021_12.2.1.4.0 Example buildArgs file after appending the OPatch patch and product patches:\ncreate --jdkVersion=8u281 --type wcp --version=12.2.1.4.0 --tag=oracle/wcportal:12.2.1.4 --pull --fromImage ghcr.io/oracle/oraclelinux:7-slim --additionalBuildCommands \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterPortal/dockerfiles/12.2.1.4/container-scripts --opatchBugNumber 28186730_13.9.4.2.5 --patches 32253037_12.2.1.4.0,32124456_12.2.1.4.0,32357288_12.2.1.4.0,32224021_12.2.1.4.0,31666198_12.2.1.4.0,31544353_12.2.1.4.0,31852495_12.2.1.4.0  Note: In the buildArgs file:\n --jdkVersion value must match the --version value used in the imagetool cache addInstaller command for --type jdk. --version value must match the --version value used in the imagetool cache addInstaller command for --type wcp. --pull always pulls the latest base Linux image oraclelinux:7-slim from the Docker registry. This flag can be removed if you want to use the Linux image oraclelinux:7-slim, which is already available on the host where the WCP image is created.   Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Create the Oracle WebCenter Portal image:\n$ imagetool @\u0026lt;absolute path to buildargs file\u0026gt;  Note: Make sure that the absolute path to the buildargs file is prepended with a @ character, as shown in the example above.\n For example:\n$ imagetool @\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterPortal/imagetool/12.2.1.4.0/buildArgs    Click here to see the sample Dockerfile generated with the imagetool command.    ########## BEGIN DOCKERFILE ########## # # Copyright (c) 2019, 2021, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # FROM ghcr.io/oracle/oraclelinux:7-slim as os_update LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;dabe3ff7-ec35-4b8d-b62a-c3c02fed5571\u0026quot; USER root RUN yum -y --downloaddir=/tmp/imagetool install gzip tar unzip libaio jq hostname procps sudo zip \\ \u0026amp;\u0026amp; yum -y --downloaddir=/tmp/imagetool clean all \\ \u0026amp;\u0026amp; rm -rf /var/cache/yum/* \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool ## Create user and group RUN if [ -z \u0026quot;$(getent group oracle)\u0026quot; ]; then hash groupadd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; groupadd oracle || exit -1 ; fi \\ \u0026amp;\u0026amp; if [ -z \u0026quot;$(getent passwd oracle)\u0026quot; ]; then hash useradd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; useradd -g oracle oracle || exit -1; fi \\ \u0026amp;\u0026amp; mkdir -p /u01 \\ \u0026amp;\u0026amp; chown oracle:oracle /u01 \\ \u0026amp;\u0026amp; chmod 775 /u01 # Install Java FROM os_update as jdk_build LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;dabe3ff7-ec35-4b8d-b62a-c3c02fed5571\u0026quot; ENV JAVA_HOME=/u01/jdk COPY --chown=oracle:oracle jdk-8u251-linux-x64.tar.gz /tmp/imagetool/ USER oracle RUN tar xzf /tmp/imagetool/jdk-8u251-linux-x64.tar.gz -C /u01 \\ \u0026amp;\u0026amp; $(test -d /u01/jdk* \u0026amp;\u0026amp; mv /u01/jdk* /u01/jdk || mv /u01/graal* /u01/jdk) \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool \\ \u0026amp;\u0026amp; rm -f /u01/jdk/javafx-src.zip /u01/jdk/src.zip # Install Middleware FROM os_update as wls_build LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;dabe3ff7-ec35-4b8d-b62a-c3c02fed5571\u0026quot; ENV JAVA_HOME=/u01/jdk \\ ORACLE_HOME=/u01/oracle \\ OPATCH_NO_FUSER=true RUN mkdir -p /u01/oracle \\ \u0026amp;\u0026amp; mkdir -p /u01/oracle/oraInventory \\ \u0026amp;\u0026amp; chown oracle:oracle /u01/oracle/oraInventory \\ \u0026amp;\u0026amp; chown oracle:oracle /u01/oracle COPY --from=jdk_build --chown=oracle:oracle /u01/jdk /u01/jdk/ COPY --chown=oracle:oracle fmw_12.2.1.4.0_infrastructure.jar fmw.rsp /tmp/imagetool/ COPY --chown=oracle:oracle fmw_12.2.1.4.0_wcportal.jar wcp.rsp /tmp/imagetool/ COPY --chown=oracle:oracle oraInst.loc /u01/oracle/ COPY --chown=oracle:oracle p28186730_139425_Generic.zip /tmp/imagetool/opatch/ COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ USER oracle RUN echo \u0026quot;INSTALLING MIDDLEWARE\u0026quot; \\ \u0026amp;\u0026amp; echo \u0026quot;INSTALLING fmw\u0026quot; \\ \u0026amp;\u0026amp; \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_infrastructure.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/fmw.rsp -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\ \u0026amp;\u0026amp; echo \u0026quot;INSTALLING wcp\u0026quot; \\ \u0026amp;\u0026amp; \\ /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_wcportal.jar -silent ORACLE_HOME=/u01/oracle \\ -responseFile /tmp/imagetool/wcp.rsp -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\ \u0026amp;\u0026amp; chmod -R g+r /u01/oracle RUN cd /tmp/imagetool/opatch \\ \u0026amp;\u0026amp; /u01/jdk/bin/jar -xf /tmp/imagetool/opatch/p28186730_139425_Generic.zip \\ \u0026amp;\u0026amp; /u01/jdk/bin/java -jar /tmp/imagetool/opatch/6880880/opatch_generic.jar -silent -ignoreSysPrereqs -force -novalidation oracle_home=/u01/oracle # Apply all patches provided at the same time RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \\ \u0026amp;\u0026amp; test $? -eq 0 \\ \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle \\ || (cat /u01/oracle/cfgtoollogs/opatch/opatch*.log \u0026amp;\u0026amp; exit 1) FROM os_update as final_build ARG ADMIN_NAME ARG ADMIN_HOST ARG ADMIN_PORT ARG MANAGED_SERVER_PORT ENV ORACLE_HOME=/u01/oracle \\ JAVA_HOME=/u01/jdk \\ PATH=${PATH}:/u01/jdk/bin:/u01/oracle/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;dabe3ff7-ec35-4b8d-b62a-c3c02fed5571\u0026quot; COPY --from=jdk_build --chown=oracle:oracle /u01/jdk /u01/jdk/ COPY --from=wls_build --chown=oracle:oracle /u01/oracle /u01/oracle/ USER oracle WORKDIR /u01/oracle #ENTRYPOINT /bin/bash ENV ORACLE_HOME=/u01/oracle \\ SCRIPT_FILE=/u01/oracle/container-scripts/* \\ USER_MEM_ARGS=\u0026quot;-Djava.security.egd=file:/dev/./urandom\u0026quot; \\ PATH=$PATH:/usr/java/default/bin:/u01/oracle/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle/container-scripts USER root RUN env \u0026amp;\u0026amp; \\ mkdir -p /u01/oracle/container-scripts \u0026amp;\u0026amp; \\ mkdir -p /u01/oracle/logs \u0026amp;\u0026amp; \\ mkdir -p /u01/esHome/esNode \u0026amp;\u0026amp; \\ chown oracle:oracle -R /u01 $VOLUME_DIR \u0026amp;\u0026amp; \\ chmod a+xr /u01 COPY --chown=oracle:oracle files/container-scripts/ /u01/oracle/container-scripts/ RUN chmod +xr $SCRIPT_FILE \u0026amp;\u0026amp; \\ rm /u01/oracle/oracle_common/lib/ons.jar /u01/oracle/oracle_common/modules/oracle.jdbc/simplefan.jar USER oracle EXPOSE $WCPORTAL_PORT $ADMIN_PORT WORKDIR ${ORACLE_HOME} CMD [\u0026quot;/u01/oracle/container-scripts/configureOrStartAdminServer.sh\u0026quot;] ########## END DOCKERFILE ##########      Check the created image using the docker images command:\n$ docker images | grep wcportal   Update an image After setting up the WebLogic Image Tool and configuring the build scripts, use the WebLogic Image Tool to update an existing Oracle WebCenter Portal Docker image:\n  Enter the following command to add the OPatch patch to the WebLogic Image Tool cache:\n$ imagetool cache addEntry --key 28186730_13.9.4.2.5 --value \u0026lt;downloaded-patches-location\u0026gt;/p28186730_139425_Generic.zip   Execute the imagetool cache addEntry command for each patch to add the required patch(es) to the WebLogic Image Tool cache. For example, to add patch p30761841_122140_Generic.zip:\n$ imagetool cache addEntry --key=32224021_12.2.1.4.0 --value \u0026lt;downloaded-patches-location\u0026gt;/p32224021_122140_Generic.zip   Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is oracle/wcportal:12.2.1.4. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool updates the OPatch if it is not already updated in the image.\n Examples   Click here to see the example of update command:    $ imagetool update --fromImage oracle/wcportal:12.2.1.4 --tag=wcportal:12.2.1.4-32224021 --patches=32224021_12.2.1.4.0 [INFO ] Image Tool build ID: 50f9b9aa-596c-4bae-bdff-c47c16b4c928 [INFO ] Temporary directory used for docker build context: /scratch/imagetoolcache/builddir/wlsimgbuilder_temp5130105621506307568 [INFO ] Using patch 28186730_13.9.4.2.5 from cache: /home/imagetool-setup/jars/p28186730_139425_Generic.zip [INFO ] Updating OPatch in final image from version 13.9.4.2.1 to version 13.9.4.2.5 [WARNING] Skipping patch conflict check, no support credentials provided [WARNING] No credentials provided, skipping validation of patches [INFO ] Using patch 32224021_12.2.1.4 from cache: /home/imagetool-setup/jars/p32224021_122140_Generic.zip [INFO ] docker cmd = docker build --no-cache --force-rm --tag wcportal:12.2.1.4-32224021 --build-arg http_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg https_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg no_proxy=\u0026lt;IP addresses and Domain address for no_proxy\u0026gt;,/var/run/docker.sock \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp5130105621506307568 Sending build context to Docker daemon 192.4MB Step 1/9 : FROM oracle/wcportal:12.2.1.4 as final_build ---\u0026gt; 5592ff7e5a02 Step 2/9 : USER root ---\u0026gt; Running in 0b3ff2600f11 Removing intermediate container 0b3ff2600f11 ---\u0026gt; faad3a32f39c Step 3/9 : ENV OPATCH_NO_FUSER=true ---\u0026gt; Running in 2beab0bfe88b Removing intermediate container 2beab0bfe88b ---\u0026gt; 6fd9e1664818 Step 4/9 : LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;50f9b9aa-596c-4bae-bdff-c47c16b4c928\u0026quot; ---\u0026gt; Running in 9a5f8fc172c9 Removing intermediate container 9a5f8fc172c9 ---\u0026gt; 499620a1f857 Step 5/9 : USER oracle ---\u0026gt; Running in fe28af056858 Removing intermediate container fe28af056858 ---\u0026gt; 3507971c35d5 Step 6/9 : COPY --chown=oracle:oracle p28186730_139425_Generic.zip /tmp/imagetool/opatch/ ---\u0026gt; c44c3c7b17f7 Step 7/9 : RUN cd /tmp/imagetool/opatch \u0026amp;\u0026amp; /u01/jdk/bin/jar -xf /tmp/imagetool/opatch/p28186730_139425_Generic.zip \u0026amp;\u0026amp; /u01/jdk/bin/java -jar /tmp/imagetool/opatch/6880880/opatch_generic.jar -silent -ignoreSysPrereqs -force -novalidation oracle_home=/u01/oracle \u0026amp;\u0026amp; rm -rf /tmp/imagetool ---\u0026gt; Running in 8380260fe62d Launcher log file is /tmp/OraInstall2021-04-08_05-18-14AM/launcher2021-04-08_05-18-14AM.log. Extracting the installer . . . . Done Checking if CPU speed is above 300 MHz. Actual 2195.098 MHz Passed Checking swap space: must be greater than 512 MB. Actual 14999 MB Passed Checking if this platform requires a 64-bit JVM. Actual 64 Passed (64-bit not required) Checking temp space: must be greater than 300 MB. Actual 152772 MB Passed Preparing to launch the Oracle Universal Installer from /tmp/OraInstall2021-04-08_05-18-14AM Installation Summary Disk Space : Required 34 MB, Available 152,736 MB Feature Sets to Install: Next Generation Install Core 13.9.4.0.1 OPatch 13.9.4.2.5 OPatch Auto OPlan 13.9.4.2.5 Session log file is /tmp/OraInstall2021-04-08_05-18-14AM/install2021-04-08_05-18-14AM.log Loading products list. Please wait. 1% 40% Loading products. Please wait. 98% 99% Updating Libraries Starting Installations 1% 94% 95% 96% Install pending Installation in progress Component : oracle.glcm.logging 1.6.4.0.0 Copying files for oracle.glcm.logging 1.6.4.0.0 Component : oracle.glcm.comdev 7.8.4.0.0 Copying files for oracle.glcm.comdev 7.8.4.0.0 Component : oracle.glcm.dependency 1.8.4.0.0 Copying files for oracle.glcm.dependency 1.8.4.0.0 Component : oracle.glcm.xmldh 3.4.4.0.0 Copying files for oracle.glcm.xmldh 3.4.4.0.0 Component : oracle.glcm.wizard 7.8.4.0.0 Copying files for oracle.glcm.wizard 7.8.4.0.0 Component : oracle.glcm.opatch.common.api 13.9.4.0.0 Copying files for oracle.glcm.opatch.common.api 13.9.4.0.0 Component : oracle.nginst.common 13.9.4.0.0 Copying files for oracle.nginst.common 13.9.4.0.0 Component : oracle.nginst.core 13.9.4.0.0 Copying files for oracle.nginst.core 13.9.4.0.0 Component : oracle.glcm.encryption 2.7.4.0.0 Copying files for oracle.glcm.encryption 2.7.4.0.0 Component : oracle.swd.opatch 13.9.4.2.5 Copying files for oracle.swd.opatch 13.9.4.2.5 Component : oracle.glcm.osys.core 13.9.1.0.0 Copying files for oracle.glcm.osys.core 13.9.1.0.0 Component : oracle.glcm.oplan.core 13.9.4.2.0 Copying files for oracle.glcm.oplan.core 13.9.4.2.0 Install successful Post feature install pending Post Feature installing Feature Set : glcm_common_lib Feature Set : glcm_common_logging_lib Post Feature installing glcm_common_lib Post Feature installing glcm_common_logging_lib Feature Set : commons-cli_1.3.1.0.0 Post Feature installing commons-cli_1.3.1.0.0 Feature Set : oracle.glcm.opatch.common.api.classpath Post Feature installing oracle.glcm.opatch.common.api.classpath Feature Set : glcm_encryption_lib Post Feature installing glcm_encryption_lib Feature Set : oracle.glcm.osys.core.classpath Post Feature installing oracle.glcm.osys.core.classpath Feature Set : oracle.glcm.oplan.core.classpath Post Feature installing oracle.glcm.oplan.core.classpath Feature Set : oracle.glcm.opatchauto.core.classpath Post Feature installing oracle.glcm.opatchauto.core.classpath Feature Set : oracle.glcm.opatchauto.core.binary.classpath Post Feature installing oracle.glcm.opatchauto.core.binary.classpath Feature Set : oracle.glcm.opatchauto.core.actions.classpath Post Feature installing oracle.glcm.opatchauto.core.actions.classpath Feature Set : oracle.glcm.opatchauto.core.wallet.classpath Post Feature installing oracle.glcm.opatchauto.core.wallet.classpath Post feature install complete String substitutions pending String substituting Component : oracle.glcm.logging 1.6.4.0.0 String substituting oracle.glcm.logging 1.6.4.0.0 Component : oracle.glcm.comdev 7.8.4.0.0 String substituting oracle.glcm.comdev 7.8.4.0.0 Component : oracle.glcm.dependency 1.8.4.0.0 String substituting oracle.glcm.dependency 1.8.4.0.0 Component : oracle.glcm.xmldh 3.4.4.0.0 String substituting oracle.glcm.xmldh 3.4.4.0.0 Component : oracle.glcm.wizard 7.8.4.0.0 String substituting oracle.glcm.wizard 7.8.4.0.0 Component : oracle.glcm.opatch.common.api 13.9.4.0.0 String substituting oracle.glcm.opatch.common.api 13.9.4.0.0 Component : oracle.nginst.common 13.9.4.0.0 String substituting oracle.nginst.common 13.9.4.0.0 Component : oracle.nginst.core 13.9.4.0.0 String substituting oracle.nginst.core 13.9.4.0.0 Component : oracle.glcm.encryption 2.7.4.0.0 String substituting oracle.glcm.encryption 2.7.4.0.0 Component : oracle.swd.opatch 13.9.4.2.5 String substituting oracle.swd.opatch 13.9.4.2.5 Component : oracle.glcm.osys.core 13.9.1.0.0 String substituting oracle.glcm.osys.core 13.9.1.0.0 Component : oracle.glcm.oplan.core 13.9.4.2.0 String substituting oracle.glcm.oplan.core 13.9.4.2.0 String substitutions complete Link pending Linking in progress Component : oracle.glcm.logging 1.6.4.0.0 Linking oracle.glcm.logging 1.6.4.0.0 Component : oracle.glcm.comdev 7.8.4.0.0 Linking oracle.glcm.comdev 7.8.4.0.0 Component : oracle.glcm.dependency 1.8.4.0.0 Linking oracle.glcm.dependency 1.8.4.0.0 Component : oracle.glcm.xmldh 3.4.4.0.0 Linking oracle.glcm.xmldh 3.4.4.0.0 Component : oracle.glcm.wizard 7.8.4.0.0 Linking oracle.glcm.wizard 7.8.4.0.0 Component : oracle.glcm.opatch.common.api 13.9.4.0.0 Linking oracle.glcm.opatch.common.api 13.9.4.0.0 Component : oracle.nginst.common 13.9.4.0.0 Linking oracle.nginst.common 13.9.4.0.0 Component : oracle.nginst.core 13.9.4.0.0 Linking oracle.nginst.core 13.9.4.0.0 Component : oracle.glcm.encryption 2.7.4.0.0 Linking oracle.glcm.encryption 2.7.4.0.0 Component : oracle.swd.opatch 13.9.4.2.5 Linking oracle.swd.opatch 13.9.4.2.5 Component : oracle.glcm.osys.core 13.9.1.0.0 Linking oracle.glcm.osys.core 13.9.1.0.0 Component : oracle.glcm.oplan.core 13.9.4.2.0 Linking oracle.glcm.oplan.core 13.9.4.2.0 Linking in progress Link successful Setup pending Setup in progress Component : oracle.glcm.logging 1.6.4.0.0 Setting up oracle.glcm.logging 1.6.4.0.0 Component : oracle.glcm.comdev 7.8.4.0.0 Setting up oracle.glcm.comdev 7.8.4.0.0 Component : oracle.glcm.dependency 1.8.4.0.0 Setting up oracle.glcm.dependency 1.8.4.0.0 Component : oracle.glcm.xmldh 3.4.4.0.0 Setting up oracle.glcm.xmldh 3.4.4.0.0 Component : oracle.glcm.wizard 7.8.4.0.0 Setting up oracle.glcm.wizard 7.8.4.0.0 Component : oracle.glcm.opatch.common.api 13.9.4.0.0 Setting up oracle.glcm.opatch.common.api 13.9.4.0.0 Component : oracle.nginst.common 13.9.4.0.0 Setting up oracle.nginst.common 13.9.4.0.0 Component : oracle.nginst.core 13.9.4.0.0 Setting up oracle.nginst.core 13.9.4.0.0 Component : oracle.glcm.encryption 2.7.4.0.0 Setting up oracle.glcm.encryption 2.7.4.0.0 Component : oracle.swd.opatch 13.9.4.2.5 Setting up oracle.swd.opatch 13.9.4.2.5 Component : oracle.glcm.osys.core 13.9.1.0.0 Setting up oracle.glcm.osys.core 13.9.1.0.0 Component : oracle.glcm.oplan.core 13.9.4.2.0 Setting up oracle.glcm.oplan.core 13.9.4.2.0 Setup successful Save inventory pending Saving inventory 97% Saving inventory complete 98% Configuration complete Component : glcm_common_logging_lib Saving the inventory glcm_common_logging_lib Component : glcm_encryption_lib Component : oracle.glcm.opatch.common.api.classpath Saving the inventory oracle.glcm.opatch.common.api.classpath Saving the inventory glcm_encryption_lib Component : cieCfg_common_rcu_lib Component : glcm_common_lib Saving the inventory cieCfg_common_rcu_lib Saving the inventory glcm_common_lib Component : oracle.glcm.logging Saving the inventory oracle.glcm.logging Component : cieCfg_common_lib Saving the inventory cieCfg_common_lib Component : svctbl_lib Saving the inventory svctbl_lib Component : com.bea.core.binxml_dependencies Saving the inventory com.bea.core.binxml_dependencies Component : svctbl_jmx_client Saving the inventory svctbl_jmx_client Component : cieCfg_wls_shared_lib Saving the inventory cieCfg_wls_shared_lib Component : rcuapi_lib Saving the inventory rcuapi_lib Component : rcu_core_lib Saving the inventory rcu_core_lib Component : cieCfg_wls_lib Saving the inventory cieCfg_wls_lib Component : cieCfg_wls_external_lib Saving the inventory cieCfg_wls_external_lib Component : cieCfg_wls_impl_lib Saving the inventory cieCfg_wls_impl_lib Component : rcu_dependencies_lib Saving the inventory rcu_dependencies_lib Component : oracle.fmwplatform.fmwprov_lib Saving the inventory oracle.fmwplatform.fmwprov_lib Component : fmwplatform-wlst-dependencies Saving the inventory fmwplatform-wlst-dependencies Component : oracle.fmwplatform.ocp_lib Saving the inventory oracle.fmwplatform.ocp_lib Component : oracle.fmwplatform.ocp_plugin_lib Saving the inventory oracle.fmwplatform.ocp_plugin_lib Component : wlst.wls.classpath Saving the inventory wlst.wls.classpath Component : maven.wls.classpath Saving the inventory maven.wls.classpath Component : com.oracle.webservices.fmw.ws-assembler Saving the inventory com.oracle.webservices.fmw.ws-assembler Component : sdpmessaging_dependencies Saving the inventory sdpmessaging_dependencies Component : sdpclient_dependencies Saving the inventory sdpclient_dependencies Component : com.oracle.jersey.fmw.client Saving the inventory com.oracle.jersey.fmw.client Component : com.oracle.webservices.fmw.client Saving the inventory com.oracle.webservices.fmw.client Component : oracle.jrf.wls.classpath Saving the inventory oracle.jrf.wls.classpath Component : oracle.jrf.wlst Saving the inventory oracle.jrf.wlst Component : fmwshare-wlst-dependencies Saving the inventory fmwshare-wlst-dependencies Component : oracle.fmwshare.pyjar Saving the inventory oracle.fmwshare.pyjar Component : com.oracle.webservices.wls.jaxws-owsm-client Saving the inventory com.oracle.webservices.wls.jaxws-owsm-client Component : glcm_common_logging_lib Component : glcm_common_lib Saving the inventory glcm_common_lib Component : glcm_encryption_lib Saving the inventory glcm_encryption_lib Component : oracle.glcm.opatch.common.api.classpath Saving the inventory oracle.glcm.opatch.common.api.classpath Component : cieCfg_common_rcu_lib Saving the inventory cieCfg_common_rcu_lib Saving the inventory glcm_common_logging_lib Component : oracle.glcm.logging Saving the inventory oracle.glcm.logging Component : cieCfg_common_lib Saving the inventory cieCfg_common_lib Component : svctbl_lib Saving the inventory svctbl_lib Component : com.bea.core.binxml_dependencies Saving the inventory com.bea.core.binxml_dependencies Component : svctbl_jmx_client Saving the inventory svctbl_jmx_client Component : cieCfg_wls_shared_lib Saving the inventory cieCfg_wls_shared_lib Component : rcuapi_lib Saving the inventory rcuapi_lib Component : rcu_core_lib Saving the inventory rcu_core_lib Component : cieCfg_wls_lib Saving the inventory cieCfg_wls_lib Component : cieCfg_wls_external_lib Saving the inventory cieCfg_wls_external_lib Component : cieCfg_wls_impl_lib Saving the inventory cieCfg_wls_impl_lib Component : soa_com.bea.core.binxml_dependencies Saving the inventory soa_com.bea.core.binxml_dependencies Component : glcm_common_logging_lib Saving the inventory glcm_common_logging_lib Component : glcm_common_lib Saving the inventory glcm_common_lib Component : glcm_encryption_lib Saving the inventory glcm_encryption_lib Component : oracle.glcm.opatch.common.api.classpath Component : oracle.glcm.oplan.core.classpath Saving the inventory oracle.glcm.oplan.core.classpath Saving the inventory oracle.glcm.opatch.common.api.classpath The install operation completed successfully. Logs successfully copied to /u01/oracle/.inventory/logs. Removing intermediate container 8380260fe62d ---\u0026gt; d57be7ffa162 Step 8/9 : COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ ---\u0026gt; dd421aae5aaf Step 9/9 : RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \u0026amp;\u0026amp; test $? -eq 0 \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle || (cat /u01/oracle/cfgtoollogs/opatch/opatch*.log \u0026amp;\u0026amp; exit 1) ---\u0026gt; Running in 323e7ae70339 Oracle Interim Patch Installer version 13.9.4.2.5 Copyright (c) 2021, Oracle Corporation. All rights reserved. Oracle Home : /u01/oracle Central Inventory : /u01/oracle/.inventory from : /u01/oracle/oraInst.loc OPatch version : 13.9.4.2.5 OUI version : 13.9.4.0.0 Log file location : /u01/oracle/cfgtoollogs/opatch/opatch2021-04-08_05-20-25AM_1.log OPatch detects the Middleware Home as \u0026quot;/u01/oracle\u0026quot; Verifying environment and performing prerequisite checks... OPatch continues with these patches: 32224021 Do you want to proceed? [y|n] Y (auto-answered by -silent) User Responded with: Y All checks passed. Please shutdown Oracle instances running out of this ORACLE_HOME on the local system. (Oracle Home = '/u01/oracle') Is the local system ready for patching? [y|n] Y (auto-answered by -silent) User Responded with: Y Backing up files... Applying interim patch '32224021' to OH '/u01/oracle' ApplySession: Optional component(s) [ oracle.webcenter.sca, 12.2.1.4.0 ] , [ oracle.webcenter.sca, 12.2.1.4.0 ] , [ oracle.webcenter.ucm, 12.2.1.4.0 ] , [ oracle.webcenter.ucm, 12.2.1.4.0 ] not present in the Oracle Home or a higher version is found. Patching component oracle.webcenter.portal, 12.2.1.4... Patching component oracle.webcenter.portal, 12.2.1.4... Patching component oracle.rcu.webcenter.portal, 12.2.1.0... Patching component oracle.rcu.webcenter.portal, 12.2.1.0... Patch 32224021 successfully applied. Log file location: /u01/oracle/cfgtoollogs/opatch/opatch2021-04-08_05-20-25AM_1.log OPatch succeeded. Oracle Interim Patch Installer version 13.9.4.2.5 Copyright (c) 2021, Oracle Corporation. All rights reserved. Oracle Home : /u01/oracle Central Inventory : /u01/oracle/.inventory from : /u01/oracle/oraInst.loc OPatch version : 13.9.4.2.5 OUI version : 13.9.4.0.0 Log file location : /u01/oracle/cfgtoollogs/opatch/opatch2021-04-08_05-27-11AM_1.log OPatch detects the Middleware Home as \u0026quot;/u01/oracle\u0026quot; Invoking utility \u0026quot;cleanup\u0026quot; OPatch will clean up 'restore.sh,make.txt' files and 'scratch,backup' directories. You will be still able to rollback patches after this cleanup. Do you want to proceed? [y|n] Y (auto-answered by -silent) User Responded with: Y Backup area for restore has been cleaned up. For a complete list of files/directories deleted, Please refer log file. OPatch succeeded. Removing intermediate container 323e7ae70339 ---\u0026gt; 0e7c514dcf7b Successfully built 0e7c514dcf7b Successfully tagged wcportal:12.2.1.4-32224021 [INFO ] Build successful. Build time=645s. Image tag=wcportal:12.2.1.4-32224021      Click here to see the example Dockerfile generated by the WebLogic Image Tool with the --dryRun option:    $ imagetool update --fromImage oracle/wcportal:12.2.1.4 --tag=wcportal:12.2.1.4-30761841 --patches=30761841_12.2.1.4.0 --dryRun [INFO ] Image Tool build ID: a473ba32-84b6-4374-9425-9e92ac90ee87 [INFO ] Temporary directory used for docker build context: /scratch/imagetoolcache/builddir/wlsimgbuilder_temp874401188519547557 [INFO ] Using patch 28186730_13.9.4.2.5 from cache: /home/imagetool-setup/jars/p28186730_139425_Generic.zip [INFO ] Updating OPatch in final image from version 13.9.4.2.1 to version 13.9.4.2.5 [WARNING] Skipping patch conflict check, no support credentials provided [WARNING] No credentials provided, skipping validation of patches [INFO ] Using patch 32224021_12.2.1.4 from cache: /home/imagetool-setup/jars/p32224021_122140_Generic.zip [INFO ] docker cmd = docker build --no-cache --force-rm --tag wcportal:12.2.1.4-32224021 --build-arg http_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg https_proxy=http://\u0026lt;YOUR-COMPANY-PROXY\u0026gt; --build-arg no_proxy=\u0026lt;IP addresses and Domain address for no_proxy\u0026gt;,/var/run/docker.sock \u0026lt;work-directory\u0026gt;/wlstmp/wlsimgbuilder_temp874401188519547557 ########## BEGIN DOCKERFILE ########## # # Copyright (c) 2019, 2021, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # FROM oracle/wcportal:12.2.1.4 as final_build USER root ENV OPATCH_NO_FUSER=true LABEL com.oracle.weblogic.imagetool.buildid=\u0026quot;a473ba32-84b6-4374-9425-9e92ac90ee87\u0026quot; USER oracle COPY --chown=oracle:oracle p28186730_139425_Generic.zip /tmp/imagetool/opatch/ RUN cd /tmp/imagetool/opatch \\ \u0026amp;\u0026amp; /u01/jdk/bin/jar -xf /tmp/imagetool/opatch/p28186730_139425_Generic.zip \\ \u0026amp;\u0026amp; /u01/jdk/bin/java -jar /tmp/imagetool/opatch/6880880/opatch_generic.jar -silent -ignoreSysPrereqs -force -novalidation oracle_home=/u01/oracle \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool COPY --chown=oracle:oracle patches/* /tmp/imagetool/patches/ # Apply all patches provided at the same time RUN /u01/oracle/OPatch/opatch napply -silent -oh /u01/oracle -phBaseDir /tmp/imagetool/patches \\ \u0026amp;\u0026amp; test $? -eq 0 \\ \u0026amp;\u0026amp; /u01/oracle/OPatch/opatch util cleanup -silent -oh /u01/oracle \\ || (cat /u01/oracle/cfgtoollogs/opatch/opatch*.log \u0026amp;\u0026amp; exit 1) ########## END DOCKERFILE ##########      Check the built image using the docker images command:\n$ docker images | grep wcportal wcportal 12.2.1.4-30761841 2ef2a67a685b About a minute ago 3.58GB $   Create an Oracle WebCenter Portal Docker image using Dockerfile For test and development purposes, you can create an Oracle WebCenter Portal image using the Dockerfile. Consult the README file for important prerequisite steps, such as building or pulling the Server JRE Docker image, Oracle Fusion Middleware Infrastructure Docker image and downloading the Oracle WebCenter Portal installer and bundle patch binaries.\nA prebuilt Oracle Fusion Middleware Infrastructure image, container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4, is available at container-registry.oracle.com. We recommend that you pull and rename this image to build the Oracle WebCenter Portal image.\n$ docker pull container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 $ docker tag container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 oracle/fmw-infrastructure:12.2.1.4 To build an Oracle Fusion Middleware Infrastructure image and on top of that the Oracle WebCenter Portal image as a layer, follow these steps:\n  Make a local clone of the sample repository:\n$ git clone https://github.com/oracle/docker-images   Download the Oracle WebCenter Portal installer from the Oracle Technology Network or e-delivery.\n Note: Copy the installer binaries to the same location as the Dockerfile.\n   Create the Oracle WebCenter Portal image by running the provided script:\n$ cd docker-images/OracleWebCenterPortal/dockerfiles $ ./buildDockerImage.sh -v 12.2.1.4 -s The image produced is named oracle/wcportal:12.2.1.4. The samples and instructions assume the Oracle WebCenter Portal image is named oracle/wcportal:12.2.1.4. You must rename your image to match this name, or update the samples to refer to the image you created.\n  "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/manage-wcportal-domains/configure-load-balancer/apachewebtier/",
	"title": "Apache webtier",
	"tags": [],
	"description": "Configure the Apache webtier load balancer for an Oracle WebCenter Portal domain.",
	"content": "To load balance Oracle WebCenter Portal domain clusters, you can install Apache webtier and configure it for non-SSL and SSL termination access of the application URL. Follow these steps to set up Apache webtier as a load balancer for an Oracle WebCenter Portal domain in a Kubernetes cluster:\n Build the Apache webtier image Create the Apache plugin configuration file Prepare the certificate and private key Install the Apache webtier Helm chart Verify domain application URL access Uninstall Apache webtier  Build the Apache webtier image To build the Apache webtier Docker image, refer to the sample.\nCreate the Apache plugin configuration file  The configuration file named custom_mod_wl_apache.conf should have all the URL routing rules for the Oracle WebCenter Portal applications deployed in the domain that needs to be accessible externally. Update this file with values based on your environment. The file content is similar to below.    Click here to see the sample content of the configuration file custom_mod_wl_apache.conf for wcp-domain domain   $ cat ${WORKDIR}/charts/apache-samples/custom-sample/custom_mod_wl_apache.conf #Copyright (c) 2018 Oracle and/or its affiliates. All rights reserved. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # \u0026lt;IfModule mod_weblogic.c\u0026gt; WebLogicHost \u0026lt;WEBLOGIC_HOST\u0026gt; WebLogicPort 7001 \u0026lt;/IfModule\u0026gt; # Directive for weblogic admin Console deployed on Weblogic Admin Server \u0026lt;Location /console\u0026gt; SetHandler weblogic-handler WebLogicHost wcp-domain-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /em\u0026gt; SetHandler weblogic-handler WebLogicHost wcp-domain-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /webcenter\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcp-cluster:8888 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /rsscrawl\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcp-cluster:8888 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /rest\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcp-cluster:8888 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /webcenterhelp\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcp-cluster:8888 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /wsrp-tools\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcportlet-cluster:8889 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /portalTools\u0026gt; WLSRequest On WebLogicCluster wcp-domain-cluster-wcportlet-cluster:8889 PathTrim /weblogic1 \u0026lt;/Location\u0026gt;     Update persistentVolumeClaimName in ${WORKDIR}/charts/apache-samples/custom-sample/input.yamlwith Persistence Volume which contains your own custom_mod_wl_apache.conf file. Use the PV/PVC created at the time of preparing environment, Copy the custom_mod_wl_apache.conf file to existing PersistantVolume.  Prepare the certificate and private key   (For the SSL termination configuration only) Run the following commands to generate your own certificate and private key using openssl.\n$ cd ${WORKDIR}/charts/apache-samples/custom-sample $ export VIRTUAL_HOST_NAME=WEBLOGIC_HOST $ export SSL_CERT_FILE=WEBLOGIC_HOST.crt $ export SSL_CERT_KEY_FILE=WEBLOGIC_HOST.key $ sh certgen.sh  NOTE: Replace WEBLOGIC_HOST with the name of the host on which Apache webtier is to be installed.\n   Click here to see the output of the certifcate generation   $ls certgen.sh custom_mod_wl_apache.conf custom_mod_wl_apache.conf_orig input.yaml README.md $ sh certgen.sh Generating certs for WEBLOGIC_HOST Generating a 2048 bit RSA private key ........................+++ .......................................................................+++ unable to write \u0026#39;random state\u0026#39; writing new private key to \u0026#39;apache-sample.key\u0026#39; ----- $ ls certgen.sh custom_mod_wl_apache.conf_orig WEBLOGIC_HOST.info config.txt input.yaml WEBLOGIC_HOST.key custom_mod_wl_apache.conf WEBLOGIC_HOST.crt README.md      Prepare input values for the Apache webtier Helm chart.\nRun the following commands to prepare the input value file for the Apache webtier Helm chart.\n$ base64 -i ${SSL_CERT_FILE} | tr -d \u0026#39;\\n\u0026#39; $ base64 -i ${SSL_CERT_KEY_FILE} | tr -d \u0026#39;\\n\u0026#39; $ touch input.yaml Update virtualHostName with the value of the WEBLOGIC_HOST in file ${WORKDIR}/charts/apache-samples/custom-sample/input.yaml\n  Click here to see the snapshot of the sample input.yaml file   $ cat apache-samples/custom-sample/input.yaml # Use this to provide your own Apache webtier configuration as needed; simply define this # path and put your own custom_mod_wl_apache.conf file under this path. persistentVolumeClaimName: wcp-domain-domain-pvc # The VirtualHostName of the Apache HTTP server. It is used to enable custom SSL configuration. virtualHostName: \u0026lt;WEBLOGIC_HOST\u0026gt;      Install the Apache webtier Helm chart   Install the Apache webtier Helm chart to the domain wcpns namespace with the specified input parameters:\n$ cd ${WORKDIR}/charts $ kubectl create namespace apache-webtier $ helm install apache-webtier --values apache-samples/custom-sample/input.yaml --namespace wcpns apache-webtier --set image=oracle/apache:12.2.1.3   Check the status of the Apache webtier:\n$ kubectl get all -n wcpns | grep apache Sample output of the status of the apache webtier:\npod/apache-webtier-apache-webtier-65f69dc6bc-zg5pj 1/1 Running 0 22h service/apache-webtier-apache-webtier NodePort 10.108.29.98 \u0026lt;none\u0026gt; 80:30305/TCP,4433:30443/TCP 22h deployment.apps/apache-webtier-apache-webtier 1/1 1 1 22h replicaset.apps/apache-webtier-apache-webtier-65f69dc6bc 1 1 1 22h   Verify domain application URL access Once the Apache webtier load balancer is up, verify that the domain applications are accessible through the load balancer port 30305/30443. The application URLs for domain of type wcp are:\n Note: Port 30305 is the LOADBALANCER-Non-SSLPORT and Port 30443 is LOADBALANCER-SSLPORT.\n Non-SSL configuration http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenter http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/webcenterhelp http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rest http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/rsscrawl SSL configuration https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenter https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/rsscrawl https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/webcenterhelp https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/rest Uninstall Apache webtier $ helm delete apache-webtier -n wcpns "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/appendix/additional-configuration/",
	"title": "Additional Configuration",
	"tags": [],
	"description": "Describes how to create connections to Oracle WebCenter Content Server to enable content integration within Oracle WebCenter Portal.",
	"content": "Creating a Connection to Oracle WebCenter Content Server To enable content integration within Oracle WebCenter Portal create a connection to Oracle WebCenter Content Server using JAX-WS. Follow the steps in the documentation link to create the connection.\n Note: If the Oracle WebCenter Content Server is configured with SSL, before creating the connection, the SSL certificate should be imported into any location under mount path of domain persistent volume to avoid loss of certificate due pod restart.\n Import SSL Certificate Import the certificate using below sample command, update the keystore location to a directory under mount path of the domain persistent volume :\n$ kubectl exec -it wcp-domain-adminserver -n wcpns /bin/bash $ cd $JAVA_HOME/bin $ ./keytool -importcert -alias collab_cert -file /filepath/sslcertificate/contentcert.crt -keystore /u01/oracle/user_projects/domains/wcp-domain/DemoTrust.jks Update the TrustStore To update the truststore location edit domain.yaml file, append -Djavax.net.ssl.trustStore to the spec.serverPod.env.JAVA_OPTIONS environment variable value. The truststore location used in -Djavax.net.ssl.trustStore option should be same as keystore location where the SSL certificate has been imported.\nserverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=true -Dweblogic.ssl.Enabled=true -Dweblogic.security.SSL.ignoreHostnameVerification=true -Djavax.net.ssl.trustStore=/u01/oracle/user_projects/domains/wcp-domain/DemoTrust.jks\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026#34; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wcp-domain-domains-pvc volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume Apply the domain.yaml file to restart the Oracle WebCenter Portal domain.\n$ kubectl apply -f domain.yaml "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/adminguide/configure-load-balancer/apache/",
	"title": "Apache webtier",
	"tags": [],
	"description": "Configure the Apache webtier load balancer for Oracle WebCenter Content domain.",
	"content": "This section provides information about how to install and configure Apache webtier to load balance Oracle WebCenter Content domain clusters. You can configure Apache webtier for non-SSL and SSL termination access of the application URL.\nFollow these steps to set up Apache webtier as a load balancer for an Oracle WebCenter Content domain in a Kubernetes cluster:\n Build the Apache webtier image Create the Apache plugin configuration file Prepare the certificate and private key Install the Apache webtier Helm chart Verify domain application URL access Uninstall Apache webtier  Build the Apache webtier image Refer to the sample, to build the Apache webtier Docker image.\nCreate the Apache plugin configuration file  The configuration file named custom_mod_wl_apache.conf should have all the URL routing rules for the Oracle WebCenter Content application deployed in the domain that needs to be accessible externally. Update this file with values based on your environment. The file content is similar to below mentioned sample.    Click here to see the sample content of the configuration file custom_mod_wl_apache.conf for Oracle WebCenter Content domain   # Copyright (c) 2018, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. \u0026lt;IfModule mod_weblogic.c\u0026gt; WebLogicHost \u0026lt;WEBLOGIC_HOST\u0026gt; WebLogicPort 7001 \u0026lt;/IfModule\u0026gt; # Directive for weblogic admin Console deployed on Weblogic Admin Server \u0026lt;Location /console\u0026gt; SetHandler weblogic-handler WebLogicHost wccinfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /em\u0026gt; SetHandler weblogic-handler WebLogicHost wccinfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; \u0026lt;Location /weblogic/ready\u0026gt; SetHandler weblogic-handler WebLogicHost wccinfra-adminserver WebLogicPort 7001 \u0026lt;/Location\u0026gt; # Directive for all application deployed on weblogic cluster with a prepath defined by LOCATION variable # For example, if the LOCAITON is set to \u0026#39;/weblogic\u0026#39;, all applications deployed on the cluster can be accessed via # http://myhost:myport/weblogic/application_end_url # where \u0026#39;myhost\u0026#39; is the IP of the machine that runs the Apache web tier, and # \u0026#39;myport\u0026#39; is the port that the Apache web tier is publicly exposed to. # Note that LOCATION cannot be set to \u0026#39;/\u0026#39; unless this is the only Location module configured. \u0026lt;Location /cs\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-ucm-cluster:16200 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /adfAuthentication\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-ucm-cluster:16200 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /ibr\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-ibr-cluster:16250 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /ibr/adfAuthentication\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-ibr-cluster:16250 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /imaging\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-ipm-cluster:16000 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /dc-console\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-capture-cluster:16400 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /dc-client\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-capture-cluster:16400 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; \u0026lt;Location /wcc\u0026gt; WLSRequest On WebLogicCluster wccinfra-cluster-wccadf-cluster:16225 PathTrim /weblogic1 \u0026lt;/Location\u0026gt; # Directive for all application deployed on weblogic cluster with a prepath defined by LOCATION2 variable # For example, if the LOCAITON2 is set to \u0026#39;/weblogic2\u0026#39;, all applications deployed on the cluster can be accessed via # http://myhost:myport/weblogic2/application_end_url # where \u0026#39;myhost\u0026#39; is the IP of the machine that runs the Apache web tier, and # \u0026#39;myport\u0026#39; is the port that the Apache webt ier is publicly exposed to. #\u0026lt;Location /weblogic2\u0026gt; #WLSRequest On #WebLogicCluster domain2-cluster-cluster-1:8021 #PathTrim /weblogic2 #\u0026lt;/Location\u0026gt;     Update persistentVolumeClaimName with your PV-claim-name which contains your custom_mod_wl_apache.conf in file kubernetes/samples/charts/apache-samples/custom-sample/input.yaml.  Prepare the certificate and private key   (For the SSL termination configuration only) Run the following commands to generate your own certificate and private key using openssl.\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ cd kubernetes/samples/charts/apache-samples/custom-sample $ export VIRTUAL_HOST_NAME=WEBLOGIC_HOST $ export SSL_CERT_FILE=WEBLOGIC_HOST.crt $ export SSL_CERT_KEY_FILE=WEBLOGIC_HOST.key $ sh certgen.sh  NOTE: Replace WEBLOGIC_HOST with the host name on which Apache webtier is to be installed.\n   Click here to see the output of the certifcate generation   $ls certgen.sh custom_mod_wl_apache.conf custom_mod_wl_apache.conf_orig input.yaml README.md $ sh certgen.sh Generating certs for WEBLOGIC_HOST Generating a 2048 bit RSA private key ........................+++ .......................................................................+++ unable to write \u0026#39;random state\u0026#39; writing new private key to \u0026#39;apache-sample.key\u0026#39; ----- $ ls certgen.sh custom_mod_wl_apache.conf_orig WEBLOGIC_HOST.info config.txt input.yaml WEBLOGIC_HOST.key custom_mod_wl_apache.conf WEBLOGIC_HOST.crt README.md      Prepare input values for the Apache webtier Helm chart.\nRun the following commands to prepare the input value file for the Apache webtier Helm chart.\n$ base64 -i ${SSL_CERT_FILE} | tr -d \u0026#39;\\n\u0026#39; $ base64 -i ${SSL_CERT_KEY_FILE} | tr -d \u0026#39;\\n\u0026#39; $ touch input.yaml Update virtualHostName with the value of the WEBLOGIC_HOST in file kubernetes/samples/charts/apache-samples/custom-sample/input.yaml\n  Click here to see the snapshot of the sample input.yaml file   $ cat apache-samples/custom-sample/input.yaml # Use this to provide your own Apache webtier configuration as needed; simply define this # path and put your own custom_mod_wl_apache.conf file under this path. persistentVolumeClaimName: \u0026lt;pv-claim-name\u0026gt; # The VirtualHostName of the Apache HTTP server. It is used to enable custom SSL configuration. virtualHostName: \u0026lt;WEBLOGIC_HOST\u0026gt;      Install the Apache webtier Helm chart   Install the Apache webtier Helm chart to the domain wccns namespace with the specified input parameters:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/charts $ kubectl create namespace apache-webtier $ helm install apache-webtier --values apache-samples/custom-sample/input.yaml --namespace wccns apache-webtier --set image=oracle/apache:12.2.1.3   Check the status of the Apache webtier:\n$ kubectl get all -n wccns | grep apache Sample output of the status of the apache webtier:\n  pod/apache-webtier-new-apache-webtier-65d8d7c59f-k27wf 1/1 Running 0 9d service/apache-webtier-new-apache-webtier NodePort 10.108.12.143 \u0026lt;none\u0026gt; 80:30505/TCP,4433:30453/TCP 9d deployment.apps/apache-webtier-new-apache-webtier 1/1 1 1 9d replicaset.apps/apache-webtier-new-apache-webtier-65d8d7c59f 1 1 1 9d Verify domain application URL access Post the Apache webtier load balancer is up, verify that the domain applications are accessible through the load balancer port 30505/30453. The application URLs for domain of type wcc are:\n Note: Port 30505 is the LOADBALANCER-Non-SSLPORT and Port 30453 is LOADBALANCER-SSLPORT.\n Non-SSL configuration http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/cs http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/ibr http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/imaging http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/dc-console http://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-Non-SSLPORT}/wcc SSL configuration https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/weblogic/ready https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/console https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/em https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/cs https://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-SSLPORT}/ibr https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/imaging https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/dc-console https://${LOADBALANCER_HOSTNAME}:${LOADBALANCER-SSLPORT}/wcc Uninstall Apache webtier $ helm delete apache-webtier -n wccns "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/installguide/configure-wcp-search/",
	"title": "Configure WebCenter Portal For Search",
	"tags": [],
	"description": "Set up search functionality in Oracle WebCenter Portal using Elasticsearch.",
	"content": " Introduction Set Up Persistent Volume and Persistent Volume Claim Create a Secret Headless Service LoadBalancer LoadBalancer Validation Elasticsearch Cluster Deployment Validation  Introduction Elasticsearch is a highly scalable search engine. It allows you to store, search, and analyze big volumes of data quickly and provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON document.\nSet Up Persistent Volume and Persistent Volume Claim Create a Kubernetes PV and PVC (Persistent Volume and Persistent Volume Claim) to store Elasticsearch data. To create PV and PVC, use the deployment YAML configuration file located at ${WORKDIR}/create-wcp-es-cluster/es-pvpvc.yaml.\napiVersion: v1 kind: PersistentVolume metadata: name: es-data-pv namespace: wcpns spec: storageClassName: es-data-pv-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain hostPath: path: \u0026#34;/scratch/esdata\u0026#34; --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: es-data-pvc namespace: wcpns spec: storageClassName: es-data-pv-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi To create PV \u0026amp; PVC run the below command:\n$ kubectl apply -f es-pvpvc.yaml Create a Secret To grant access to Oracle WebCenter Portal, create a Kubernetes secret using the deployment YAML configuration file located at ${WORKDIR}/create-wcp-es-cluster/es-secret.yaml\napiVersion: v1 kind: Secret metadata: name: es-secret namespace: wcpns data: # base64 encoded strings wls-admin: d2VibG9naWM= wls-admin-pwd: d2VsY29tZTE= search-admin: d2NjcmF3bGFkbWlu search-admin-pwd: d2VsY29tZTE= Where: wls-admin :Oracle WebCenter Admin UserName wls-admin-pwd :Oracle WebCenter Admin Password search-admin :ElasticSearch Username search-admin-pwd : ElasticSearch Password  To create Kubernetes Secret run the below command:\n$ kubectl apply -f es-secret.yaml Headless Service Each node in Elasticsearch cluster can communicate using a headless service. Create a headless service using the deployment YAML configuration file located at ${WORKDIR}/create-wcp-es-cluster/es-service.yaml to establish cluster communication.\napiVersion: v1 kind: Service metadata: name: es-svc namespace: wcpns labels: service: elasticsearch spec: # headless service clusterIP: None ports: - port: 9200 name: http - port: 9300 name: transport selector: service: elasticsearch To create Headless Service run below command:\n$ kubectl apply -f es-service.yaml LoadBalancer To access the Elasticsearch service outside of the Kubernetes cluster, create an external loadbalancer. Then access the Elasticsearch service by using the external IP of loadbalancer, create a loadbalancer using the deployment YAML configuration file located at ${WORKDIR}/create-wcp-es-cluster/es-loadbalancer.yaml.\napiVersion: v1 kind: Service metadata: name: es-loadbalancer namespace: wcpns labels: type: external spec: type: LoadBalancer selector: service: elasticsearch ports: - name: http port: 9200 targetPort: 9200 To create a loadbalancer run below command:\n$ kubectl apply -f es-loadbalancer.yaml LoadBalancer Validation Once the loadbalancer is successfully deployed, validate it by running the following command:\n$ kubectl get svc -n wcpns -l type=external Make a note of the external IP from the above command and use this below sample URL to access Elasticsearch cluster health : http://externalIP:9200/_cluster/health\nElasticsearch Cluster Using the Kubernetes StatefulSet controller create an Elasticsearch Cluster comprising of three node using the deployment YAML configuration file located at ${WORKDIR}/create-wcp-es-cluster/es-statefulset.yaml\napiVersion: apps/v1 kind: StatefulSet metadata: name: es-statefulset namespace: wcpns labels: service: elasticsearch spec: serviceName: es-svc replicas: 1 selector: matchLabels: service: elasticsearch template: metadata: labels: service: elasticsearch spec: initContainers: - name: increase-the-vm-max-map-count image: busybox command: - sysctl - -w - vm.max_map_count=262144 securityContext: privileged: true - name: increase-the-ulimit image: busybox command: - sh - -c - ulimit -n 65536 securityContext: privileged: true volumes: - name: es-node persistentVolumeClaim: claimName: es-data-pvc - name: wcp-domain persistentVolumeClaim: claimName: wcp-domain-domain-pvc containers: - name: es-container image: oracle/wcportal:12.2.1.4 imagePullPolicy: IfNotPresent command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/u01/oracle/container-scripts/configureOrStartElasticsearch.sh\u0026#34; ] readinessProbe: httpGet: path: / port: 9200 httpHeaders: - name: Authorization value: Basic d2NjcmF3bGFkbWluOndlbGNvbWUx initialDelaySeconds: 150 periodSeconds: 30 timeoutSeconds: 10 successThreshold: 1 failureThreshold: 10 lifecycle: preStop: exec: command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/u01/oracle/container-scripts/elasticsearchPreStopHandler.sh\u0026#34; ] ports: - containerPort: 9200 name: http - containerPort: 9300 name: tcp env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: UNICAST_HOST_LIST value: \u0026#34;es-svc\u0026#34; - name: ADMIN_USERNAME valueFrom: secretKeyRef: name: es-secret key: wls-admin - name: ADMIN_PASSWORD valueFrom: secretKeyRef: name: es-secret key: wls-admin-pwd - name: SEARCH_APP_USERNAME valueFrom: secretKeyRef: name: es-secret key: search-admin - name: SEARCH_APP_USER_PASSWORD valueFrom: secretKeyRef: name: es-secret key: search-admin-pwd - name: ADMIN_SERVER_CONTAINER_NAME value: wcp-domain-adminserver - name: ADMIN_PORT value: \u0026#34;7001\u0026#34; - name: ES_CLUSTER_NAME value: es-cluster - name: DOMAIN_NAME value: wcp-domain - name: CONFIGURE_ES_CONNECTION value: \u0026#34;true\u0026#34; - name: LOAD_BALANCER_IP value: \u0026#34;es-loadbalancer.wcpns.svc.cluster.local\u0026#34; volumeMounts: - name: es-node mountPath: /u01/esHome/esNode - name: wcp-domain mountPath: /u01/oracle/user_projects/domains  Note: The values used for ADMIN_PORT and Image name should be same as values passed to create-domain.sh job while creating domain.\n To create a es-statefulset run below command:\n$ kubectl apply -f es-statefulset.yaml  Note: After setting up Elasticsearch cluster restart all the instance of Oracle WebCenter Portal server.\n Deployment Validation Validate the deployment by running the following command:\n$ kubectl get pods -n wcpns -l service=elasticsearch "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/installguide/additional-steps-to-launch-native-binaries/",
	"title": "Launch Oracle Webcenter Content Native Applications in Containers",
	"tags": [],
	"description": "How to launch Oracle WebCenter Content native binaries from inside containerized environment.",
	"content": "This section provides the steps required to use product native binaries with user interfaces.\nIssue with Launching Headful User Interfaces for Oracle WebCenter Content Native Binaries Oracle WebCenter Content (UCM) provide a set of native binaries with headful UIs, which are located inside the persistent volume, as part of the domain. WebCenter Content container images are, by default, created with Oracle slim linux image, which doesn\u0026rsquo;t come with all the packages pre-installed to support headful applications with UIs to be launched. With current Oracle WebCenter Content container images, running native applications fails, being unable to launch UIs.\nThe following sections document the solution, by providing a set of instructions, enabling users to run UCM native applications with UIs.\nThese instructions are divided in two parts -\n Steps to update the existing container image Steps to launch native apps using VNC sessions  Steps to Update out-of-the-box Oracle WebCenter Content Container Image Using WebLogic Image Tool This section describes the method to update image with a OS package using WebLogic Image Tool. Please refer this for setting up the WebLogic Image Tool.\nAdditional Build Commands The installation of required OS packages in the image, can be done using yum command in additional build command option available in WebLogic Image Tool. Here is the sample additionalBuildCmds.txt file, to be used, to install required Linux packages (libXext.x86_64, libXrender.x86_64 and libXtst.x86_64).\n[final-build-commands] USER root RUN yum -y --downloaddir=/tmp/imagetool install libXext libXrender libXtst \\ \u0026amp;\u0026amp; yum -y --downloaddir=/tmp/imagetool clean all \\ \u0026amp;\u0026amp; rm -rf /var/cache/yum/* \\ \u0026amp;\u0026amp; rm -rf /tmp/imagetool USER oracle  Note: It is important to change the user to oracle, otherwise the user during the container execution will be root.\n Build arguments The arguments required for updating the image can be passed as file to the WebLogic Image Tool.\n'update' is the sub command to Image Tool for updating an existing docker image. '--fromImage' option provides the existing docker image that has to be updated. '--tag' option should be provided with the new tag for the updated image. '--additionalBuildCommands' option should be provided with the above created additional build commands file. '--chown oracle:root' option should be provided to update file permissions.  Below is a sample build argument (buildArgs) file, to be used for updating the image,\n update --fromImage \u0026lt;existing_WCContent_image_without_dependent_packages\u0026gt; --tag \u0026lt;name_of_updated_WCContent_image_to_be_built\u0026gt; --additionalBuildCommands ./additionalBuildCmds.txt --chown oracle:root Update Oracle WebCenter Content Container Image Now we can execute the WebLogic Image Tool to update the out-of-the-box image, using the build-argument file described above -\n$ imagetool @buildArgs WebLogic Image Tool provides multiple options for updating the image. For detailed information on the update options, please refer to this document.\nUpdating the image does not modify the \u0026lsquo;CMD\u0026rsquo; from the source image unless it is modified in the additional build commands.\n$ docker inspect -f '{{.Config.Cmd}}' \u0026lt;name_of_updated_Wccontent_image\u0026gt; [/u01/oracle/container-scripts/createDomainandStartAdmin.sh] Steps to launch Oracle WebCenter Content native applications using VNC sessions. Once updated image is successfully built and available on all required nodes, do the following: a. Update the domain.yaml file with updated image name and apply the domain.yaml file.\n$ kubectl apply -f domain.yaml b. After applying the modified domain.yaml, pods will get restarted and start running with updated image with required packages.\n$ kubectl get pods -n \u0026lt;namespace_being_used_for_wccontent_domain\u0026gt; c. Create VNC sessions on the master node to launch native apps. These are the steps to be followed using the VNC session.\nd. Run this command on each VNC session:\n$ xhost + \u0026lt;HOST-IP or HOST-NAME of the node, on which POD is deployed\u0026gt;  Note: The above command works for multi-node clusters (in which master node and worker nodes are deployed on different hosts and pods are distributed among worker nodes, running on different hosts). In case of single node clusters (where there is only master node and no worker nodes and all pods are deployed on the host, on which master node is running), one needs to use container/pod’s IP instead of the master-node’s HOST-IP itself.\n To obtain the container IP, follow the command mentioned in step g, from within that container\u0026rsquo;s shell.\n$ xhost + \u0026lt;IP of the container, from which binaries are to be run \u0026gt; e. Get into the pod\u0026rsquo;s (for example, wccinfra-ucm-server1) shell:\n$ kubectl exec -n wccns -it wccinfra-ucm-server1 -- /bin/bash f. Traverse to the binaries location:\n$ cd /u01/oracle/user_projects/domains/wccinfra/ucm/cs/bin g. Get the container IP:\n$ hostname -i h. Set DISPLAY variable within the container:\n$ export DISPLAY=\u0026lt;HOST-IP/HOST-NAME of the master node, where VNC session was created\u0026gt;:vnc-session display-id i. Launch any native UCM application, from within the container, like this:\n$ ./SystemProperties If the application has an UI, it will get launched now.\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/oracle-cloud/configure-wcc-for-idcs/",
	"title": "Configuring Oracle WebCenter Content for  Oracle Identity Cloud Service (IDCS)",
	"tags": [],
	"description": "Configuring Oracle WebCenter Content for  Oracle Identity Cloud Service (IDCS)",
	"content": "Contents  Introduction Updating SSL.hostnameVerifier Property Configuring IDCS Security Provider Configuring Oracle Identity Cloud Integrator Provider Setting Up Trust between IDCS and WebLogic Creating Admin User in IDCS Admin Console for WebCenter Content Managing Group Memberships, Roles, and Accounts Configuring WebCenter Content for User Logout  Introduction Configuring WebCenter Content for Oracle Identity Cloud Service (IDCS) on OKE. Configuration information is provided in the following sections:\n Updating SSL.hostnameVerifier Property Configuring IDCS Security Provider Configuring WebCenter Content for User Logout  Updating SSL.hostnameVerifier Property To update SSL.hostnameVerifier property, do the following: This is necessary for the IDCS provider to access IDCS.\n  Stop all the servers in the domain including Administration server and all Managed WebLogic servers.\n  Update the SSL.hostnameVerifier property:\nedit the file \u0026lt;DOMAIN_HOME\u0026gt;//bin/setDomainEnv.sh: go to pv location file system and modify the file setDomainEnv.sh sample: /WCCFS/wccinfra/bin/setDomainEnv.sh\nOR\nAlternatively create or modify the file \u0026lt;DOMAIN_HOME\u0026gt;/\u0026lt;domain_name\u0026gt;/bin/setUserOverrides.sh. Add the SSL.hostnameVerifier property for the IDCS Authenticator: sample: /WCCFS/wccinfra/bin/setUserOverrides.sh\nEXTRA_JAVA_PROPERTIES=\u0026#34;${EXTRA_JAVA_PROPERTIES}-Dweblogic.security.SSL.hostnameVerifier=weblogic.security.utils.SSLWLSWildcardHostnameVerifier\u0026#34; export EXTRA_JAVA_PROPERTIES   Start the Administration server and all Managed WebLogic servers.\n  Configuring IDCS Security Provider   Log in to the IDCS administration console.\n  Create a trusted application. In the Add Confidential Application wizard:\n Enter the client name and the description (optional). Select the Configure this application as a client now option. To configure this application, expand the Client Configuration in the Configuration tab. In the Allowed Grant Types , select Client Credentials field the check box. In the Grant the client access to Identity Cloud Service Admin APIs section, click Add to add the APP Roles (application roles). You can add the Identity Domain Administrator role. Keep the default settings for the pages and click Finish. Record/Copy the Client ID and Client Secret.This is needed when you will create the IDCS provider. Activate the application.    Configuring Oracle Identity Cloud Integrator Provider To configure Identity Cloud Integrator Provider:\n  Log in to the WebLogic Server Administration console.\n  Select Security Realm in the Domain Structure pane.\n  On the Summary of Security Realms page, select the name of the realm (for example, myrealm). Click myrealm. The Settings for myrealm page appears.\n  On the Settings for Realm Name page, select Providers and then Authentication. To create a new Authentication Provider, in the Authentication Providers table, click New.\n  In the Create a New Authentication Provider page, enter the name of the authentication provider, for example, IDCSIntegrator and select the OracleIdentityCloudIntegrator type of authentication provider from the drop-down list and click OK.\n  In the Authentication Providers table, click the newly created Oracle Identity Cloud Integrator, IDCSIntegrator link.\n  In the Settings for IDCSIntegrator page, for the Control Flag field, select the Sufficient option from the drop-down list Click Save.\n  Go to the Provider Specific page to configure the additional attributes for the security provider. Enter the values for the following fields \u0026amp; Click Save:\n Host Port 443(default) select SSLEnabled Tenant Client Id Client Secret.   NOTE: If IDCS URL is idcs-abcde.identity.example.com, then IDCS host would be identity.example.com and tenant name would be idcs-abcde. Keep the default settings for other sections of the page.\n   Select Security Realm, then myrealm, and then Providers. In the Authentication Providers table, click Reorder.\n  In the Reorder Authentication Providers page, move IDCSIntegrator on the top and click OK.\n  In the Authentication Providers table, click the DefaultAuthenticator link. In the Settings for DefaultAuthenticator page, for the Control Flag field, select the Sufficient option from the drop-down list. Click Save.\n  All changes will be activated. Restart the Administration server.\n  Setting Up Trust between IDCS and WebLogic To set up trust between IDCS and WebLogic\n Import certificate in KSS store.  Run this from the Administration Server node. Get IDCS certificate: echo -n | openssl s_client -showcerts -servername \u0026lt;IDCS_URL\u0026gt; -connect \u0026lt;IDCS_URL\u0026gt;:443|sed -ne \u0026#39;/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p\u0026#39; \u0026gt; /tmp/idcs_cert_chain.crt #sample echo -n | openssl s_client -showcerts -servername xyz.identity.oraclecloud.com -connect idcs-xyz.identity.oraclecloud.com:443|sed -ne \u0026#39;/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p\u0026#39; \u0026gt; /tmp/idcs_cert_chain.crt #copy the certificate inside the admin_pod kubectl cp /tmp/idcs_cert_chain.crt wccns/xyz-adminserver:/u01/idcs_cert_chain.crt  Import certificate. Run \u0026lt;ORACLE_HOME\u0026gt;/oracle_common/common/bin/wlst.sh file. connect(\u0026#39;weblogic\u0026#39;,\u0026#39;Welcome_1\u0026#39;,\u0026#39;t3://\u0026lt;WEBLOGIC_HOST\u0026gt;:7001\u0026#39;) svc=getOpssService(name=\u0026#39;KeyStoreService\u0026#39;) svc.importKeyStoreCertificate(appStripe=\u0026#39;system\u0026#39;,name=\u0026#39;trust\u0026#39;,password=\u0026#39;\u0026#39;,alias=\u0026#39;idcs_cert_chain\u0026#39;,type=\u0026#39;TrustedCertificate\u0026#39;,filepath=\u0026#39;/tmp/idcs_cert_chain.crt\u0026#39;,keypassword=\u0026#39;\u0026#39;) syncKeyStores(appStripe=\u0026#39;system\u0026#39;,keystoreFormat=\u0026#39;KSS\u0026#39;) #sample $./wlst.sh wls:/offline\u0026gt; connect(\u0026#39;weblogic\u0026#39;,\u0026#39;welcome\u0026#39;,\u0026#39;t3://xyz-adminserver:7001\u0026#39;) wls:/wccinfra/serverConfig/\u0026gt; svc=getOpssService(name=\u0026#39;KeyStoreService\u0026#39;) wls:/wccinfra/serverConfig/\u0026gt;svc.importKeyStoreCertificate(appStripe=\u0026#39;system\u0026#39;,name=\u0026#39;trust\u0026#39;,password=\u0026#39;\u0026#39;,alias=\u0026#39;idcs_cert_chain\u0026#39;,type=\u0026#39;TrustedCertificate\u0026#39;,filepath=\u0026#39;/u01/idcs_cert_chain.crt\u0026#39;,keypassword=\u0026#39;\u0026#39;) wls:/wccinfra/domainRuntime/\u0026gt;syncKeyStores(appStripe=\u0026#39;system\u0026#39;,keystoreFormat=\u0026#39;KSS\u0026#39;)  exit()   Restart the Administration server and Managed servers  Creating Admin User in IDCS Administration Console for WebCenter Content It is important to create the Admin user in IDCS because once the Managed servers are configured for SAML, the domain admin user (typically weblogic user) will not be able to log into the Managed servers.\nTo create WebLogic Admin user in IDCS for WebCenter Content JaxWS connection:\n Go to the Groups tab and create Administrators and sysmanager roles in IDCS. Go to the Users tab and create a wls admin user, for example, weblogic and assign it to Administrators and sysmanager groups. Restart all the Managed servers.  Managing Group Memberships, Roles, and Accounts This will require modifying OPSS and libOVD to access IDCS. The following steps are required if using IDCS for user authorization. Do not run these steps if you are using IDCS only for user authentication. Ensure that all the servers are stopped (including Administration) before proceeding with the following steps:\n NOTE: Shutdown all the servers using WebLogic Server Administration Console. Please keep in mind - kubectl patch domain command is the recommended way for starting/stopping pods. Please refrain from using WebLogic Server Administration Console for the same, anywhere else.\n   Run the following script:\n#exec the Administration server kubectl exec -n wccns -it wccinfra-adminserver -- /bin/bash #Run the wlst.sh cd /u01/oracle/oracle_common/common/bin/ ./wlst.sh  NOTE: It\u0026rsquo;s not required to connect to WebLogic Administration Server.\n   Read the domain:\nreadDomain(\u0026lt;DOMAIN_HOME\u0026gt;) #sample wls:/offline\u0026gt; readDomain(\u0026#39;/u01/oracle/user_projects/domains/wccinfra\u0026#39;)   Add the template:\naddTemplate(\u0026lt;MIDDLEWARE_HOME\u0026gt;/oracle_common/common/templates/wls/oracle.opss_scim_template.jar\u0026#34;) #sample wls:/offline/wccinfra\u0026gt;addTemplate(\u0026#39;/u01/oracle/oracle_common/common/templates/wls/oracle.opss_scim_template.jar\u0026#39;)  NOTE: This step may throw a warning, which can be ignored. The addTemplate is deprecated. Use selectTemplate followed by loadTemplates in place of addTemplate.\n   Update the domain:\nupdateDomain() #sample wls:/offline/wccinfra\u0026gt; updateDomain()   Close the domain:\ncloseDomain() #sample wls:/offline/wccinfra\u0026gt; closeDomain()   Exit from the Administration server container:\nexit   Start the servers (Administration and Managed).\n  Configuring WebCenter Content for User Logout If the Logout link is selected, you will be re-authenticated by SAML. To be able to select the Logout link:\n Log in to WebCenter Content Server as an administrator. Select Administration, then Admin Server, and then General Configuration. In the Additional Configuration Variables pane, add the following parameter: EXTRA_JAVA_PROPERTIES=\u0026#34;${EXTRA_JAVA_PROPERTIES}-Dweblogic.security.SSL.hostnameVerifier=weblogic.security.utils.SSLWLSWildcardHostnameVerifier\u0026#34;  Click Save. Restart the Administration and Managed servers.  "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/create-or-update-image/",
	"title": "Create or update an image",
	"tags": [],
	"description": "Create or update an Oracle WebCenter Content Docker image used for deploying Oracle WebCenter Content domains. An Oracle WebCenter Content Docker image can be created using the WebLogic Image Tool or using the Dockerfile approach.",
	"content": "If you have access to the My Oracle Support (MOS), and there is a need to build a new image with a patch (bundle or interim), it is recommended to use the WebLogic Image Tool to build an Oracle WebCenter Content image for production deployments.\n Create or update an Oracle WebCenter Content Docker image using the WebLogic Image Tool  Set up the WebLogic Image Tool Create an image Update an image   Create an Oracle WebCenter Content Docker image using Dockerfile  Create or update an Oracle WebCenter Content Docker image using the WebLogic Image Tool Using the WebLogic Image Tool, you can create a new Oracle WebCenter Content Docker image (can include patches as well) or update an existing image with one or more patches (bundle patch and interim patches).\n Recommendations:\n Use create for creating a new Oracle WebCenter Content Docker image either:  without any patches or, containing the Oracle WebCenter Content binaries, bundle patch and interim patches. This is the recommended approach if you have access to the Oracle WebCenter Content patches because it optimizes the size of the image.   Use update for patching an existing Oracle WebCenter Content Docker image with a single interim patch. Note that the patched image size may increase considerably due to additional image layers introduced by the patch application tool.   Set up the WebLogic Image Tool  Prerequisites Set up the WebLogic Image Tool Validate setup WebLogic Image Tool build directory WebLogic Image Tool cache Set up additional build scripts  Prerequisites Verify that your environment meets the following prerequisites:\n Docker client and daemon on the build machine, with minimum Docker version 18.03.1.ce. Bash version 4.0 or later, to enable the command complete feature. JAVA_HOME environment variable set to the appropriate JDK location.  Set up the WebLogic Image Tool To set up the WebLogic Image Tool:\n  Create a working directory and change to it. In these steps, this directory is imagetool-setup.\n$ mkdir imagetool-setup $ cd imagetool-setup   Download the latest version of the WebLogic Image Tool from the releases page.\n  Unzip the release ZIP file to the imagetool-setup directory.\n  Execute the following commands to set up the WebLogic Image Tool on a Linux environment:\n$ cd imagetool-setup/imagetool/bin $ source setup.sh   Validate setup To validate the setup of the WebLogic Image Tool:\n  Enter the following command to retrieve the version of the WebLogic Image Tool:\n$ imagetool --version   Enter imagetool then press the Tab key to display the available imagetool commands:\n$ imagetool \u0026lt;TAB\u0026gt; cache create help rebase update   WebLogic Image Tool build directory The WebLogic Image Tool creates a temporary Docker context directory, prefixed by wlsimgbuilder_temp, every time the tool runs. Under normal circumstances, this context directory will be deleted. However, if the process is aborted or the tool is unable to remove the directory, it is safe for you to delete it manually. By default, the WebLogic Image Tool creates the Docker context directory under the user\u0026rsquo;s home directory. If you prefer to use a different directory for the temporary context, set the environment variable WLSIMG_BLDDIR:\n$ export WLSIMG_BLDDIR=\u0026#34;/path/to/buid/dir\u0026#34; WebLogic Image Tool cache The WebLogic Image Tool maintains a local file cache store. This store is used to look up where the Java, WebLogic Server installers, and WebLogic Server patches reside in the local file system. By default, the cache store is located in the user\u0026rsquo;s $HOME/cache directory. Under this directory, the lookup information is stored in the .metadata file. All automatically downloaded patches also reside in this directory. You can change the default cache store location by setting the environment variable WLSIMG_CACHEDIR:\n$ export WLSIMG_CACHEDIR=\u0026#34;/path/to/cachedir\u0026#34; Set up additional build scripts Creating an Oracle WebCenter Content Docker image using the WebLogic Image Tool requires additional container scripts for Oracle WebCenter Content domains.\n  Clone the docker-images repository to set up those scripts. In these steps, this directory is DOCKER_REPO:\n$ cd imagetool-setup $ git clone https://github.com/oracle/docker-images.git   Copy the additional WebLogic Image Tool build files from the WebLogic Kubernetes Operator source repository to the imagetool-setup location:\n$ mkdir -p imagetool-setup/docker-images/WebCenterContent/imagetool/12.2.1.4.0 $ cd imagetool-setup/docker-images/WebCenterContent/imagetool/12.2.1.4.0 $ cp -rf ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/imagetool-scripts/* .   Create an image After setting up the WebLogic Image Tool and required build scripts, follow these steps to use the WebLogic Image Tool to create a new Oracle WebCenter Content Docker image.\nDownload the Oracle WebCenter Content installation binaries and patches You must download the required Oracle WebCenter Content installation binaries and patches as listed below from the Oracle Software Delivery Cloud and save them in a directory of your choice. In these steps, this directory is download location.\n  Click here to see the sample list of installation binaries and patches:     JDK:\n jdk-8u251-linux-x64.tar.gz    Fusion MiddleWare Infrastructure installer:\n fmw_12.2.1.4.0_infrastructure_generic.jar    WebCenter Content installers:\n fmw_12.2.1.4.0_wccontent.jar    Fusion MiddleWare Infrastructure patches:\n p28186730_139424_Generic-23574493.zip (Opatch)    WebCenter Content patches:\n p31390302_122140_Generic.zip (wcc)       Note: This is a sample list of patches. You must get the appropriate list of patches for your Oracle WebCenter Content image.\n Update required build files The following files available in the code repository location \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterContent/imagetool/12.2.1.4.0 are used for creating the image.\n additionalBuildCmds.txt buildArgs    In the buildArgs file, update all the occurrences of %DOCKER_REPO% with the docker-images repository location, which is the complete path of imagetool-setup/docker-images.\nFor example, update:\n%DOCKER_REPO%/OracleWebCenterContent/imagetool/12.2.1.4.0/\nto:\n\u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterContent/imagetool/12.2.1.4.0/\n  Similarly, update the placeholders %JDK_VERSION% and %BUILDTAG% with appropriate values.\n  Create the image   Add a JDK package to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type jdk --version 8u251 --path \u0026lt;download location\u0026gt;/jdk-8u251-linux-x64.tar.gz   Add the downloaded installation binaries to the WebLogic Image Tool cache:\n$ imagetool cache addInstaller --type fmw --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_infrastructure.jar $ imagetool cache addInstaller --type wcc --version 12.2.1.4.0 --path \u0026lt;download location\u0026gt;/fmw_12.2.1.4.0_wccontent.jar   Add the downloaded patches to the WebLogic Image Tool cache:\n  Click here to see the commands to add patches in to the cache:   ``` bash $ imagetool cache addEntry --key p33578966_122140_Generic --path \u0026lt;download location\u0026gt;/p33578966_122140_Generic.zip $ imagetool cache addEntry --key 28186730_13.9.4.2.8 --path \u0026lt;download location\u0026gt;/p28186730_139428_Generic-24497645.zip ```      Update the patches list to buildArgs.\nTo the create command in the buildArgs file, append the Oracle WebCenter Content patches list using the --patches flag and Opatch patch using the --opatchBugNumber flag. Sample options for the list of patches above are:\n--patches 33578966_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8 Example buildArgs file after appending product\u0026rsquo;s list of patches and Opatch patch:\ncreate --jdkVersion=8u251 --type WCC --version=12.2.1.4.0 --tag=oracle/wccontent_create_1015:12.2.1.4.0 --pull --chown oracle:root --additionalBuildCommands \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterContent/imagetool/12.2.1.4.0/additionalBuildCmds.txt --additionalBuildFiles \u0026lt;imagetool-setup-location\u0026gt;/docker-images/OracleWebCenterContent/dockerfiles/12.2.1.4.0/container-scripts --patches 33578966_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8 Refer to this page for the complete list of options available with the WebLogic Image Tool create command.\n  Enter the following command to create the Oracle WebCenter Content image:\n$ imagetool @\u0026lt;absolute path to `buildargs` file\u0026gt;\u0026#34;     Click here to see the sample Dockerfile generated with the imagetool command.   ########## BEGIN DOCKERFILE ########## # # Copyright (c) 2019, 2021, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # FROM ghcr.io/oracle/oraclelinux:7-slim as os_update LABEL com.oracle.weblogic.imagetool.buildid=\u0026#34;f46ab190-077e-4ed7-b747-7bb170fe592c\u0026#34; USER root RUN yum -y --downloaddir=/tmp/imagetool install gzip tar unzip libaio jq hostname \\  \u0026amp;\u0026amp; yum -y --downloaddir=/tmp/imagetool clean all \\  \u0026amp;\u0026amp; rm -rf /var/cache/yum/* \\  \u0026amp;\u0026amp; rm -rf /tmp/imagetool ## Create user and group RUN if [ -z \u0026#34;$(getent group root)\u0026#34; ]; then hash groupadd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; groupadd root || exit -1 ; fi \\  \u0026amp;\u0026amp; if [ -z \u0026#34;$(getent passwd oracle)\u0026#34; ]; then hash useradd \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; useradd -g root oracle || exit -1; fi \\  \u0026amp;\u0026amp; mkdir -p /u01 \\  \u0026amp;\u0026amp; chown oracle:root /u01 \\  \u0026amp;\u0026amp; chmod 775 /u01 # Install Java FROM os_update as jdk_build LABEL com.oracle.weblogic.imagetool.buildid=\u0026#34;f46ab190-077e-4ed7-b747-7bb170fe592c\u0026#34; ENV JAVA_HOME=/u01/jdk COPY --chown=oracle:root jdk-8u251-linux-x64.tar.gz /tmp/imagetool/ USER oracle RUN tar xzf /tmp/imagetool/jdk-8u251-linux-x64.tar.gz -C /u01 \\  \u0026amp;\u0026amp; $(test -d /u01/jdk* \u0026amp;\u0026amp; mv /u01/jdk* /u01/jdk || mv /u01/graal* /u01/jdk) \\  \u0026amp;\u0026amp; rm -rf /tmp/imagetool \\  \u0026amp;\u0026amp; rm -f /u01/jdk/javafx-src.zip /u01/jdk/src.zip # Install Middleware FROM os_update as wls_build LABEL com.oracle.weblogic.imagetool.buildid=\u0026#34;f46ab190-077e-4ed7-b747-7bb170fe592c\u0026#34; ENV JAVA_HOME=/u01/jdk \\  ORACLE_HOME=/u01/oracle \\  OPATCH_NO_FUSER=true RUN mkdir -p /u01/oracle \\  \u0026amp;\u0026amp; mkdir -p /u01/oracle/oraInventory \\  \u0026amp;\u0026amp; chown oracle:root /u01/oracle/oraInventory \\  \u0026amp;\u0026amp; chown oracle:root /u01/oracle COPY --from=jdk_build --chown=oracle:root /u01/jdk /u01/jdk/ COPY --chown=oracle:root fmw_12.2.1.4.0_infrastructure_generic.jar fmw.rsp /tmp/imagetool/ COPY --chown=oracle:root fmw_12.2.1.4.0_wccontent.jar wcc.rsp /tmp/imagetool/ COPY --chown=oracle:root oraInst.loc /u01/oracle/ USER oracle RUN echo \u0026#34;INSTALLING MIDDLEWARE\u0026#34; \\  \u0026amp;\u0026amp; echo \u0026#34;INSTALLING fmw\u0026#34; \\  \u0026amp;\u0026amp; \\  /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_infrastructure_generic.jar -silent ORACLE_HOME=/u01/oracle \\  -responseFile /tmp/imagetool/fmw.rsp -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\  \u0026amp;\u0026amp; echo \u0026#34;INSTALLING wcc\u0026#34; \\  \u0026amp;\u0026amp; \\  /u01/jdk/bin/java -Xmx1024m -jar /tmp/imagetool/fmw_12.2.1.4.0_wccontent.jar -silent ORACLE_HOME=/u01/oracle \\  -responseFile /tmp/imagetool/wcc.rsp -invPtrLoc /u01/oracle/oraInst.loc -ignoreSysPrereqs -force -novalidation \\  \u0026amp;\u0026amp; chmod -R g+r /u01/oracle FROM os_update as final_build ARG ADMIN_NAME ARG ADMIN_HOST ARG ADMIN_PORT ARG MANAGED_SERVER_PORT ENV ORACLE_HOME=/u01/oracle \\  JAVA_HOME=/u01/jdk \\  PATH=${PATH}:/u01/jdk/bin:/u01/oracle/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle LABEL com.oracle.weblogic.imagetool.buildid=\u0026#34;f46ab190-077e-4ed7-b747-7bb170fe592c\u0026#34; COPY --from=jdk_build --chown=oracle:root /u01/jdk /u01/jdk/ COPY --from=wls_build --chown=oracle:root /u01/oracle /u01/oracle/ USER oracle WORKDIR /u01/oracle #ENTRYPOINT /bin/bash ENV ORACLE_HOME=/u01/oracle \\  VOLUME_DIR=/u01/oracle/user_projects \\  SCRIPT_FILE=/u01/oracle/container-scripts/* \\  USER_MEM_ARGS=\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34; \\  PATH=$PATH:$JAVA_HOME/bin:$ORACLE_HOME/oracle_common/common/bin:/u01/oracle/wlserver/common/bin:/u01/oracle/container-scripts USER root RUN mkdir -p $VOLUME_DIR \u0026amp;\u0026amp; \\  mkdir -p /u01/oracle/container-scripts \u0026amp;\u0026amp; \\  mkdir -p /u01/oracle/silent-install-files-tmp/config \u0026amp;\u0026amp; \\  mkdir -p /u01/oracle/logs \u0026amp;\u0026amp; \\  chown oracle:root -R /u01 $VOLUME_DIR \u0026amp;\u0026amp; \\  chmod a+xr /u01 COPY --chown=oracle:root files/container-scripts/ /u01/oracle/container-scripts/ RUN chmod +xr $SCRIPT_FILE USER oracle EXPOSE $UCM_PORT $UCM_INTRADOC_PORT $IBR_INTRADOC_PORT $IBR_PORT $ADMIN_PORT WORKDIR ${ORACLE_HOME} CMD [\u0026#34;/u01/oracle/container-scripts/createDomainandStartAdmin.sh\u0026#34;] ########## END DOCKERFILE ##########      Check the created image using the docker images command:\n$ docker images | grep wcc   Update an image After setting up the WebLogic Image Tool and required build scripts, use the WebLogic Image Tool to update an existing Oracle WebCenter Content Docker image:\n  Enter the following command for each patch to add the required patch(es) to the WebLogic Image Tool cache:\n$ cd \u0026lt;imagetool-setup\u0026gt; $ imagetool cache addEntry --key=33578966_12.2.1.4.0 --value \u0026lt;downloaded-patches-location\u0026gt;/p33578966_122140_Generic.zip [INFO ] Added entry 33578966_12.2.1.4.0=\u0026lt;downloaded-patches-location\u0026gt;/p33578966_122140_Generic.zip   Provide the following arguments to the WebLogic Image Tool update command:\n –-fromImage - Identify the image that needs to be updated. In the example below, the image to be updated is wccontent:12.2.1.4.0. –-patches - Multiple patches can be specified as a comma-separated list. --tag - Specify the new tag to be applied for the image being built.  Refer here for the complete list of options available with the WebLogic Image Tool update command.\n Note: The WebLogic Image Tool cache should have the latest OPatch zip. The WebLogic Image Tool will update the OPatch if it is not already updated in the image.\n Examples     Click here to see the example `update` command:    # If you are using a pre-built Oracle WebCenter Content image, obtained from My Oracle Support, then please use this command: $ imagetool update --fromImage oracle/wccontent:12.2.1.4.0 --tag=oracle/wccontent_update_1015:12.2.1.4.0 --patches=33578966_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8 # In case, you chose to build an Oracle WebCenter Content image, please use the command given below: $ imagetool update --chown oracle:root --fromImage oracle/wccontent:12.2.1.4.0 --tag=oracle/wccontent_update_1015:12.2.1.4.0 --patches=33578966_12.2.1.4.0 --opatchBugNumber=28186730_13.9.4.2.8     Check the built image using the docker images command: $ docker images | grep wcc   Create an Oracle WebCenter Content Docker image using Dockerfile For test and development purposes, you can create an Oracle WebCenter Content image using the Dockerfile. Consult the README file for important prerequisite steps, such as building or pulling the Server JRE Docker image, Oracle FMW Infrastructure Docker image, and downloading the Oracle WebCenter Content installer and bundle patch binaries.\nA prebuilt Oracle Fusion Middleware Infrastructure image, container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4-210407, is available at container-registry.oracle.com. We recommend that you pull and rename this image to build the Oracle WebCenter Content image.\n$ docker pull container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4-210407 $ docker tag container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4-210407 oracle/fmw-infrastructure:12.2.1.4.0 Follow these steps to build an Oracle WebCenter Content image :\n  Make a local clone of the sample repository:\n$ git clone https://github.com/oracle/docker-images   Download the Oracle WebCenter Content installer from the Oracle Technology Network or e-delivery.\n Note: Copy the installer binaries to the same location as the Dockerfile.\n   Create the Oracle WebCenter Content image by running the provided script:\n$ cd docker-images/OracleWebCenterContent/dockerfiles $ ./buildDockerImage.sh -v 12.2.1.4 -s The image produced will be named oracle/wccontent:12.2.1.4. The samples and instructions assume the Oracle WebCenter Content image is named wccontent:12.2.1.4.0. You must rename your image to match this name, or update the samples to refer to the image you created.\n$ docker tag oracle/wccontent:12.2.1.4 wccontent:12.2.1.4.0   "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/adminguide/logging-fluentd-setup/",
	"title": "Publish logs to Elasticsearch Using Fluentd",
	"tags": [],
	"description": "Configure a WebLogic domain to use Fluentd to send log information to Elasticsearch.",
	"content": "Introduction This page describes to how to configure a WebLogic domain to use Fluentd to send log information to Elasticsearch. Here’s the general mechanism for how this works:\n fluentd runs as a separate container in the Administration Server and Managed Server pods The log files reside on a volume that is shared between the weblogic-server and fluentd containers fluentd tails the domain logs files and exports them to Elasticsearch A ConfigMap contains the filter and format rules for exporting log records.  Create fluentd configuration Create a ConfigMap named fluentd-config in the namespace of the domain. The ConfigMap contains the parsing rules and Elasticsearch configuration. Here’s an explanation of some elements defined in the ConfigMap:\n The @type tail indicates that tail will be used to obtain updates to the log file The path of the log file is obtained from the LOG_PATH environment variable that is defined in the fluentd container The tag value of log records is obtained from the DOMAIN_UID environment variable that is defined in the fluentd container The parse section defines how to interpret and tag each element of a log record The match section contains the configuration information for connecting to Elasticsearch and defines the index name of each record to be the domainUID  Here is a sample configmap for fluentd configuration,\n  Click here to see sample configmap for fluentd configuration `fluentd_configmap.yaml`.   apiVersion: v1 kind: ConfigMap metadata: labels: weblogic.domainUID: wccinfra weblogic.resourceVersion: domain-v2 name: fluentd-config namespace: wccns data: fluentd.conf: | \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; \u0026lt;source\u0026gt; @type tail path \u0026quot;#{ENV['LOG_PATH']}\u0026quot; pos_file /tmp/server.log.pos read_from_head true tag \u0026quot;#{ENV['DOMAIN_UID']}\u0026quot; # multiline_flush_interval 20s \u0026lt;parse\u0026gt; @type multiline format_firstline /^####/ format1 /^####\u0026lt;(?\u0026lt;timestamp\u0026gt;(.*?))\u0026gt;/ format2 / \u0026lt;(?\u0026lt;level\u0026gt;(.*?))\u0026gt;/ format3 / \u0026lt;(?\u0026lt;subSystem\u0026gt;(.*?))\u0026gt;/ format4 / \u0026lt;(?\u0026lt;serverName\u0026gt;(.*?))\u0026gt;/ format5 / \u0026lt;(?\u0026lt;serverName2\u0026gt;(.*?))\u0026gt;/ format6 / \u0026lt;(?\u0026lt;threadName\u0026gt;(.*?))\u0026gt;/ format7 / \u0026lt;(?\u0026lt;info1\u0026gt;(.*?))\u0026gt;/ format8 / \u0026lt;(?\u0026lt;info2\u0026gt;(.*?))\u0026gt;/ format9 / \u0026lt;(?\u0026lt;info3\u0026gt;(.*?))\u0026gt;/ format10 / \u0026lt;(?\u0026lt;sequenceNumber\u0026gt;(.*?))\u0026gt;/ format11 / \u0026lt;(?\u0026lt;severity\u0026gt;(.*?))\u0026gt;/ format12 / \u0026lt;(?\u0026lt;messageID\u0026gt;(.*?))\u0026gt;/ format13 / \u0026lt;(?\u0026lt;message\u0026gt;(.*?))\u0026gt;/ \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;match **\u0026gt; @type elasticsearch host \u0026quot;#{ENV['ELASTICSEARCH_HOST']}\u0026quot; port \u0026quot;#{ENV['ELASTICSEARCH_PORT']}\u0026quot; user \u0026quot;#{ENV['ELASTICSEARCH_USER']}\u0026quot; password \u0026quot;#{ENV['ELASTICSEARCH_PASSWORD']}\u0026quot; index_name \u0026quot;#{ENV['DOMAIN_UID']}\u0026quot; \u0026lt;/match\u0026gt;    Create the ConfigMap using the following command\n$kubectl create -f fluentd_configmap.yaml Mount fluentd configuration - Configmap as volume in the WebLogic container. Edit the domain definition and configure a volume for the ConfigMap containing the fluentd configuration.\n$kubectl edit domain -n wccns Below sample yaml code add Configmap as volume,\n volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wccinfra-domain-pvc - configMap: defaultMode: 420 name: fluentd-config name: fluentd-config-volume Add fluentd container to WebLogic Server pods Add a \u0026ldquo;fluentd container yaml\u0026rdquo; to the domain under serverPod: section that will run fluentd in the Administration Server and Managed Server pods.\nNotice the container definition:\n Defines a LOG_PATH environment variable that points to the log location of WebLogic servers. Defines ELASTICSEARCH_HOST, ELASTICSEARCH_PORT, ELASTICSEARCH_USER, and ELASTICSEARCH_PASSWORD environment variables. Has volume mounts for the fluentd-config ConfigMap and the volume containing the domain logs.  $kubectl edit domain -n wccns    Click here to see sample fluentd container yaml `fluentd container`.   containers: - args: - -c - /etc/fluent.conf env: - name: DOMAIN_UID valueFrom: fieldRef: fieldPath: metadata.labels['weblogic.domainUID'] - name: SERVER_NAME valueFrom: fieldRef: fieldPath: metadata.labels['weblogic.serverName'] - name: LOG_PATH value: /u01/oracle/user_projects/domains/logs/wccinfra/$(SERVER_NAME).log - name: FLUENTD_CONF value: fluentd.conf - name: FLUENT_ELASTICSEARCH_SED_DISABLE value: \u0026quot;true\u0026quot; - name: ELASTICSEARCH_HOST value: elasticsearch.default.svc.cluster.local - name: ELASTICSEARCH_PORT value: \u0026quot;9200\u0026quot; - name: ELASTICSEARCH_USER value: elastic - name: ELASTICSEARCH_PASSWORD value: changeme image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 imagePullPolicy: IfNotPresent name: fluentd resources: {} volumeMounts: - mountPath: /fluentd/etc/fluentd.conf name: fluentd-config-volume subPath: fluentd.conf - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume    Restart WebLogic Servers To restart the servers, edit the domain and change serverStartPolicy to NEVER for the WebLogic servers to shutdown\n$kubectl edit domain -n wccns After all the servers are shutdown edit domain again and set serverStartPolicy to IF_NEEDED for the servers to start again.\nCreate index pattern in Kibana Create an index pattern \u0026ldquo;wccinfra*\u0026rdquo; in Kibana \u0026gt; Management. After the server starts, you will be able to see the log data in the Kibana dashboard,\n"
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/cleanup-domain-setup/",
	"title": "Uninstall an Oracle WebCenter Portal domain",
	"tags": [],
	"description": "Clean up the Oracle WebCenter Portal domain setup.",
	"content": "To clean up the Oracle WebCenter Portal domain setup, follow the steps below.\nDelete the Generated Domain Home To remove a domain home that you generated by running the create-domain.sh script in your production or testing environment, use the delete-domain-job.yaml file located at, \u0026lt;$WORKDIR\u0026gt;/create-wcp-domain/domain-home-on-pv/output/weblogic-domains/wcp-domain\u0026gt; directory.\n$ kubectl create -f delete-domain-job.yaml Clean Up the create-domain-job script After Execution Failure To clean up the create-domain-job script:\n  Get the create domain job and configmaps:\n$ kubectl get configmaps,jobs -n wcpns |grep \u0026#34;create-domain-job\u0026#34;   Delete the job and configmap:\n$ kubectl delete job job.batch/wcp-domain-create-fmw-infra-sample-domain-job -n wcpns $ kubectl delete configmap wcp-domain-create-fmw-infra-sample-domain-job-cm -n wcpns   Delete the contents of the PV, if any:\n$ sudo rm -rf /scratch/kubevolume   "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/cleanup-domain-setup/",
	"title": "Uninstall",
	"tags": [],
	"description": "Clean up the Oracle WebCenter Content domain setup.",
	"content": "Learn how to clean up the Oracle WebCenter Content domain setup.\nStop all Administration and Managed server pods First stop the all pods related to a domain. This can be done by patching domain \u0026ldquo;serverStartPolicy\u0026rdquo; to \u0026ldquo;NEVER\u0026rdquo;. Here is the sample command for the same.\n$ kubectl patch domain wcc-domain-name -n wcc-namespace --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; For example:\nkubectl patch domain wccinfra -n wccns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NEVER\u0026#34; }]\u0026#39; Remove the domain   Remove the domain\u0026rsquo;s ingress (for example, Traefik ingress) using Helm:\n$ helm uninstall wcc-domain-ingress -n sample-domain1-ns For example:\n$ helm uninstall wccinfra-traefik -n wccns   Remove the domain resources by using the sample delete-weblogic-domain-resources.sh script present at ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain $ ./delete-weblogic-domain-resources.sh -d sample-domain1 For example:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain $ ./delete-weblogic-domain-resources.sh -d wccinfra   Use kubectl to confirm that the server pods and domain resource are deleted:\n$ kubectl get pods -n sample-domain1-ns $ kubectl get domains -n sample-domain1-ns For example:\n$ kubectl get pods -n wccns $ kubectl get domains -n wccns   Drop the RCU schemas Follow these steps to drop the RCU schemas created for Oracle WebCenter Content domain.\nRemove the domain namespace   Configure the installed ingress load balancer (for example, Traefik) to stop managing the ingresses in the domain namespace:\n$ helm upgrade traefik-operator traefik/traefik \\  --namespace traefik \\  --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --wait   Configure the WebLogic Kubernetes Operator to stop managing the domain:\n$ helm upgrade sample-weblogic-operator \\  kubernetes/charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait For example:\n$ cd ${WORKDIR}/weblogic-kubernetes-operator $ helm upgrade weblogic-kubernetes-operator \\  kubernetes/charts/weblogic-operator \\  --namespace opns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait   Delete the domain namespace:\n$ kubectl delete namespace sample-domain1-ns For example:\n$ kubectl delete namespace wccns   Remove the WebLogic Kubernetes Operator   Remove the WebLogic Kubernetes Operator:\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns For example:\n$ helm uninstall weblogic-kubernetes-operator -n opns   Remove WebLogic Kubernetes Operator\u0026rsquo;s namespace:\n$ kubectl delete namespace sample-weblogic-operator-ns For example:\n$ kubectl delete namespace opns   Remove the load balancer   Remove the installed ingress based load balancer (for example, Traefik):\n$ helm uninstall traefik -n traefik   Remove the Traefik namespace:\n$ kubectl delete namespace traefik   Delete the domain home To remove the domain home that is generated using the create-domain.sh script, with appropriate privileges manually delete the contents of the storage attached to the domain home persistent volume (PV).\nFor example, for the domain\u0026rsquo;s persistent volume of type host_path:\n$ rm -rf /scratch/k8s_dir/WCC "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/adminguide/configure-mount-share/",
	"title": "Configure an additional mount or shared space to a domain for Imaging and Capture",
	"tags": [],
	"description": "Configure an additional mount or shared space to a domain, for WebCenter Imaging and WebCenter Capture",
	"content": "A volume can be mounted to a server pod which can be accessible directly from outside Kubernetes cluster so that an external application could write new files to it.\nThis can be used specifically in WebCenter Imaging and WebCenter Capture applications for File Imports.\nKubernetes supports several types of volumes as given in Volumes | Kubernetes.\nFurther in this section, we will take nfs volume as an example.\nMount \u0026ldquo;nfs\u0026rdquo; as volume To use a volume, specify the volumes to provide for the Pod in .spec.volumes and declare where to mount those volumes into containers in .spec.containers[*].volumeMounts in domain.yaml file.\nUpdate the domain.yaml and apply the changes as shown in sample below for mounting nfs server (for example, 100.XXX.XXX.X with shared export path at /sharedir) to all the server pods at /u01/sharedir.\nThe path /u01/sharedir can be configured as the file import path in WebCenter Imaging and WebCenter Capture applications and the files put to /sharedir will be processed by the applications.\nSample entry of domain.yaml with nfs-volume configuration\n... serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026#34; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wccinfra-domain-pvc - name: nfs-volume nfs: server: 100.XXX.XXX.XXX path: /sharedir volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - mountPath: /u01/sharedir name: nfs-volume ... "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/appendix/",
	"title": "Appendix",
	"tags": [],
	"description": "",
	"content": "This section provides information on miscellaneous tasks related to the Oracle WebCenter Portal deployment on Kubernetes.\n Domain resource sizing  Describes the resourse sizing information for the Oracle WebCenter Portal domain setup on Kubernetes cluster.\n Quick start deployment on-premise  Describes how to quickly get an Oracle WebCenter Portal domain instance running (using the defaults, nothing special) for development and test purposes.\n Security hardening  Review resources for the Docker and Kubernetes cluster hardening.\n Additional Configuration  Describes how to create connections to Oracle WebCenter Content Server to enable content integration within Oracle WebCenter Portal.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/oracle-cloud/",
	"title": "Oracle Cloud Infrastructure",
	"tags": [],
	"description": "",
	"content": "This is a guide to run WebLogic Kubernetes Operator managed WebCenter Content domain on Oracle Cloud Infrastructure. This section of the documentation is certified for WebLogic Kubernetes Operator version 3.3.0 and Oracle WebCenter Content 12.2.1.4 May'22 PSU (container image for this release can be downloaded from My Oracle Support MOS patch 34192566).\n Preparing an OKE environment  Running WebLogic Kubernetes Operator managed WebCenter Content domain on Oracle Kubernetes Engine (OKE).\n Preparing a file system  Running WebLogic Kubernetes Operator managed Oracle WebCenter Content domains on OKE\n Preparing OCIR  Running WebLogic Kubernetes Operator managed Oracle WebCenter Content domains on OKE\n Prepare environment for WCC domain  Prepare environment for WCC domain on Oracle Kubernetes Engine (OKE).\n Set up a load balancer  Configure different load balancers for Oracle WebCenter Content domains.\n Create Oracle WebCenter Content domain  Create Oracle WebCenter Content domain on Oracle Kubernetes Engine (OKE).\n Configuring Oracle WebCenter Content for Oracle Identity Cloud Service (IDCS)  Configuring Oracle WebCenter Content for Oracle Identity Cloud Service (IDCS)\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/",
	"title": "Oracle WebCenter Content",
	"tags": [],
	"description": "WebLogic Kubernetes Operator (the “operator”) supports deployment of Oracle WebCenter Content servers such as Oracle WebCenter Content(Content Server) and Oracle WebCenter Content(Inbound Refinery Server). Follow the instructions in this guide to set up Oracle WebCenter Content domain on Kubernetes.",
	"content": "In this release, Oracle WebCenter Content domain is supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nThe WebLogic Kubernetes Operator has several key features to assist you with deploying and managing Oracle WebCenter Content domain in a Kubernetes environment. You can:\n Create Oracle WebCenter Content instances(Oracle WebCenter Content server \u0026amp; Oracle WebCenter Content Inbounnd Refinery server) in a Kubernetes persistent volume (PV). This PV can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the Oracle WebCenter Content services and composites for external access. Scale Oracle WebCenter Content domains by starting and stopping Managed Servers on demand, or by integrating with a REST API. Publish WebLogic Kubernetes Operator and WebLogic Server logs to Elasticsearch and interact with them in Kibana. Monitor the Oracle WebCenter Content instance using Prometheus and Grafana.  Current production release The current supported production release of WebLogic Kubernetes Operator, for Oracle WebCenter Content domain deployment is 3.3.0.\nRecent changes See the Release Notes for recent changes for Oracle WebCenter Content domain deployment on Kubernetes.\nLimitations See here for limitations in this release.\nAbout this documentation This documentation includes sections targeted to different audiences. To help you find what you are looking for easily, please consult this table of contents:\n  Quick Start explains how to quickly get an Oracle WebCenter Content instance running, using the defaults. Note that this is only for development and test purposes.\n  Install Guide and Administration Guide provide detailed information about all aspects of using WebLogic Kubernetes Operator including:\n Installing and configuring WebLogic Kubernetes Operator. Using WebLogic Kubernetes Operator to create and manage Oracle WebCenter Content domain. Configuring Kubernetes load balancers. Configuring Custom SSL certificates. Configuring Elasticsearch and Kibana to access the WebLogic Kubernetes Operator and WebLogic Server log files. Deploying composite applications for Oracle WebCenter Content. Patching an Oracle WebCenter Content Docker image. Removing/deleting domain. And much more!    Additional reading Oracle WebCenter Content domain deployment on Kubernetes leverages WebLogic Kubernetes Operator framework.\n To develop an understanding of WebLogic Kubernetes Operator, including design, architecture, domain life cycle management, and configuration overrides, review the WebLogic Kubernetes Operator documentation. To learn more about the Oracle WebCenter Content architecture and components, see Understanding Oracle WebCenter Content.  "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wccontent-domains/appendix/",
	"title": "Appendix",
	"tags": [],
	"description": "",
	"content": "This section provides information on miscellaneous tasks related to Oracle WebCenter Content domain deployment on Kubernetes.\n Domain resource sizing  Describes the resourse sizing information for Oracle WebCenter Content domain setup on Kubernetes cluster.\n Quick start deployment guide  Describes how to quickly get an Oracle WebCenter Content domain instance running (using the defaults, nothing special) for development and test purposes.\n Security hardening  Review resources for the Docker and Kubernetes cluster hardening.\n "
},
{
	"uri": "/fmw-kubernetes/22.2.3/wcportal-domains/",
	"title": "Oracle WebCenter Portal",
	"tags": [],
	"description": "The WebLogic Kubernetes operator (the “operator”) supports deployment of Oracle WebCenter Portal. Follow the instructions in this guide to set up Oracle WebCenter Portal domain on Kubernetes.",
	"content": "With the WebLogic Kubernetes operator (operator), you can deploy your Oracle WebCenter Portal on Kubernetes.\nIn this release, Oracle WebCenter Portal domain is based on the “domain on a persistent volume” model, where the domain home is located in a persistent volume.\nIn this release the support for Portlet Managed Server has been added.\nThe operator has several key features to assist you with deploying and managing the Oracle WebCenter Portal domain in a Kubernetes environment. You can:\n Create Oracle WebCenter Portal instances in a Kubernetes PV. This PV can reside in an Network File System (NFS) or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the Oracle WebCenter Portal services for external access. Scale Oracle WebCenter Portal domain by starting and stopping Managed Servers on demand, or by integrating with a REST API. Publish operator and WebLogic Server logs to Elasticsearch and interact with them in Kibana. Monitor the Oracle WebCenter Portal instance using Prometheus and Grafana.  Current release The current release for the Oracle WebCenter Portal domain deployment on Kubernetes is 22.2.3. This release uses the WebLogic Kubernetes Operator version 3.3.0.\n Note that this release is only for evaluation purposes and hence applicable to Development and Test deployments only.\n Recent changes and known issues See the Release Notes for recent changes and known issues with the Oracle WebCenter Portal domain deployment on Kubernetes.\nAbout this documentation This documentation includes sections targeted to different audiences. To help you find what you are looking for more easily, please use this table of contents:\n  Quick Start explains how to quickly get an Oracle WebCenter Portal domain instance running, using the defaults, nothing special. Note that this is only for development and test purposes.\n  Install Guide and Administration Guide provide detailed information about all aspects of using the Kubernetes operator including:\n Installing and configuring the operator Using the operator to create and manage Oracle WebCenter Portal domain Configuring WebCenter Portal for Search Configuring Kubernetes load balancers Configuring Prometheus and Grafana to monitor WebCenter Portal Configuring Logging using ElasticSearch    Documentation for earlier releases To view documentation for an earlier release, see:\n Version 21.2.3  "
},
{
	"uri": "/fmw-kubernetes/22.2.3/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/fmw-kubernetes/22.2.3/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]